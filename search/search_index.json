{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Technotes to structure newly learned stuff or to us it as a memory aid. Is generated from https://github.com/CrowdSalat/technotes .","title":"Home"},{"location":"#home","text":"Technotes to structure newly learned stuff or to us it as a memory aid. Is generated from https://github.com/CrowdSalat/technotes .","title":"Home"},{"location":"WSL/","text":"Linux Subsystem for Windows (WSL) WSL1 vs. WSL2 WSL2 uses a real linux kernel. WSL1 is just a linux api wrapper around the windows kernel. # list installed wls with version wsl --list --verbose # update wsl from wsl1 to wsl2 wsl --set-version Ubuntu-20.04 2 # delete wsl wsl --unregister Ubuntu-20.04 Troubleshoot WORKAROUND: No Internet connection in WSL2 Run: sudo bash -c 'echo \"nameserver 1.1.1.1\" > /etc/resolv.conf' or add the command to you bashrc. To permanently set this nameserver you need to disable the auto generation this file: # info cat /etc/resolv.conf > # This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf: > # [network] > # generateResolvConf = false # stop automatic generation of /etc/resolv.conf sudo bash -c 'printf \"[network]\\ngenerateResolvConf = false\\n\" >> /etc/wsl.conf' # shutodwn wsl wsl.exe --shutdown # remove /etc/resolv.conf which is a useless symlink after deactivating the generation of this file sudo rm /etc/resolv.conf # start wsl and add new name server sudo bash -c 'echo \"nameserver 1.1.1.1\" > /etc/resolv.conf' time not set correctly Some auth methods of CLIs like aws, azure or gcloud depend on a working clock. # check time date # see https://github.com/microsoft/WSL/issues/4245 sudo hwclock -s # or alternativly sudo apt install ntpdate sudo ntpdate pool.ntp.org Git under WSL Official doc Use the git credential manager: git config --global credential.helper \"/mnt/c/dev/Git/mingw64/libexec/git-core/git-credential-manager.exe\"","title":"Linux Subsystem for Windows (WSL)"},{"location":"WSL/#linux-subsystem-for-windows-wsl","text":"","title":"Linux Subsystem for Windows (WSL)"},{"location":"WSL/#wsl1-vs-wsl2","text":"WSL2 uses a real linux kernel. WSL1 is just a linux api wrapper around the windows kernel. # list installed wls with version wsl --list --verbose # update wsl from wsl1 to wsl2 wsl --set-version Ubuntu-20.04 2 # delete wsl wsl --unregister Ubuntu-20.04","title":"WSL1 vs. WSL2"},{"location":"WSL/#troubleshoot","text":"","title":"Troubleshoot"},{"location":"WSL/#workaround-no-internet-connection-in-wsl2","text":"Run: sudo bash -c 'echo \"nameserver 1.1.1.1\" > /etc/resolv.conf' or add the command to you bashrc. To permanently set this nameserver you need to disable the auto generation this file: # info cat /etc/resolv.conf > # This file was automatically generated by WSL. To stop automatic generation of this file, add the following entry to /etc/wsl.conf: > # [network] > # generateResolvConf = false # stop automatic generation of /etc/resolv.conf sudo bash -c 'printf \"[network]\\ngenerateResolvConf = false\\n\" >> /etc/wsl.conf' # shutodwn wsl wsl.exe --shutdown # remove /etc/resolv.conf which is a useless symlink after deactivating the generation of this file sudo rm /etc/resolv.conf # start wsl and add new name server sudo bash -c 'echo \"nameserver 1.1.1.1\" > /etc/resolv.conf'","title":"WORKAROUND: No Internet connection in WSL2"},{"location":"WSL/#time-not-set-correctly","text":"Some auth methods of CLIs like aws, azure or gcloud depend on a working clock. # check time date # see https://github.com/microsoft/WSL/issues/4245 sudo hwclock -s # or alternativly sudo apt install ntpdate sudo ntpdate pool.ntp.org","title":"time not set correctly"},{"location":"WSL/#git-under-wsl","text":"Official doc Use the git credential manager: git config --global credential.helper \"/mnt/c/dev/Git/mingw64/libexec/git-core/git-credential-manager.exe\"","title":"Git under WSL"},{"location":"ansible-overview/","text":"ansible My ansible boilerplate concepts Ansible runs agentless. The remote machine just needs ssh installed (and for many modules python and pip). the machine which got the executables installed is called control node the managed machines are called managed nodes or informally hosts a list of managed nodes is called inventory or informally hostfile modules are the units of code Ansible executes a task executes a module with very specific arguments a playbook is a executable list of ansible task playbook and play will often get used synonymously. A playbook is the yaml file which may container one or more plays. A play is seperated by --- in the yaml file) up and running install control node: intall ansible managed node open ssh port python installed (for most modules) pip installed (for most modules) create inventory Official guide You can add managed nodes in the file /etc/ansible/hosts either via the ini format or via the yaml format. Or set path via the -i flag. Hostfiles need to have the following rights: sudo chmod 744 /etc/ansible/hosts and might otherwise clain that the file cannot be parsed (\"Unable to parse /etc/ansible/hosts as an inventory source\"). A simple host file in ini format: 192.168.0.2 ansible_user=pi A simple host file in yaml format: all: hosts: name_of_node: ansible_host: 192.168.0.2 ansible_user: pi Add your public ssh key to the managed nodes ~/.ssh/known_hosts files. Use the ssh-copy-id command or copy it manually. Use ansible all -m ping to test if all nodes in your inventory are reachable. ad-hoc commands official guide The syntax of a ad-hoc command is as following: # ansible [pattern] -m [module] -a \"[module options]\" # use of ping module ansible all -m ping # a exmaple which is run on all hosts ansible all -a \"/bin/echo hello\" If you do not specify otherwise the ad-hoc will be run by the command module. playbooks official guide playbooks are defined in yaml format To run a playbook: ansible-playbook playbook_file.yml prerequisite for most modules like docker_image or docker_container: the host has python and pip installed To run a playbook on one host you can: define the host explicitly in the playbook instead of using the all target define the host field as a external variable ( {{target}} ) and set it via command line ( ansible-playbook playbook.yml --extra-vars \"target=pi\" ) run the playbook with a given a specific host file which contains only the one client ( ansible-playbook -i host-pi.yml playbook.yml ) install software via playbook: --- - hosts: name_of_node tasks: - name: Install Java package: name='java-1.8.0-openjdk' state=latest This example uses the package module with no arguments. start docker container via playbook: --- - hosts: all become: true vars: container_name: hello-world container_image: hypriot/armhf-hello-world tasks: - name: install docker module for python pip: name: docker - name: pull docker image docker_image: name: \"{{ container_image }}\" source: pull - name: start docker containers docker_container: name: \"{{ container_name }}\" image: \"{{ container_image }}\" state: present This example uses the pip , docker_image and the docker_container module with a set of arguments. Note: there was an error while using dokcer_image module No module named ssl_match_hostname . The solution was to run sudo apt-get remove python-configparser optional grouping of managed nodes You can address the all nodes in a group via ansible optional_group_name -m ping and a specific node by its name ansible name_of_node -m ping A host file witch uses groups (children) and a node without a group in yaml format: all: hosts: name_of_node_without_group: ansible_host: 192.168.0.2 ansible_user: pi children: group_name1: hosts: name_of_node_g1_1: ansible_host: 192.168.0.3 ansible_user: pi name_of_node_g1_2: ansible_host: 192.168.0.4 ansible_user: pi group_name2: hosts: name_of_node_g2_1: ansible_host: 192.168.0.5 ansible_user: pi test stuff locally run ansible-playbook filename - name: My play hosts: localhost # to prevent ssh connection: local tasks: - name: Caculate Free Harddrive Memory assert: that: - not {{ item.mount == '/' and (item.size_available < 8589934592 ) }} with_items: \"{{ ansible_facts['mounts'] }}\" - name: Print all Hostvars debug: msg: \"{{ hostvars }}\" access dicitonary Define dictionary: example: key1: val1 key2: val2 selected_key: key1 - name: print val1 by using literal key debug: msg: \"{{ example ['key1']}\" - name: print val1 by using variable key debug: msg: \"{{ example [ selected_key ]}\" test ansible in vagrant see restart failed task Seems not to be possible, but you can use --start-at-task parameter of ansible-playbook cli. running anisble in pipeline Container images for ansible: 2.3-2.13 To use Ansible galaxy hosted on git you might need a workaround for authentification via token access set host_key_checking in ansible.cfg to false otheriwse you will get a \"Host key verification failed\" or set env variable ANSIBLE_HOST_KEY_CHECKING in the variables part of the pipeline Container image requirements: bash (execute deploy.sh script) ansible git + ssh-agent (to check out ansible galaxy roles from git) ansible output config In ansible.cfg [default] ... # more human readable log stdout_callback = yaml force_color = true # print summary of task with execution times at the end of play callback_whitelist = profile_tasks [callback_profile_tasks] sort_order = none task_output_limit = 200 Worked with ansible 2.8 change user become without become_user runs a command with a prepended sudo. When you add become_user it runs sudo -u <become_user> /bin/sh -c <command> . Even though sudo su - <user> works without password prompt sudo -u <user> might not work without password prompt. If you add become_method: su it runs su <become_user> -c /bin/sh -c <command> To run sudo su - <become_user> /bin/sh -c <command> add become_exe='sudo su -' to ansible.cfg and add become_method: su . See here or here becoming unpriviliged user Failed to set permissions on the temporary files Ansible needs to create when becoming an unprivileged user (rc: 1, err: chown: changing ownership of '/var/tmp/ansible-tmp-1672820952.604988-42392-112740864206926/': Operation not permitted chown: changing ownership of '/var/tmp/ansible-tmp-1672820952.604988-42392-112740864206926/AnsiballZ_command.py': Operation not permitted }). For information on working around this, see https://docs.ansible.com/ansible/become.html#becoming-an-unprivileged-user You can put the allow_world_readable_tmpfiles in the ansible.cfg configuration file Or make sure the setfacl tool (provided by the acl package) is installed on the remote host. Source update bashrc Use lineinfile module. hostvars naming convention of variables in roles Do not use dictionaries e.g. my.var.a, my.bla.b. If you access a top level part of the dictionary like my you likely overwrite all entries e.g. var, bla. You can change this default behavior by setting hash_behaviour=merge in ansible.cfg Source roles accessing templates in playbooks The following path are searched by the template module (paths get printed as an error when it cannot find a template): Searched template file: template.yml.j2' Searched in: ./../role/templates/template.yml.j2 ./../role/template.yml.j2 ./../role/tasks/templates/template.yml.j2 ./../role/tasks/template.yml.j2 ./../playbook/templates/template.yml.j2 ./../playbook/template.yml.j2","title":"ansible"},{"location":"ansible-overview/#ansible","text":"My ansible boilerplate","title":"ansible"},{"location":"ansible-overview/#concepts","text":"Ansible runs agentless. The remote machine just needs ssh installed (and for many modules python and pip). the machine which got the executables installed is called control node the managed machines are called managed nodes or informally hosts a list of managed nodes is called inventory or informally hostfile modules are the units of code Ansible executes a task executes a module with very specific arguments a playbook is a executable list of ansible task playbook and play will often get used synonymously. A playbook is the yaml file which may container one or more plays. A play is seperated by --- in the yaml file)","title":"concepts"},{"location":"ansible-overview/#up-and-running","text":"","title":"up and running"},{"location":"ansible-overview/#install","text":"control node: intall ansible managed node open ssh port python installed (for most modules) pip installed (for most modules)","title":"install"},{"location":"ansible-overview/#create-inventory","text":"Official guide You can add managed nodes in the file /etc/ansible/hosts either via the ini format or via the yaml format. Or set path via the -i flag. Hostfiles need to have the following rights: sudo chmod 744 /etc/ansible/hosts and might otherwise clain that the file cannot be parsed (\"Unable to parse /etc/ansible/hosts as an inventory source\"). A simple host file in ini format: 192.168.0.2 ansible_user=pi A simple host file in yaml format: all: hosts: name_of_node: ansible_host: 192.168.0.2 ansible_user: pi Add your public ssh key to the managed nodes ~/.ssh/known_hosts files. Use the ssh-copy-id command or copy it manually. Use ansible all -m ping to test if all nodes in your inventory are reachable.","title":"create inventory"},{"location":"ansible-overview/#ad-hoc-commands","text":"official guide The syntax of a ad-hoc command is as following: # ansible [pattern] -m [module] -a \"[module options]\" # use of ping module ansible all -m ping # a exmaple which is run on all hosts ansible all -a \"/bin/echo hello\" If you do not specify otherwise the ad-hoc will be run by the command module.","title":"ad-hoc commands"},{"location":"ansible-overview/#playbooks","text":"official guide playbooks are defined in yaml format To run a playbook: ansible-playbook playbook_file.yml prerequisite for most modules like docker_image or docker_container: the host has python and pip installed To run a playbook on one host you can: define the host explicitly in the playbook instead of using the all target define the host field as a external variable ( {{target}} ) and set it via command line ( ansible-playbook playbook.yml --extra-vars \"target=pi\" ) run the playbook with a given a specific host file which contains only the one client ( ansible-playbook -i host-pi.yml playbook.yml ) install software via playbook: --- - hosts: name_of_node tasks: - name: Install Java package: name='java-1.8.0-openjdk' state=latest This example uses the package module with no arguments. start docker container via playbook: --- - hosts: all become: true vars: container_name: hello-world container_image: hypriot/armhf-hello-world tasks: - name: install docker module for python pip: name: docker - name: pull docker image docker_image: name: \"{{ container_image }}\" source: pull - name: start docker containers docker_container: name: \"{{ container_name }}\" image: \"{{ container_image }}\" state: present This example uses the pip , docker_image and the docker_container module with a set of arguments. Note: there was an error while using dokcer_image module No module named ssl_match_hostname . The solution was to run sudo apt-get remove python-configparser","title":"playbooks"},{"location":"ansible-overview/#optional-grouping-of-managed-nodes","text":"You can address the all nodes in a group via ansible optional_group_name -m ping and a specific node by its name ansible name_of_node -m ping A host file witch uses groups (children) and a node without a group in yaml format: all: hosts: name_of_node_without_group: ansible_host: 192.168.0.2 ansible_user: pi children: group_name1: hosts: name_of_node_g1_1: ansible_host: 192.168.0.3 ansible_user: pi name_of_node_g1_2: ansible_host: 192.168.0.4 ansible_user: pi group_name2: hosts: name_of_node_g2_1: ansible_host: 192.168.0.5 ansible_user: pi","title":"optional grouping of managed nodes"},{"location":"ansible-overview/#test-stuff-locally","text":"run ansible-playbook filename - name: My play hosts: localhost # to prevent ssh connection: local tasks: - name: Caculate Free Harddrive Memory assert: that: - not {{ item.mount == '/' and (item.size_available < 8589934592 ) }} with_items: \"{{ ansible_facts['mounts'] }}\" - name: Print all Hostvars debug: msg: \"{{ hostvars }}\"","title":"test stuff locally"},{"location":"ansible-overview/#access-dicitonary","text":"Define dictionary: example: key1: val1 key2: val2 selected_key: key1 - name: print val1 by using literal key debug: msg: \"{{ example ['key1']}\" - name: print val1 by using variable key debug: msg: \"{{ example [ selected_key ]}\"","title":"access dicitonary"},{"location":"ansible-overview/#test-ansible-in-vagrant","text":"see","title":"test ansible in vagrant"},{"location":"ansible-overview/#restart-failed-task","text":"Seems not to be possible, but you can use --start-at-task parameter of ansible-playbook cli.","title":"restart failed task"},{"location":"ansible-overview/#running-anisble-in-pipeline","text":"Container images for ansible: 2.3-2.13 To use Ansible galaxy hosted on git you might need a workaround for authentification via token access set host_key_checking in ansible.cfg to false otheriwse you will get a \"Host key verification failed\" or set env variable ANSIBLE_HOST_KEY_CHECKING in the variables part of the pipeline Container image requirements: bash (execute deploy.sh script) ansible git + ssh-agent (to check out ansible galaxy roles from git)","title":"running anisble in pipeline"},{"location":"ansible-overview/#ansible-output-config","text":"In ansible.cfg [default] ... # more human readable log stdout_callback = yaml force_color = true # print summary of task with execution times at the end of play callback_whitelist = profile_tasks [callback_profile_tasks] sort_order = none task_output_limit = 200 Worked with ansible 2.8","title":"ansible output config"},{"location":"ansible-overview/#change-user","text":"become without become_user runs a command with a prepended sudo. When you add become_user it runs sudo -u <become_user> /bin/sh -c <command> . Even though sudo su - <user> works without password prompt sudo -u <user> might not work without password prompt. If you add become_method: su it runs su <become_user> -c /bin/sh -c <command> To run sudo su - <become_user> /bin/sh -c <command> add become_exe='sudo su -' to ansible.cfg and add become_method: su . See here or here","title":"change user"},{"location":"ansible-overview/#becoming-unpriviliged-user","text":"Failed to set permissions on the temporary files Ansible needs to create when becoming an unprivileged user (rc: 1, err: chown: changing ownership of '/var/tmp/ansible-tmp-1672820952.604988-42392-112740864206926/': Operation not permitted chown: changing ownership of '/var/tmp/ansible-tmp-1672820952.604988-42392-112740864206926/AnsiballZ_command.py': Operation not permitted }). For information on working around this, see https://docs.ansible.com/ansible/become.html#becoming-an-unprivileged-user You can put the allow_world_readable_tmpfiles in the ansible.cfg configuration file Or make sure the setfacl tool (provided by the acl package) is installed on the remote host. Source","title":"becoming unpriviliged user"},{"location":"ansible-overview/#update-bashrc","text":"Use lineinfile module.","title":"update bashrc"},{"location":"ansible-overview/#hostvars-naming-convention-of-variables-in-roles","text":"Do not use dictionaries e.g. my.var.a, my.bla.b. If you access a top level part of the dictionary like my you likely overwrite all entries e.g. var, bla. You can change this default behavior by setting hash_behaviour=merge in ansible.cfg Source","title":"hostvars naming convention of variables in roles"},{"location":"ansible-overview/#roles-accessing-templates-in-playbooks","text":"The following path are searched by the template module (paths get printed as an error when it cannot find a template): Searched template file: template.yml.j2' Searched in: ./../role/templates/template.yml.j2 ./../role/template.yml.j2 ./../role/tasks/templates/template.yml.j2 ./../role/tasks/template.yml.j2 ./../playbook/templates/template.yml.j2 ./../playbook/template.yml.j2","title":"roles accessing templates in playbooks"},{"location":"argocd/","text":"Argocd argo projects if you use argo project != default you might need to add additional configuration to them e.g. \"Source Repositories\" or \"Destinations\" in order that the argo applications in the project work as expected. To make it work you can also use \"*\" for these configurations, but I would not recommend it. Better to understand it correctly and set specific values, have a look at Argo official Project doc To see this in UI: Settings -> Projects Self-signed & Untrusted TLS Certificates If a git repository uses a self signed certificate you might see the exception \"failed to verify certificate x509 certificate signed by unknown authority\" when creating a argo app. You can add the certficate via settings, for details see official doc . It seems that the easiest way is to add the root ca. It seems that if the CN is not correct for the \"Repository Server Name\" the validation fails in argo (but to be honest I am not entirely sure). Double check that you have written the exact \"Repository Server Name\" that was shown in the exception. credential template (for git repositories) Based on official argocd doc beware when using ssh: ingress and routes do not support ssh over port 22, only https credential template allow that you connect to a git repository without defining the credentials. The URL configured for a credential template (e.g. https://github.com/argoproj) must match as prefix for the repository URL in an Argo application. A credential template might be overriden by the credentials in a \"repository\" config of argo. Steps: Add git server to known host by appending an existing configmap (ssh only) Check if known host is picked up in UI: Settings -> Repository certificates and known hosts (ssh only) Create the credential template by creating a secret Check if credential template is picked up in UI: Settings -> Repositories profit: use this template in your argo application 1. Add git server to known host by appending an existing configmap # copy the content of ssh-keyscan URL_OF_MY_GIT_SERVER # to oc edit -n openshift-gitops configmap argocd-ssh-known-hosts-cm 3. Create the credential template by creating a secret kind: Secret apiVersion: v1 metadata: name: git-repo-credential-template namespace: openshift-gitops labels: # needed to be used by argo argocd.argoproj.io/secret-type: repo-creds data: # echo -n \"git\" | base64 type: Z2l0Cg== # cat id_ed25519 | base64 sshPrivateKey: PLACEHOLDER # for ssh: echo -n \"git@gitlab.example\" | base64 # for https: echo -n \"https://gitlab.example\" | base64 url: PLACEHOLDER type: Opaque Or if you want to create the yaml with cli: # generate Repository template for ArgoCD oc create secret generic git-repo-template --from-file=sshPrivateKey=./key --from-literal=url=git@gitlab.example --from-literal=type=git -o yaml --dry-run=client > argo-repositorytemplate-secret.yaml ## then add label so it is used by argo: # argocd.argoproj.io/secret-type: repo-creds code argo-repositorytemplate-secret.yaml # oc apply -f argo-repositorytemplate.secret.yaml -n openshift-gitops or for https instead of ssh Tbh a bit less failure prone while creating and also no issues when you host git server via K8s/OpenShift with Ingress/Route: apiVersion: v1 kind: Secret metadata: name: private-repo-creds namespace: openshift-gitops labels: argocd.argoproj.io/secret-type: repo-creds stringData: type: git url: https://github.com/argoproj password: my-password username: my-username insecure: false The command argocd repo add example/repo.git --insecure-skip-server-verification --name test --type git --username=BLA --password=BLA will generate a similar secret (but insecure=true) with the name repo-**** in the ArgoCD namespace example application apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: test spec: destination: server: 'https://kubernetes.default.svc' project: default source: path: . repoURL: 'https://gitlab.apps.ocp.weyrich.dev/gitops/vms.git' targetRevision: HEAD","title":"Argocd"},{"location":"argocd/#argocd","text":"","title":"Argocd"},{"location":"argocd/#argo-projects","text":"if you use argo project != default you might need to add additional configuration to them e.g. \"Source Repositories\" or \"Destinations\" in order that the argo applications in the project work as expected. To make it work you can also use \"*\" for these configurations, but I would not recommend it. Better to understand it correctly and set specific values, have a look at Argo official Project doc To see this in UI: Settings -> Projects","title":"argo projects"},{"location":"argocd/#self-signed-untrusted-tls-certificates","text":"If a git repository uses a self signed certificate you might see the exception \"failed to verify certificate x509 certificate signed by unknown authority\" when creating a argo app. You can add the certficate via settings, for details see official doc . It seems that the easiest way is to add the root ca. It seems that if the CN is not correct for the \"Repository Server Name\" the validation fails in argo (but to be honest I am not entirely sure). Double check that you have written the exact \"Repository Server Name\" that was shown in the exception.","title":"Self-signed &amp; Untrusted TLS Certificates"},{"location":"argocd/#credential-template-for-git-repositories","text":"Based on official argocd doc beware when using ssh: ingress and routes do not support ssh over port 22, only https credential template allow that you connect to a git repository without defining the credentials. The URL configured for a credential template (e.g. https://github.com/argoproj) must match as prefix for the repository URL in an Argo application. A credential template might be overriden by the credentials in a \"repository\" config of argo. Steps: Add git server to known host by appending an existing configmap (ssh only) Check if known host is picked up in UI: Settings -> Repository certificates and known hosts (ssh only) Create the credential template by creating a secret Check if credential template is picked up in UI: Settings -> Repositories profit: use this template in your argo application","title":"credential template (for git repositories)"},{"location":"argocd/#1-add-git-server-to-known-host-by-appending-an-existing-configmap","text":"# copy the content of ssh-keyscan URL_OF_MY_GIT_SERVER # to oc edit -n openshift-gitops configmap argocd-ssh-known-hosts-cm","title":"1. Add git server to known host by appending an existing configmap"},{"location":"argocd/#3-create-the-credential-template-by-creating-a-secret","text":"kind: Secret apiVersion: v1 metadata: name: git-repo-credential-template namespace: openshift-gitops labels: # needed to be used by argo argocd.argoproj.io/secret-type: repo-creds data: # echo -n \"git\" | base64 type: Z2l0Cg== # cat id_ed25519 | base64 sshPrivateKey: PLACEHOLDER # for ssh: echo -n \"git@gitlab.example\" | base64 # for https: echo -n \"https://gitlab.example\" | base64 url: PLACEHOLDER type: Opaque Or if you want to create the yaml with cli: # generate Repository template for ArgoCD oc create secret generic git-repo-template --from-file=sshPrivateKey=./key --from-literal=url=git@gitlab.example --from-literal=type=git -o yaml --dry-run=client > argo-repositorytemplate-secret.yaml ## then add label so it is used by argo: # argocd.argoproj.io/secret-type: repo-creds code argo-repositorytemplate-secret.yaml # oc apply -f argo-repositorytemplate.secret.yaml -n openshift-gitops","title":"3. Create the credential template by creating a secret"},{"location":"argocd/#or-for-https-instead-of-ssh","text":"Tbh a bit less failure prone while creating and also no issues when you host git server via K8s/OpenShift with Ingress/Route: apiVersion: v1 kind: Secret metadata: name: private-repo-creds namespace: openshift-gitops labels: argocd.argoproj.io/secret-type: repo-creds stringData: type: git url: https://github.com/argoproj password: my-password username: my-username insecure: false The command argocd repo add example/repo.git --insecure-skip-server-verification --name test --type git --username=BLA --password=BLA will generate a similar secret (but insecure=true) with the name repo-**** in the ArgoCD namespace","title":"or for https instead of ssh"},{"location":"argocd/#example-application","text":"apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: test spec: destination: server: 'https://kubernetes.default.svc' project: default source: path: . repoURL: 'https://gitlab.apps.ocp.weyrich.dev/gitops/vms.git' targetRevision: HEAD","title":"example application"},{"location":"buildpacks/","text":"Buildpacks tools pack cli Paketo Buildpacks implement the Buildpack API described in the Cloud Native Buildpacks Specification . terms A builder (image) contains a set of buildpacks and a stack a buildpack examine your app source code, identify and gather dependencies, and output OCI compliant app and dependency layers. It operates in two phases: detect and build. a stack (image) consist of build image and a run image","title":"Buildpacks"},{"location":"buildpacks/#buildpacks","text":"","title":"Buildpacks"},{"location":"buildpacks/#tools","text":"pack cli Paketo Buildpacks implement the Buildpack API described in the Cloud Native Buildpacks Specification .","title":"tools"},{"location":"buildpacks/#terms","text":"A builder (image) contains a set of buildpacks and a stack a buildpack examine your app source code, identify and gather dependencies, and output OCI compliant app and dependency layers. It operates in two phases: detect and build. a stack (image) consist of build image and a run image","title":"terms"},{"location":"certificates/","text":"Certificates and encryption download pem with cli openssl s_client -showcerts -connect www.serverfault.com:443 < /dev/null certificate expiration validity period: when this period ends the certificate and all derived certs are expired renewal period: period at the end of the validity period where the old certificate is replaced by a new one with a new validity period. The private key stays unchanged. renewal (in contrast to replacing) keeps the private key of a certificate and only recreate the public certificate with a new validity period wildcard certificate star works only for one subdomain. file types .pem: contains as first line: -----BEGIN CERTIFICATE----- can be a private key or a public certificate can be read with text editor but is base64 encoded or with: openssl x509 -in cert.pem -text .der (.cer) binary encoded x509 certificate .key private key in PEM format .cert;.cer;.crt: certificate with private key (DER or PEM) .p12 - pkcs12 container which contains private key and public certificate .pfx - pkcs12 .csr (certificate signing request): temporary more on pem A PEM file looks like this: -----BEGIN [TYPE OF DATA]----- [Base64 encoded binary data] -----END [TYPE OF DATA]----- The [TYPE OF DATA] part indicates the format of the binary data within the base64 block. This is where PKCS#1, PKCS#8, and OpenSSH formats come into play for private keys. PKCS#1 PEM Format (-----BEGIN RSA PRIVATE KEY-----) - : PKCS#1 (Public-Key Cryptography Standards #1) specifically defines the syntax for RSA public and private keys PKCS#8 PEM Format (-----BEGIN PRIVATE KEY----- or -----BEGIN ENCRYPTED PRIVATE KEY-----) - PKCS#8 (Private-Key Information Syntax Specification) is a more generic standard for storing private keys for any public-key algorithm, not just RSA. OpenSSH Private Key Format (-----BEGIN OPENSSH PRIVATE KEY-----) - This is a custom, OpenSSH-specific format, introduced in OpenSSH 7.8 (released in 2017). default locations Linux Source: Where does Go lang search for certificates Debian/Ubuntu/Gentoo etc: \"/etc/ssl/certs/ca-certificates.crt\" Fedora/RHEL 6: \"/etc/pki/tls/certs/ca-bundle.crt\" OpenSUSE: \"/etc/ssl/ca-bundle.pem\" OpenELEC: \"/etc/pki/tls/cacert.pem\" CentOS/RHEL 7: \"/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\" Alpine Linux: \"/etc/ssl/cert.pem\" CentOS/RHEL 7 Source to add a certficate as ca add the pem or der file to /etc/pki/ca-trust/source/anchors/ (higher precedence) or /usr/share/pki/ca-trust-source/anchors/ (lower precedence) and run update-ca-trust if you ever feel tempted to edit /etc/pki/ca-trust/extracted/ directly (instead of generating it) double check if you use the right format BEGIN/END CERTIFICATE vs. BEGIN/END TRUSTED CERTIFICATE and the file right name. See the following listing. # list certs trust list # add pem or der certs to: /etc/pki/ca-trust/source/anchors/ or /usr/share/pki/ca-trust-source/anchors/ update-ca-trust # see manpage: https://www.unix.com/man-page/centos/8/update-ca-trust/ # updates certs in the following folders: ## /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt ## /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem ## /etc/pki/ca-trust/extracted/pem/email-ca-bundle.pem ## /etc/pki/ca-trust/extracted/pem/objsign-ca-bundle.pem ## /etc/pki/ca-trust/extracted/java/cacerts ## and then in /etc/pki/tls/certs/ca-bundle.crt (open)Suse add pem certs to /usr/share/pki/anchors run sudo update-ca-certificates create pem from p12 PASSWORD=\"test\" ## deprecared: -nodes = \"no DES\" no password for private keys ## --d - private key will not be encrypted (no password for private keys neeed) # ca.crt (PEM) openssl pkcs12 -in keystore.p12 -nodes -out out/ca.crt -cacerts -nokeys -passin pass:$PASSWORD # tls.crt (PEM) openssl pkcs12 -in keystore.p12 -nodes -out out/tls.crt -nokeys -passin pass:$PASSWORD # tls.key (PEM) openssl pkcs12 -in keystore.p12 -nodes -out out/tls.key -nocerts -passin pass:$PASSWORD generate certificate and private key in pem format (self signed) # create ## with subject prompt openssl req -x509 -newkey rsa:4096 -keyout ca.demo.key -out ca.demo.crt ## without subject prompt openssl req -x509 -newkey rsa:4096 -keyout ca.demo.key -out ca.demo.crt -subj \"/CN=ca.demo.de\" ## without encrypted private key and without subject prompt openssl req --nodes -x509 -newkey rsa:4096 -keyout my.key -out my.crt -subj \"/CN=ca.demo.de\" # read openssl x509 -in my.crt -text -noout openssl rsa -in my.crt -text -noout generate certificate and private key in pem format (ca signed) # generate ca key and cert in pem format (or use existing CA key and cert) openssl req -x509 -newkey rsa:4096 -keyout ca.demo.key -out ca.demo.crt -subj \"/CN=ca.demo.de\" # create key for client openssl genrsa -out client.demo.key 2048 # create signing requets for a client (unsigned certificate) openssl req -new -key client.demo.key -subj \"/CN=client.demo.de\" -out client.demo.csr # create signed client certificate openssl x509 -req -in client.demo.csr -signkey client.demo.key -out client.demo.crt download leaf cert from url HOSTNAME= PORT= openssl s_client -showcerts -connect ${HOSTNAME}:${PORT} /dev/null|openssl x509 -outform PEM >leafcert-${HOSTNAME}.pem pem headers examples -----BEGIN CERTIFICATE----- -----BEGIN CERTIFICATE REQUEST----- -----BEGIN RSA PRIVATE KEY----- -----BEGIN ENCRYPTED PRIVATE KEY----- ssh key To generate ssh keys run ssh-keygen which generates ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa. The private and public key do no use the pem format. The name of the owner is at the end of the pub file. Java truststore and keystore A Java keystore holds your private Keys + and your certificate A Java truststore holds the certificates you trust when you connect yourself to a server You may need to assure that the truststore contains the whole trust-chain otherwise java may not accept a connection to the server .fileending (Format): .jks (JSK), .keystore (JSK), .p12 (PKCS12) pkcs12 is the recommended format for keystores, not jks Javas bundled truststore resides in $JAVA_HOME/jre/lib/security/jssecacerts and $JAVA_HOME/jre/lib/security/cacerts java Properties javax.net.ssl.keyStore* javax.net.ssl.trustStore* javax.net.debug to activate debugging javax.net.ssl.*trust*StoreType from java 9 onwards keyStoreType is pkcs12. Before it was jks. # assumes that .crt and .key already exist ## 1. create pem with full chain of trust cat my.crt > full-chain.crt ## 2. create Keystore in .p12 format from .key and .crt. To pass prompt use -passin -passout ### you may want to set keystorepassword=privatekeypassword because some libraries assume that openssl pkcs12 -export -in full-chain.crt -inkey my.key -out keystore.p12 ## 3. create Truststore (add .crt) - which trust the cert and use changeit as password for the truststore openssl pkcs12 -export -nokeys -in full-chain.crt -out truststore.p12 ## (not recommended) keystore as jks. convert p12 to jks keytool -importkeystore -srckeystore keystore.p12 -destkeystore keystore.jks -srcstoretype pkcs12 -deststoretype jks ## (not recommended) truststore as jks keytool -keystore truststore.jks -alias full-chain.crt -import -file full-chain.crt -noprompt -storepass changeit # read p12 truststore PASSWORD=\"bla\" openssl pkcs12 -info -in truststore.p12 -passin pass:$PASSWORD # read p12 keystore # if -nodes is not set openssl will prompt \"Enter PEM pass phrase\" which will then used to encryp the private key openssl pkcs12 -info -in keystore.p12 -passin pass:$PASSWORD -nodes create full chain pem public cert cat cert.pem intermediate-ca.pem root-ca.pem cert-fullchain.pem Or while exporting the cert with chrome (or other browser) choose as filetype certficate-chain instead of certificat tools mkcert mkcert vscode opensslutils opensslutils trusted certficcate format There is a difference between: BEGIN/END CERTIFICATE BEGIN/END TRUSTED CERTIFICATE see manpage of x509 for further information.","title":"Certificates and encryption"},{"location":"certificates/#certificates-and-encryption","text":"","title":"Certificates and encryption"},{"location":"certificates/#download-pem-with-cli","text":"openssl s_client -showcerts -connect www.serverfault.com:443 < /dev/null","title":"download pem with cli"},{"location":"certificates/#certificate-expiration","text":"validity period: when this period ends the certificate and all derived certs are expired renewal period: period at the end of the validity period where the old certificate is replaced by a new one with a new validity period. The private key stays unchanged. renewal (in contrast to replacing) keeps the private key of a certificate and only recreate the public certificate with a new validity period","title":"certificate expiration"},{"location":"certificates/#wildcard-certificate","text":"star works only for one subdomain.","title":"wildcard certificate"},{"location":"certificates/#file-types","text":".pem: contains as first line: -----BEGIN CERTIFICATE----- can be a private key or a public certificate can be read with text editor but is base64 encoded or with: openssl x509 -in cert.pem -text .der (.cer) binary encoded x509 certificate .key private key in PEM format .cert;.cer;.crt: certificate with private key (DER or PEM) .p12 - pkcs12 container which contains private key and public certificate .pfx - pkcs12 .csr (certificate signing request): temporary","title":"file types"},{"location":"certificates/#more-on-pem","text":"A PEM file looks like this: -----BEGIN [TYPE OF DATA]----- [Base64 encoded binary data] -----END [TYPE OF DATA]----- The [TYPE OF DATA] part indicates the format of the binary data within the base64 block. This is where PKCS#1, PKCS#8, and OpenSSH formats come into play for private keys. PKCS#1 PEM Format (-----BEGIN RSA PRIVATE KEY-----) - : PKCS#1 (Public-Key Cryptography Standards #1) specifically defines the syntax for RSA public and private keys PKCS#8 PEM Format (-----BEGIN PRIVATE KEY----- or -----BEGIN ENCRYPTED PRIVATE KEY-----) - PKCS#8 (Private-Key Information Syntax Specification) is a more generic standard for storing private keys for any public-key algorithm, not just RSA. OpenSSH Private Key Format (-----BEGIN OPENSSH PRIVATE KEY-----) - This is a custom, OpenSSH-specific format, introduced in OpenSSH 7.8 (released in 2017).","title":"more on pem"},{"location":"certificates/#default-locations","text":"","title":"default locations"},{"location":"certificates/#linux","text":"Source: Where does Go lang search for certificates Debian/Ubuntu/Gentoo etc: \"/etc/ssl/certs/ca-certificates.crt\" Fedora/RHEL 6: \"/etc/pki/tls/certs/ca-bundle.crt\" OpenSUSE: \"/etc/ssl/ca-bundle.pem\" OpenELEC: \"/etc/pki/tls/cacert.pem\" CentOS/RHEL 7: \"/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\" Alpine Linux: \"/etc/ssl/cert.pem\"","title":"Linux"},{"location":"certificates/#centosrhel-7","text":"Source to add a certficate as ca add the pem or der file to /etc/pki/ca-trust/source/anchors/ (higher precedence) or /usr/share/pki/ca-trust-source/anchors/ (lower precedence) and run update-ca-trust if you ever feel tempted to edit /etc/pki/ca-trust/extracted/ directly (instead of generating it) double check if you use the right format BEGIN/END CERTIFICATE vs. BEGIN/END TRUSTED CERTIFICATE and the file right name. See the following listing. # list certs trust list # add pem or der certs to: /etc/pki/ca-trust/source/anchors/ or /usr/share/pki/ca-trust-source/anchors/ update-ca-trust # see manpage: https://www.unix.com/man-page/centos/8/update-ca-trust/ # updates certs in the following folders: ## /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt ## /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem ## /etc/pki/ca-trust/extracted/pem/email-ca-bundle.pem ## /etc/pki/ca-trust/extracted/pem/objsign-ca-bundle.pem ## /etc/pki/ca-trust/extracted/java/cacerts ## and then in /etc/pki/tls/certs/ca-bundle.crt","title":"CentOS/RHEL 7"},{"location":"certificates/#opensuse","text":"add pem certs to /usr/share/pki/anchors run sudo update-ca-certificates","title":"(open)Suse"},{"location":"certificates/#create-pem-from-p12","text":"PASSWORD=\"test\" ## deprecared: -nodes = \"no DES\" no password for private keys ## --d - private key will not be encrypted (no password for private keys neeed) # ca.crt (PEM) openssl pkcs12 -in keystore.p12 -nodes -out out/ca.crt -cacerts -nokeys -passin pass:$PASSWORD # tls.crt (PEM) openssl pkcs12 -in keystore.p12 -nodes -out out/tls.crt -nokeys -passin pass:$PASSWORD # tls.key (PEM) openssl pkcs12 -in keystore.p12 -nodes -out out/tls.key -nocerts -passin pass:$PASSWORD","title":"create pem from p12"},{"location":"certificates/#generate-certificate-and-private-key-in-pem-format-self-signed","text":"# create ## with subject prompt openssl req -x509 -newkey rsa:4096 -keyout ca.demo.key -out ca.demo.crt ## without subject prompt openssl req -x509 -newkey rsa:4096 -keyout ca.demo.key -out ca.demo.crt -subj \"/CN=ca.demo.de\" ## without encrypted private key and without subject prompt openssl req --nodes -x509 -newkey rsa:4096 -keyout my.key -out my.crt -subj \"/CN=ca.demo.de\" # read openssl x509 -in my.crt -text -noout openssl rsa -in my.crt -text -noout","title":"generate certificate and private key in pem format (self signed)"},{"location":"certificates/#generate-certificate-and-private-key-in-pem-format-ca-signed","text":"# generate ca key and cert in pem format (or use existing CA key and cert) openssl req -x509 -newkey rsa:4096 -keyout ca.demo.key -out ca.demo.crt -subj \"/CN=ca.demo.de\" # create key for client openssl genrsa -out client.demo.key 2048 # create signing requets for a client (unsigned certificate) openssl req -new -key client.demo.key -subj \"/CN=client.demo.de\" -out client.demo.csr # create signed client certificate openssl x509 -req -in client.demo.csr -signkey client.demo.key -out client.demo.crt","title":"generate certificate and private key in pem format (ca signed)"},{"location":"certificates/#download-leaf-cert-from-url","text":"HOSTNAME= PORT= openssl s_client -showcerts -connect ${HOSTNAME}:${PORT} /dev/null|openssl x509 -outform PEM >leafcert-${HOSTNAME}.pem","title":"download leaf cert from url"},{"location":"certificates/#pem-headers-examples","text":"-----BEGIN CERTIFICATE----- -----BEGIN CERTIFICATE REQUEST----- -----BEGIN RSA PRIVATE KEY----- -----BEGIN ENCRYPTED PRIVATE KEY-----","title":"pem headers examples"},{"location":"certificates/#ssh-key","text":"To generate ssh keys run ssh-keygen which generates ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa. The private and public key do no use the pem format. The name of the owner is at the end of the pub file.","title":"ssh key"},{"location":"certificates/#java-truststore-and-keystore","text":"A Java keystore holds your private Keys + and your certificate A Java truststore holds the certificates you trust when you connect yourself to a server You may need to assure that the truststore contains the whole trust-chain otherwise java may not accept a connection to the server .fileending (Format): .jks (JSK), .keystore (JSK), .p12 (PKCS12) pkcs12 is the recommended format for keystores, not jks Javas bundled truststore resides in $JAVA_HOME/jre/lib/security/jssecacerts and $JAVA_HOME/jre/lib/security/cacerts java Properties javax.net.ssl.keyStore* javax.net.ssl.trustStore* javax.net.debug to activate debugging javax.net.ssl.*trust*StoreType from java 9 onwards keyStoreType is pkcs12. Before it was jks. # assumes that .crt and .key already exist ## 1. create pem with full chain of trust cat my.crt > full-chain.crt ## 2. create Keystore in .p12 format from .key and .crt. To pass prompt use -passin -passout ### you may want to set keystorepassword=privatekeypassword because some libraries assume that openssl pkcs12 -export -in full-chain.crt -inkey my.key -out keystore.p12 ## 3. create Truststore (add .crt) - which trust the cert and use changeit as password for the truststore openssl pkcs12 -export -nokeys -in full-chain.crt -out truststore.p12 ## (not recommended) keystore as jks. convert p12 to jks keytool -importkeystore -srckeystore keystore.p12 -destkeystore keystore.jks -srcstoretype pkcs12 -deststoretype jks ## (not recommended) truststore as jks keytool -keystore truststore.jks -alias full-chain.crt -import -file full-chain.crt -noprompt -storepass changeit # read p12 truststore PASSWORD=\"bla\" openssl pkcs12 -info -in truststore.p12 -passin pass:$PASSWORD # read p12 keystore # if -nodes is not set openssl will prompt \"Enter PEM pass phrase\" which will then used to encryp the private key openssl pkcs12 -info -in keystore.p12 -passin pass:$PASSWORD -nodes","title":"Java truststore and keystore"},{"location":"certificates/#create-full-chain-pem-public-cert","text":"cat cert.pem intermediate-ca.pem root-ca.pem cert-fullchain.pem Or while exporting the cert with chrome (or other browser) choose as filetype certficate-chain instead of certificat","title":"create full chain pem public cert"},{"location":"certificates/#tools","text":"","title":"tools"},{"location":"certificates/#mkcert","text":"mkcert","title":"mkcert"},{"location":"certificates/#vscode-opensslutils","text":"opensslutils","title":"vscode opensslutils"},{"location":"certificates/#trusted-certficcate-format","text":"There is a difference between: BEGIN/END CERTIFICATE BEGIN/END TRUSTED CERTIFICATE see manpage of x509 for further information.","title":"trusted certficcate format"},{"location":"citrix/","text":"Citrix troubleshoot keyboard layout The linux client might use the false keyboard layout if the system language contradicts with the layout. To change this behavio edit the file ~/.ICAClient/All_Regions.ini. After that you might need to restart the Citrix Client. # open folder code ~/.ICAClient/All_Regions.ini # set # KeyboardLayout=German maximize window issue Maximizing windows on linux might lead to a white bar on the lower side of the monitor or to duplication of the image. It also seems to be connected to a loose of connection between keyboard and citrix. Citrix does not support wayland ( 12.11.2024 ), but only X11 or X.Org. There is a preview which supports wayland. The solution is not tested because I used a internal containerized version of citrix which just worked on linux. Map Mac keyboard layout to windows Working solution ss described in your obsidian notes.","title":"Citrix troubleshoot"},{"location":"citrix/#citrix-troubleshoot","text":"","title":"Citrix troubleshoot"},{"location":"citrix/#keyboard-layout","text":"The linux client might use the false keyboard layout if the system language contradicts with the layout. To change this behavio edit the file ~/.ICAClient/All_Regions.ini. After that you might need to restart the Citrix Client. # open folder code ~/.ICAClient/All_Regions.ini # set # KeyboardLayout=German","title":"keyboard layout"},{"location":"citrix/#maximize-window-issue","text":"Maximizing windows on linux might lead to a white bar on the lower side of the monitor or to duplication of the image. It also seems to be connected to a loose of connection between keyboard and citrix. Citrix does not support wayland ( 12.11.2024 ), but only X11 or X.Org. There is a preview which supports wayland. The solution is not tested because I used a internal containerized version of citrix which just worked on linux.","title":"maximize window issue"},{"location":"citrix/#map-mac-keyboard-layout-to-windows","text":"Working solution ss described in your obsidian notes.","title":"Map Mac keyboard layout to windows"},{"location":"cockroach/","text":"minimal notes on cockroach distributed relational database (acid) transparent sharding (which can be controlled if needed: Topology Patterns ) in contrast to postgres it does not use active-passive replication (uses quorum) CAP theorem: is a cp database uses postgres \"interface\": ORM, SQL dialect, etc. runs on kubernetes serverless offer in cloud (uses k8s under the hood)","title":"minimal notes on cockroach"},{"location":"cockroach/#minimal-notes-on-cockroach","text":"distributed relational database (acid) transparent sharding (which can be controlled if needed: Topology Patterns ) in contrast to postgres it does not use active-passive replication (uses quorum) CAP theorem: is a cp database uses postgres \"interface\": ORM, SQL dialect, etc. runs on kubernetes serverless offer in cloud (uses k8s under the hood)","title":"minimal notes on cockroach"},{"location":"code-analysis/","text":"code analysis Analysis Tools Website lists a comprehensive set of analysis tools. You may find additional tools on the website of shields which provide the little badges for markdown files, which are often found on github. Code quality Quality ands security SonarQube or SonarCloud codeclimate semgrep Security lgtm (is joining github) Analyse Containers, Filesystem, Git Repos Trivy Security for dependecies, code, containers and infrastructure-code Snyk check git repos for secrets gitguardian security check for used libraries dependabot kiks scans IaC code for vulnerabilities Test coverage (may need additional client library which produce the reports in ci) codecov coverall Tools may need existing reports which are build in ci pipepline java jacoco cobertura License check: fossa Uptime check: uptimerobot python security bandit bash ShellCheck","title":"code analysis"},{"location":"code-analysis/#code-analysis","text":"Analysis Tools Website lists a comprehensive set of analysis tools. You may find additional tools on the website of shields which provide the little badges for markdown files, which are often found on github. Code quality Quality ands security SonarQube or SonarCloud codeclimate semgrep Security lgtm (is joining github) Analyse Containers, Filesystem, Git Repos Trivy Security for dependecies, code, containers and infrastructure-code Snyk check git repos for secrets gitguardian security check for used libraries dependabot kiks scans IaC code for vulnerabilities Test coverage (may need additional client library which produce the reports in ci) codecov coverall Tools may need existing reports which are build in ci pipepline java jacoco cobertura License check: fossa Uptime check: uptimerobot python security bandit bash ShellCheck","title":"code analysis"},{"location":"codeassistant/","text":"Setup local ai code assistent Integrate you local ai code assistent into your IDE with Continue . Use a local model server like Ollama or InstructLab to host a granite code model which can be used by Continue. ## use ollama # intall brew install ollama # run ollama server ollama serve # pull granite model in other terminal # see list of models (and tags): https://ollama.com/library ollama pull granite-code:20b # ~11GB ollama pull granite-code:8b # ~4,6GB ollama pull granite3-dense:2b # ~1,6GB # install continue in vscode code --install-extension continue.continue To use it in vscode Continue extensions edit code $HOME/.continue/config.json or use or use ui of extension in vscode to add new sessions and select ollama with auto detect. \"models\": [ { \"title\": \"Granite Code 20b\", \"provider\": \"ollama\", \"model\": \"granite-code:20b\" } ], ... \"tabAutocompleteModel\": { \"title\": \"Granite Code 8b\", \"provider\": \"ollama\", \"model\": \"granite-code:8b\" }, notes on downloaded models and disk size ollama LLM are saved here: /usr/share/ollama/.ollama/model or ~/.ollama/models list downloaded models: ollama list remove model from disk: ollama rm MODEL instruct lab LLM are saved here: ~/. cache/instructlab/models/","title":"Setup local ai code assistent"},{"location":"codeassistant/#setup-local-ai-code-assistent","text":"Integrate you local ai code assistent into your IDE with Continue . Use a local model server like Ollama or InstructLab to host a granite code model which can be used by Continue. ## use ollama # intall brew install ollama # run ollama server ollama serve # pull granite model in other terminal # see list of models (and tags): https://ollama.com/library ollama pull granite-code:20b # ~11GB ollama pull granite-code:8b # ~4,6GB ollama pull granite3-dense:2b # ~1,6GB # install continue in vscode code --install-extension continue.continue To use it in vscode Continue extensions edit code $HOME/.continue/config.json or use or use ui of extension in vscode to add new sessions and select ollama with auto detect. \"models\": [ { \"title\": \"Granite Code 20b\", \"provider\": \"ollama\", \"model\": \"granite-code:20b\" } ], ... \"tabAutocompleteModel\": { \"title\": \"Granite Code 8b\", \"provider\": \"ollama\", \"model\": \"granite-code:8b\" },","title":"Setup local ai code assistent"},{"location":"codeassistant/#notes-on-downloaded-models-and-disk-size","text":"","title":"notes on downloaded models and disk size"},{"location":"codeassistant/#ollama","text":"LLM are saved here: /usr/share/ollama/.ollama/model or ~/.ollama/models list downloaded models: ollama list remove model from disk: ollama rm MODEL","title":"ollama"},{"location":"codeassistant/#instruct-lab","text":"LLM are saved here: ~/. cache/instructlab/models/","title":"instruct lab"},{"location":"codespace/","text":"codespaces / devcontainers Official documentation configure new one with wizard In vscode prompt: > add dev container configuration files See use own Dockerfile If you use a custom dockerifle add prebuilts to keep the startup snappy. Otherwise codespaces will build it all over again each time you open a codespace.","title":"codespaces / devcontainers"},{"location":"codespace/#codespaces-devcontainers","text":"Official documentation","title":"codespaces / devcontainers"},{"location":"codespace/#configure-new-one-with-wizard","text":"In vscode prompt: > add dev container configuration files See","title":"configure new one with wizard"},{"location":"codespace/#use-own-dockerfile","text":"If you use a custom dockerifle add prebuilts to keep the startup snappy. Otherwise codespaces will build it all over again each time you open a codespace.","title":"use own Dockerfile"},{"location":"confidential_computing/","text":"Confidential Computing states of data data in motion/transit data at rest data in use encryption types Point-to-point encryption - transport encryption End-to-end encryption - message encryption cpu Can be encrypted with Secure Encrypted Virtualization (SEV).","title":"Confidential Computing"},{"location":"confidential_computing/#confidential-computing","text":"","title":"Confidential Computing"},{"location":"confidential_computing/#states-of-data","text":"data in motion/transit data at rest data in use","title":"states of data"},{"location":"confidential_computing/#encryption-types","text":"Point-to-point encryption - transport encryption End-to-end encryption - message encryption","title":"encryption types"},{"location":"confidential_computing/#cpu","text":"Can be encrypted with Secure Encrypted Virtualization (SEV).","title":"cpu"},{"location":"cors/","text":"Cross-Origin Resource Sharing (CORS) CORS is an HTTP-header based mechanism that allows a server to indicate any origins (domain, scheme, or port) other than its own from which a browser should permit loading resources. Source preflight Send an http request before the acutal request to check if the actual request would be allowed CORS-wise. Header The Access-Control-Allow-Origin will be set server-side and need to contain the domain of the website which calls the service. Is not needed if both client and server are raeachable under the same url.","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"cors/#cross-origin-resource-sharing-cors","text":"CORS is an HTTP-header based mechanism that allows a server to indicate any origins (domain, scheme, or port) other than its own from which a browser should permit loading resources. Source","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"cors/#preflight","text":"Send an http request before the acutal request to check if the actual request would be allowed CORS-wise.","title":"preflight"},{"location":"cors/#header","text":"The Access-Control-Allow-Origin will be set server-side and need to contain the domain of the website which calls the service. Is not needed if both client and server are raeachable under the same url.","title":"Header"},{"location":"dns/","text":"DNS dig # see which nameservers are involved dig weyrich.dev +trace # see information on domain dig weyrich.dev +all # use specific dns to resolve url dig @ns1-09.azure-dns.com bla.example.cloud DNS Zones Is part of a dns tree e.g. bla.examples.com and blubb.example.com can be two different zones. record types NS: Name Server Resource Record CNAME vs ALIAS apex domain vs subdomain An apex domain also called root domain: weyrich.dev Subdomain: bla.weyrich.dev hint: setting ns When dig says BAD (HORIZONTAL) REFERRAL you probably created a loop in a ns entry. Check if you used the correct address for nameservers of the subdomain. In doubt double check the NS addresses. E.g. azure uses different nameserver and randomly assign four to one dns zone. Add domain temporarily in firefox Firefox plugin to add temp domains","title":"DNS"},{"location":"dns/#dns","text":"","title":"DNS"},{"location":"dns/#dig","text":"# see which nameservers are involved dig weyrich.dev +trace # see information on domain dig weyrich.dev +all # use specific dns to resolve url dig @ns1-09.azure-dns.com bla.example.cloud","title":"dig"},{"location":"dns/#dns-zones","text":"Is part of a dns tree e.g. bla.examples.com and blubb.example.com can be two different zones.","title":"DNS Zones"},{"location":"dns/#record-types","text":"NS: Name Server Resource Record","title":"record types"},{"location":"dns/#cname-vs-alias","text":"","title":"CNAME vs ALIAS"},{"location":"dns/#apex-domain-vs-subdomain","text":"An apex domain also called root domain: weyrich.dev Subdomain: bla.weyrich.dev","title":"apex domain vs subdomain"},{"location":"dns/#hint-setting-ns","text":"When dig says BAD (HORIZONTAL) REFERRAL you probably created a loop in a ns entry. Check if you used the correct address for nameservers of the subdomain. In doubt double check the NS addresses. E.g. azure uses different nameserver and randomly assign four to one dns zone.","title":"hint: setting ns"},{"location":"dns/#add-domain-temporarily-in-firefox","text":"Firefox plugin to add temp domains","title":"Add domain temporarily in firefox"},{"location":"flyway/","text":"Flyway motivation Flyway aims to streamline the database migration process, making it faster and less error-prone for development teams. It integrates perfectly with with CI/CD pipelines. Safe and reliable deployments : Flyway ensures that database schema changes are applied in the correct order across all environments (development, testing, production). This reduces the risk of errors and unexpected behavior. Safety checks : Flyway verifies if the scripts have already been applied to the database before running them again. This prevents accidental re-runs that could cause issues. Rollback support : If a migration causes problems, Flyway allows you to easily rollback to a previous state. alternatives Liquibase: Like flyway but uses xml for configuration manual execution of sql scripts basics Write migration scripts Execute flyway commands write migration scripts Flyway uses versioned filenames to ensure migrations are applied in the correct order. Typically: V<version number>__<description>.sql (e.g. V001__create_users_table.sql). Inside the script file, write your SQL statements that modify the database schema. This could include creating tables, adding columns, or altering existing structures. execute flyway commands migrate : This command instructs Flyway to scan for new or pending migrations and apply them to your database. info : This command displays information about the current state of Flyway, such as which migrations have been applied and the Flyway version. baseline : This command is typically used for initial setup. It helps Flyway create the internal tracking table in your database to manage the migration history. clean : This command removes applied migrations from your database, essentially rolling back all changes. Use with caution!","title":"Flyway"},{"location":"flyway/#flyway","text":"","title":"Flyway"},{"location":"flyway/#motivation","text":"Flyway aims to streamline the database migration process, making it faster and less error-prone for development teams. It integrates perfectly with with CI/CD pipelines. Safe and reliable deployments : Flyway ensures that database schema changes are applied in the correct order across all environments (development, testing, production). This reduces the risk of errors and unexpected behavior. Safety checks : Flyway verifies if the scripts have already been applied to the database before running them again. This prevents accidental re-runs that could cause issues. Rollback support : If a migration causes problems, Flyway allows you to easily rollback to a previous state.","title":"motivation"},{"location":"flyway/#alternatives","text":"Liquibase: Like flyway but uses xml for configuration manual execution of sql scripts","title":"alternatives"},{"location":"flyway/#basics","text":"Write migration scripts Execute flyway commands","title":"basics"},{"location":"flyway/#write-migration-scripts","text":"Flyway uses versioned filenames to ensure migrations are applied in the correct order. Typically: V<version number>__<description>.sql (e.g. V001__create_users_table.sql). Inside the script file, write your SQL statements that modify the database schema. This could include creating tables, adding columns, or altering existing structures.","title":"write migration scripts"},{"location":"flyway/#execute-flyway-commands","text":"migrate : This command instructs Flyway to scan for new or pending migrations and apply them to your database. info : This command displays information about the current state of Flyway, such as which migrations have been applied and the Flyway version. baseline : This command is typically used for initial setup. It helps Flyway create the internal tracking table in your database to manage the migration history. clean : This command removes applied migrations from your database, essentially rolling back all changes. Use with caution!","title":"execute flyway commands"},{"location":"git/","text":"Git ssh-agent Even with the agent running, Git needs to know it should contact the agent to save those passwords. Make sure your ~/.ssh/config has this global setting (or a specific one for the git server): Host * AddKeysToAgent yes initial setup mac ## base config MAIL_ADDRESS=\"jan.weyrich@example.de\" git config --global user.name \"Jan Weyrich\" git config --global user.email \"$MAIL_ADDRESS\" ## alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status ## generate ssh key ssh-keygen -t ed25519 -C \"$MAIL_ADDRESS\" # add ssh key agent for passwort prompt # see https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent ## gpg brew install gpg2 gnupg pinentry-mac gpg --full-generate-key # config for git gpg -K --keyid-format SHORT export GPG_SEC_ID=8627004B gpg --armor --export $GPG_SEC_ID > ~/gpg-public.pem git config --global user.signingkey $GPG_SEC_ID git config --global gpg.program $(which gpg) git config --global commit.gpgsign true echo 'export GPG_TTY=$(tty)' >> ~/.zshrc source ~/.zshrc # for pw prompt see here: https://unixb0y.de/blog/articles/2019-01/gpg-password-macos-keychain echo 'use-agent' > ~/.gnupg/gpg.conf echo \"pinentry-program /opt/homebrew/bin/pinentry-mac\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent # check with echo \"test\" | gpg --clearsign # if you got: gpg: signing failed: No pinentry # you may want to try with: echo \"/opt/homebrew/bin/pinentry\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent ## save passphrase for twelve hours echo \"default-cache-ttl 43200\" >> ~/.gnupg/gpg-agent.conf linux # config export MAIL_ADDRESS=\"jan.weyrich@example.de\" git config --global user.name \"Jan Weyrich\" git config --global user.email \"$MAIL_ADDRESS\" # alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status # ssh ssh-keygen -t ed25519 -C \"$MAIL_ADDRESS\" echo 'eval `ssh-agent -s`' >> ~/.bashrc && source ~/.bashrc ssh-add ~/.ssh/*_rsa # gpg gpg --full-generate-key gpg --armor --export 7C37CF3A > ~/gpg-public.pem git config --global gpg.program $(which gpg) git config --global commit.gpgsign true ## save passphrase for twelve hours echo \"default-cache-ttl 43200\" > ~/.gnupg/gpg-agent.conf ## view ssh pub key cat ~/.ssh/id_rsa.pub ## view gpg pub key gpg --armor --export 7C37CF3A > ~/gpg-public.pem https and password manager windows # check existing methods git help -a | grep credential # there options might be available ## credential Retrieve and store user ## credential-cache Helper to temporarily store passwords in memory ## credential-store Helper to store credentials on disk in plaintext (~/.git-credentials) # credential-cache git config --global credential.helper \"cache --timeout 30000\" # credential-store git config --global credential.helper \"store\" # you might need to install manager-core to use it # and then set it via # git config --global credential.helper \"manager-core\" Workaround if no credential manager works (in windows probably): git config --global credential.helper manager stores the credentials in plain text file ~/.git-credentials the next time you log in. The format in the file is like this: https://user:password@git.example.de. It only works, when using https. Sometimes the password is not saved after the frist prompt, in this case you can add the following to ~/.gitconfig: [credential \"https://git.example.de\"] provider = generic configure output behavior (disable pager) git config --global pager.branch false see here wording Good references Usage of index explained index = staging area working tree = working directory (branch) head: pointer to the newest commit on a branch tag: fixed pointer to commit HEAD: movable pointer which usually points to the head/tip of a branch. The next commit will take action on the commit which HEAD points to. reset (HEAD vs index vs working tree) Reset Soft (HEAD only) Mixed (Head and index) Hard (Head, index, Working Tree) clear working directory (with untracked files) With reset: To last commit including ignored files: git reset --hard && git clean -dfx To last commit: git reset --hard && git clean -df With stash. Allows to revert changes, but not recommended with many files: To last commit including ignored files: git stash --all To last commit: git stash --include-untracked Permanently deleting files: git stash drop Revert changes: git stash pop checkout remote branch git fetch git switch BRANCH_NAME # or git checkouto -B BRANCH_NAME origin/BRANCH_NAME revert or delete commits interactive wizard # rework last commit () git reset HEAD^ # delete last commit git reset --hard HEAD^ change old commit Source git rebase --interactive HEAD~10 # mark commit with 'e' or 'edit' # change files and add to stage git add <stuff> git commit --amend --no-edit git rebase --continue clean files in history If you want to delete passwords or other sensible stuff from git history use bfg . It does not touch your current commit (HEAD) just older commits. # easy install (instead of working with bfg.jar) brew install bfg # example how to erase password # in git repo directory echo \"supersecret\" > passwords.txt bfg --replace-text passwords.txt # push changed git history git push --force git submodules # init submodule git submodule update --init --recursive # point submodule to HEAD git submodule update --remote --merge certs ignore: via system env variable: env GIT_SSL_NO_VERIFY=true or via cli: git config --global http.sslVerify \"false\" Windows: git config --global http.sslbackend channel tag # create tag and push (beware push only tags not commits) git tag 1.2.3 #git tag -a 1.2.3 -m \"message\" git push --tags # delte remote tag git push --delete oririn 1.2.3 # delete local git tag -d 1.2.3 ssh generate keys ssh-keygen -t ed25519 ssh remember passphrase In bash: echo 'eval `ssh-agent -s`' >> ~/.bashrc && source ~/.bashrc ssh-add ~/.ssh/*_rsa For WSL: sudo apt install keychain echo \"eval `keychain --quiet --eval --agents ssh id_rsa`\" >> ~/.bashrc && source ~/.bashrc work with gpg Sign commitsa # for one commit git co -S -m \"\" # for all git config --global commit.gpgsign true # in vscode add to settings json: \"git.enableCommitSigning\": true create and configure gpg keys: Gitlab doc Save passphrase for twelve hours echo \"default-cache-ttl 43200\" > ~/.gnupg/gpg-agent.conf","title":"Git"},{"location":"git/#git","text":"","title":"Git"},{"location":"git/#ssh-agent","text":"Even with the agent running, Git needs to know it should contact the agent to save those passwords. Make sure your ~/.ssh/config has this global setting (or a specific one for the git server): Host * AddKeysToAgent yes","title":"ssh-agent"},{"location":"git/#initial-setup","text":"","title":"initial setup"},{"location":"git/#mac","text":"## base config MAIL_ADDRESS=\"jan.weyrich@example.de\" git config --global user.name \"Jan Weyrich\" git config --global user.email \"$MAIL_ADDRESS\" ## alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status ## generate ssh key ssh-keygen -t ed25519 -C \"$MAIL_ADDRESS\" # add ssh key agent for passwort prompt # see https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent ## gpg brew install gpg2 gnupg pinentry-mac gpg --full-generate-key # config for git gpg -K --keyid-format SHORT export GPG_SEC_ID=8627004B gpg --armor --export $GPG_SEC_ID > ~/gpg-public.pem git config --global user.signingkey $GPG_SEC_ID git config --global gpg.program $(which gpg) git config --global commit.gpgsign true echo 'export GPG_TTY=$(tty)' >> ~/.zshrc source ~/.zshrc # for pw prompt see here: https://unixb0y.de/blog/articles/2019-01/gpg-password-macos-keychain echo 'use-agent' > ~/.gnupg/gpg.conf echo \"pinentry-program /opt/homebrew/bin/pinentry-mac\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent # check with echo \"test\" | gpg --clearsign # if you got: gpg: signing failed: No pinentry # you may want to try with: echo \"/opt/homebrew/bin/pinentry\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent ## save passphrase for twelve hours echo \"default-cache-ttl 43200\" >> ~/.gnupg/gpg-agent.conf","title":"mac"},{"location":"git/#linux","text":"# config export MAIL_ADDRESS=\"jan.weyrich@example.de\" git config --global user.name \"Jan Weyrich\" git config --global user.email \"$MAIL_ADDRESS\" # alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status # ssh ssh-keygen -t ed25519 -C \"$MAIL_ADDRESS\" echo 'eval `ssh-agent -s`' >> ~/.bashrc && source ~/.bashrc ssh-add ~/.ssh/*_rsa # gpg gpg --full-generate-key gpg --armor --export 7C37CF3A > ~/gpg-public.pem git config --global gpg.program $(which gpg) git config --global commit.gpgsign true ## save passphrase for twelve hours echo \"default-cache-ttl 43200\" > ~/.gnupg/gpg-agent.conf ## view ssh pub key cat ~/.ssh/id_rsa.pub ## view gpg pub key gpg --armor --export 7C37CF3A > ~/gpg-public.pem","title":"linux"},{"location":"git/#https-and-password-manager","text":"","title":"https and password manager"},{"location":"git/#windows","text":"# check existing methods git help -a | grep credential # there options might be available ## credential Retrieve and store user ## credential-cache Helper to temporarily store passwords in memory ## credential-store Helper to store credentials on disk in plaintext (~/.git-credentials) # credential-cache git config --global credential.helper \"cache --timeout 30000\" # credential-store git config --global credential.helper \"store\" # you might need to install manager-core to use it # and then set it via # git config --global credential.helper \"manager-core\" Workaround if no credential manager works (in windows probably): git config --global credential.helper manager stores the credentials in plain text file ~/.git-credentials the next time you log in. The format in the file is like this: https://user:password@git.example.de. It only works, when using https. Sometimes the password is not saved after the frist prompt, in this case you can add the following to ~/.gitconfig: [credential \"https://git.example.de\"] provider = generic","title":"windows"},{"location":"git/#configure-output-behavior-disable-pager","text":"git config --global pager.branch false see here","title":"configure output behavior (disable pager)"},{"location":"git/#wording","text":"Good references Usage of index explained index = staging area working tree = working directory (branch) head: pointer to the newest commit on a branch tag: fixed pointer to commit HEAD: movable pointer which usually points to the head/tip of a branch. The next commit will take action on the commit which HEAD points to. reset (HEAD vs index vs working tree) Reset Soft (HEAD only) Mixed (Head and index) Hard (Head, index, Working Tree)","title":"wording"},{"location":"git/#clear-working-directory-with-untracked-files","text":"With reset: To last commit including ignored files: git reset --hard && git clean -dfx To last commit: git reset --hard && git clean -df With stash. Allows to revert changes, but not recommended with many files: To last commit including ignored files: git stash --all To last commit: git stash --include-untracked Permanently deleting files: git stash drop Revert changes: git stash pop","title":"clear working directory (with untracked files)"},{"location":"git/#checkout-remote-branch","text":"git fetch git switch BRANCH_NAME # or git checkouto -B BRANCH_NAME origin/BRANCH_NAME","title":"checkout remote branch"},{"location":"git/#revert-or-delete-commits","text":"interactive wizard # rework last commit () git reset HEAD^ # delete last commit git reset --hard HEAD^","title":"revert or delete commits"},{"location":"git/#change-old-commit","text":"Source git rebase --interactive HEAD~10 # mark commit with 'e' or 'edit' # change files and add to stage git add <stuff> git commit --amend --no-edit git rebase --continue","title":"change old commit"},{"location":"git/#clean-files-in-history","text":"If you want to delete passwords or other sensible stuff from git history use bfg . It does not touch your current commit (HEAD) just older commits. # easy install (instead of working with bfg.jar) brew install bfg # example how to erase password # in git repo directory echo \"supersecret\" > passwords.txt bfg --replace-text passwords.txt # push changed git history git push --force","title":"clean files in history"},{"location":"git/#git-submodules","text":"# init submodule git submodule update --init --recursive # point submodule to HEAD git submodule update --remote --merge","title":"git submodules"},{"location":"git/#certs","text":"ignore: via system env variable: env GIT_SSL_NO_VERIFY=true or via cli: git config --global http.sslVerify \"false\" Windows: git config --global http.sslbackend channel","title":"certs"},{"location":"git/#tag","text":"# create tag and push (beware push only tags not commits) git tag 1.2.3 #git tag -a 1.2.3 -m \"message\" git push --tags # delte remote tag git push --delete oririn 1.2.3 # delete local git tag -d 1.2.3","title":"tag"},{"location":"git/#ssh-generate-keys","text":"ssh-keygen -t ed25519","title":"ssh generate keys"},{"location":"git/#ssh-remember-passphrase","text":"In bash: echo 'eval `ssh-agent -s`' >> ~/.bashrc && source ~/.bashrc ssh-add ~/.ssh/*_rsa For WSL: sudo apt install keychain echo \"eval `keychain --quiet --eval --agents ssh id_rsa`\" >> ~/.bashrc && source ~/.bashrc","title":"ssh remember passphrase"},{"location":"git/#work-with-gpg","text":"Sign commitsa # for one commit git co -S -m \"\" # for all git config --global commit.gpgsign true # in vscode add to settings json: \"git.enableCommitSigning\": true create and configure gpg keys: Gitlab doc Save passphrase for twelve hours echo \"default-cache-ttl 43200\" > ~/.gnupg/gpg-agent.conf","title":"work with gpg"},{"location":"gitlab/","text":"Gitlab Runner Guide how to install a gitlab-runner A gitlab runner can use different executors e.g. shell, docker or kubernetes pipeline templates used by auto devops ansible galaxy auth Ansible-Galaxy does not support (gitlab) oauth tokens in the url. See Issue One solution is to redirect the ssh git link in the ansible galaxy requirements.yaml to https and addtionally add a config for a git credential manager which adds the token for the redirected https url git config --global credential.helper \"store --file https://example.com/ansible-roles git config --global url.\"https://example.com/ansible-roles\".insteadOf \"git@example.com:ansible-roles\" Sources: https://forum.gitlab.com/t/gilab-ci-ansible-galaxy-requirements/51515/4 https://gist.github.com/wtfred/abeee62d6a3e4371dbb4b309322dac30","title":"Gitlab Runner"},{"location":"gitlab/#gitlab-runner","text":"Guide how to install a gitlab-runner A gitlab runner can use different executors e.g. shell, docker or kubernetes pipeline templates used by auto devops","title":"Gitlab Runner"},{"location":"gitlab/#ansible-galaxy-auth","text":"Ansible-Galaxy does not support (gitlab) oauth tokens in the url. See Issue One solution is to redirect the ssh git link in the ansible galaxy requirements.yaml to https and addtionally add a config for a git credential manager which adds the token for the redirected https url git config --global credential.helper \"store --file https://example.com/ansible-roles git config --global url.\"https://example.com/ansible-roles\".insteadOf \"git@example.com:ansible-roles\" Sources: https://forum.gitlab.com/t/gilab-ci-ansible-galaxy-requirements/51515/4 https://gist.github.com/wtfred/abeee62d6a3e4371dbb4b309322dac30","title":"ansible galaxy auth"},{"location":"gitpod/","text":"Gitpod Dashboard Gitpodifying \u2014 The Ultimate Guide start a workspace To start workspace in gitpod: use gitpod firefox or chrome extension prefix git repository url with: gitpod.io/# configure workspace (gitpodify your project) .gitpod.yml in the opened repository (run gp init to create one) .gitpod.yaml in the central definitely-gp repository automagically inferred gitpod config To scaffold a .gitpod.yml use gitpod cli in a started gitpod workspace: gp init configure used container image configure custom image in .gitpod.Dockerfile and reference it in .gitpod.yml: configure image from Dockerhub: image: image:tag if non image is set the default gitpod workspace image is used # your own image image: file: .gitpod.Dockerfile # dockerhub image image: node:alpine For compatibility reasons you may want to use a gitpod workspace image as base image configure prebuild and start command Start tasks contains different kinds of commands (before (command), init (command), (plain) command). are executed in a defined order and only in defined start modes. Init command is run when creating a workspace (container) not when restarting or creating a snapshot (Plain) commands are run when starting a workspace (container) You can configure that the init command is run every commit. This is called prebuild and can be configured with git webhooks or vscode extensions. tasks: - init: | yarn yarn build command: yarn dev --host 0.0.0.0 # to execute task in parallel just add an additional entry to the tasks list (blocking is currently not natively supported) - command: echo \"hello concurrency, i do not need yarn build so i can be run concurrently\" configure used extension Official documentation To add a extension just go to the extension tab, click on the extension you want to add and click 'Add to .gitpod.yml' from the menu symbol. To view build in extension type @builtin in the extension tab search bar.","title":"Gitpod"},{"location":"gitpod/#gitpod","text":"Dashboard Gitpodifying \u2014 The Ultimate Guide","title":"Gitpod"},{"location":"gitpod/#start-a-workspace","text":"To start workspace in gitpod: use gitpod firefox or chrome extension prefix git repository url with: gitpod.io/#","title":"start a workspace"},{"location":"gitpod/#configure-workspace-gitpodify-your-project","text":".gitpod.yml in the opened repository (run gp init to create one) .gitpod.yaml in the central definitely-gp repository automagically inferred gitpod config To scaffold a .gitpod.yml use gitpod cli in a started gitpod workspace: gp init","title":"configure workspace (gitpodify your project)"},{"location":"gitpod/#configure-used-container-image","text":"configure custom image in .gitpod.Dockerfile and reference it in .gitpod.yml: configure image from Dockerhub: image: image:tag if non image is set the default gitpod workspace image is used # your own image image: file: .gitpod.Dockerfile # dockerhub image image: node:alpine For compatibility reasons you may want to use a gitpod workspace image as base image","title":"configure used container image"},{"location":"gitpod/#configure-prebuild-and-start-command","text":"Start tasks contains different kinds of commands (before (command), init (command), (plain) command). are executed in a defined order and only in defined start modes. Init command is run when creating a workspace (container) not when restarting or creating a snapshot (Plain) commands are run when starting a workspace (container) You can configure that the init command is run every commit. This is called prebuild and can be configured with git webhooks or vscode extensions. tasks: - init: | yarn yarn build command: yarn dev --host 0.0.0.0 # to execute task in parallel just add an additional entry to the tasks list (blocking is currently not natively supported) - command: echo \"hello concurrency, i do not need yarn build so i can be run concurrently\"","title":"configure prebuild and start command"},{"location":"gitpod/#configure-used-extension","text":"Official documentation To add a extension just go to the extension tab, click on the extension you want to add and click 'Add to .gitpod.yml' from the menu symbol. To view build in extension type @builtin in the extension tab search bar.","title":"configure used extension"},{"location":"gradle-memory-aid/","text":"Gradle memory aid gradle commands vs. maven commands Maven Command Gradle Command Explanation mvn clean gradle clean Cleans the build artifacts and temporary files. mvn compile gradle compileJava Compiles the Java source code. mvn test gradle test Runs unit tests. mvn package gradle assemble Packages the compiled code into a distributable. mvn install gradle install Installs the artifact in the local repository. mvn deploy gradle publish Publishes the artifact to a remote repository. mvn dependency:tree gradle dependencies Displays project dependencies as a tree. mvn spring-boot:run gradle bootRun Runs a Spring Boot application. mvn clean install gradle clean build Cleans and then installs the artifact. mvn tomcat:run gradle bootRun Runs a Tomcat server with the web application. mvn site gradle site Generates project site documentation. mvn eclipse:eclipse gradle eclipse Generates Eclipse IDE project files. mvn dependency:purge-local-repository N/A Removes project dependencies from local repository. mvn validate gradle check Validates the project (syntax, configurations). troubleshoot IDLE/Lock problem Error: # command gradle assemble # error message Starting a Gradle Daemon, 1 incompatible and 3 stopped Daemons could not be reused, use --status for details FAILURE: Build failed with an exception. * What went wrong: Gradle could not start your build. > Cannot create service of type BuildSessionActionExecutor using method LauncherServices$ToolingBuildSessionScopeServices.createActionExecutor() as there is a problem with parameter #21 of type FileSystemWatchingInformation. > Cannot create service of type BuildLifecycleAwareVirtualFileSystem using method VirtualFileSystemServices$GradleUserHomeServices.createVirtualFileSystem() as there is a problem with parameter #7 of type GlobalCacheLocations. > Cannot create service of type GlobalCacheLocations using method GradleUserHomeScopeServices.createGlobalCacheLocations() as there is a problem with parameter #1 of type List<GlobalCache>. > Could not create service of type FileAccessTimeJournal using GradleUserHomeScopeServices.createFileAccessTimeJournal(). > Timeout waiting to lock journal cache (/Users/XXX/.gradle/caches/journal-1). It is currently in use by another Gradle instance. Owner PID: 33272 Our PID: 33401 Owner Operation: Our operation: Lock file: /Users/XXX/.gradle/caches/journal-1/journal-1.lock * Try: > Run with --stacktrace option to get the stack trace. > Run with --info or --debug option to get more log output. > Run with --scan to get full insights. > Get more help at https://help.gradle.org. Solution: gralde --stop gradle assemble #or when this occurs \"Timeout waiting to lock journal cache (/Users/wej/.gradle/caches/journal-1). It is currently in use by another Gradle instance.\" find ~/.gradle -type f -name \"*.lock\" -delete","title":"Gradle memory aid"},{"location":"gradle-memory-aid/#gradle-memory-aid","text":"","title":"Gradle memory aid"},{"location":"gradle-memory-aid/#gradle-commands-vs-maven-commands","text":"Maven Command Gradle Command Explanation mvn clean gradle clean Cleans the build artifacts and temporary files. mvn compile gradle compileJava Compiles the Java source code. mvn test gradle test Runs unit tests. mvn package gradle assemble Packages the compiled code into a distributable. mvn install gradle install Installs the artifact in the local repository. mvn deploy gradle publish Publishes the artifact to a remote repository. mvn dependency:tree gradle dependencies Displays project dependencies as a tree. mvn spring-boot:run gradle bootRun Runs a Spring Boot application. mvn clean install gradle clean build Cleans and then installs the artifact. mvn tomcat:run gradle bootRun Runs a Tomcat server with the web application. mvn site gradle site Generates project site documentation. mvn eclipse:eclipse gradle eclipse Generates Eclipse IDE project files. mvn dependency:purge-local-repository N/A Removes project dependencies from local repository. mvn validate gradle check Validates the project (syntax, configurations).","title":"gradle commands vs. maven commands"},{"location":"gradle-memory-aid/#troubleshoot","text":"","title":"troubleshoot"},{"location":"gradle-memory-aid/#idlelock-problem","text":"Error: # command gradle assemble # error message Starting a Gradle Daemon, 1 incompatible and 3 stopped Daemons could not be reused, use --status for details FAILURE: Build failed with an exception. * What went wrong: Gradle could not start your build. > Cannot create service of type BuildSessionActionExecutor using method LauncherServices$ToolingBuildSessionScopeServices.createActionExecutor() as there is a problem with parameter #21 of type FileSystemWatchingInformation. > Cannot create service of type BuildLifecycleAwareVirtualFileSystem using method VirtualFileSystemServices$GradleUserHomeServices.createVirtualFileSystem() as there is a problem with parameter #7 of type GlobalCacheLocations. > Cannot create service of type GlobalCacheLocations using method GradleUserHomeScopeServices.createGlobalCacheLocations() as there is a problem with parameter #1 of type List<GlobalCache>. > Could not create service of type FileAccessTimeJournal using GradleUserHomeScopeServices.createFileAccessTimeJournal(). > Timeout waiting to lock journal cache (/Users/XXX/.gradle/caches/journal-1). It is currently in use by another Gradle instance. Owner PID: 33272 Our PID: 33401 Owner Operation: Our operation: Lock file: /Users/XXX/.gradle/caches/journal-1/journal-1.lock * Try: > Run with --stacktrace option to get the stack trace. > Run with --info or --debug option to get more log output. > Run with --scan to get full insights. > Get more help at https://help.gradle.org. Solution: gralde --stop gradle assemble #or when this occurs \"Timeout waiting to lock journal cache (/Users/wej/.gradle/caches/journal-1). It is currently in use by another Gradle instance.\" find ~/.gradle -type f -name \"*.lock\" -delete","title":"IDLE/Lock problem"},{"location":"gradle/","text":"Gradle local repository Gradle stores dependecies under ~/.gradle/caches. If uses ~/.m2 if you configure: repositories { mavenLocal() } on macos m2 Getting the following error when running ./gradlew bootBuildImage on apple m2: Connection to the Docker daemon at 'localhost' failed with error \"[2] No such file or directory\"; ensure the Docker daemon is running and accessible Solution","title":"Gradle"},{"location":"gradle/#gradle","text":"","title":"Gradle"},{"location":"gradle/#local-repository","text":"Gradle stores dependecies under ~/.gradle/caches. If uses ~/.m2 if you configure: repositories { mavenLocal() }","title":"local repository"},{"location":"gradle/#on-macos-m2","text":"Getting the following error when running ./gradlew bootBuildImage on apple m2: Connection to the Docker daemon at 'localhost' failed with error \"[2] No such file or directory\"; ensure the Docker daemon is running and accessible Solution","title":"on macos m2"},{"location":"helm/","text":"Helm Helm is a package manager for k8s (like apt, yum, pip, ...) a chart is a package in helm which contains all resources for running an application a repository is a source for charts (by default artifacthub ) a release is an instance of a running chart cli overview # generate k8s manifest file from the helm chart in the current directory helm template <my_release_name> . > manifest.yml # directly apply helm chart from a repository to k8s cluster helm install <my_release_name> <chart_name> # delete helm-release from cluster helm uninstall <my_release_name> # overwrite variables with values file helm template <my_release_name> -f values.yaml . # overwrite variables on cli helm template <my_release_name> -f values.yaml --set overwriteKey=overwriteVal . #search chart in public repository helm search hub <chart_name> # if need be you can add the old repository (helm1 and helm2 chart) from https://github.com/helm/charts helm repo add deprecated_incubator https://charts.helm.sh/incubator/ helm repo add deprecated_stable https://charts.helm.sh/stable/ #search chart in your locally added repositories helm search repo <chart_name> # list default config of a helm chart helm show values <chart_name> # overwrite default config of helm chart helm install -f config.yaml <chart_name> # list installed helm-releases helm list # uninstall helm-release helm uninstall <my_release_name> # download helm sources of chart to local folder helm pull <chart_name> --untar # show manifest of a installed release tmp helm get manifest tmp # show values of a installed release tmp helm get values tmp umbrella chart pattern one single root chart.yml connection to other charts via dependencies one value file to overwrite sub charts once to initiate run: helm dep up to support multiple environments or tenants you can add additional value.yaml files extend content of fields It is easiert to extend a map than a list. In short, you can not extend/merge lists which is defined in a another values.yaml. A list is overriden by default. A map entry can be overriden by its name. Or the map is extended if you use a new entry field name. local dependecy The version and the chart name need to match event when the reference is locally. In Chart.yaml ... dependencies: - name: your-dependency-name version: \"1.2.3\" repository: \"file://../path/to/your/local/chart\" notes on certificates Not all cli command support ignoring or setting ca certs ( e.g. helm dependency update does not ) so you might consider to add the needed certificates system wide on os/container level. snippets Helm template guide gives a quick overview of the flow control and the available template functions. Under the hood helm uses a mix of Go template language and Sprig template library . required values If you want to check at the top of the file without rendering it to the template: {{ $_ := required \"A valid .Values.who entry required!\" .Values.who}} {{ $_ := required \"A valid .Values.who entry required!\" .Values.who2}} ... value: {{ .Values.who }} or inline value: {{ required \"A valid .Values.who entry required!\" .Values.who }} load file spec: inlineMultilineField: | {{ tpl (.Files.Get \"relativePath/file.json\" ) . | indent 4 }} doc in values.yaml see best practices # serverHost is the host name for the webserver serverHost: example # serverPort is the HTTP listener port for the webserver serverPort: 9191 range Different examples with result iterate over inline array iterate over array iterate over dictionary/map iterate over complex type // count loop {{- range until (.Value.upperBound | int) }} count: {{ . }} {{- end }} If you want to access all values (not only the looped on) inside of range, you need to access the global scope e.g. $.Values.bla compare numbers (eq, ne, lt, gt, le, ge) If you want to compare a value with a integer literal it might say that you cannot compare types. In this case add a decimal place to it. // change {{- if eq .Value.number 1 }} // to {{- if eq .Value.number 1.0 }} access dotted values # values.yaml ca.password: bla --- # template apiVersion: v1 kind: Secret metadata: # variable usage name: {{ index .Values \"ca.password\" }} .... variables # variable declaration and initiation {{- $random := ( randAlphaNum 5 | quote) }} apiVersion: v1 kind: Secret metadata: # variable usage name: name-{{ $random }} .... runtime values Runtime values can be accessed with the lookup function. The lookup values will only be visible when calling install. Helm template will not substitute these values. It might not work with argocd, because argo uses helm template and kubectl apply and not helm install. You can find an nice example how to lookup a password here . trigger reload of resource when connfigmap or secret changes kind: Deployment spec: template: metadata: annotations: checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}","title":"Helm"},{"location":"helm/#helm","text":"Helm is a package manager for k8s (like apt, yum, pip, ...) a chart is a package in helm which contains all resources for running an application a repository is a source for charts (by default artifacthub ) a release is an instance of a running chart","title":"Helm"},{"location":"helm/#cli-overview","text":"# generate k8s manifest file from the helm chart in the current directory helm template <my_release_name> . > manifest.yml # directly apply helm chart from a repository to k8s cluster helm install <my_release_name> <chart_name> # delete helm-release from cluster helm uninstall <my_release_name> # overwrite variables with values file helm template <my_release_name> -f values.yaml . # overwrite variables on cli helm template <my_release_name> -f values.yaml --set overwriteKey=overwriteVal . #search chart in public repository helm search hub <chart_name> # if need be you can add the old repository (helm1 and helm2 chart) from https://github.com/helm/charts helm repo add deprecated_incubator https://charts.helm.sh/incubator/ helm repo add deprecated_stable https://charts.helm.sh/stable/ #search chart in your locally added repositories helm search repo <chart_name> # list default config of a helm chart helm show values <chart_name> # overwrite default config of helm chart helm install -f config.yaml <chart_name> # list installed helm-releases helm list # uninstall helm-release helm uninstall <my_release_name> # download helm sources of chart to local folder helm pull <chart_name> --untar # show manifest of a installed release tmp helm get manifest tmp # show values of a installed release tmp helm get values tmp","title":"cli overview"},{"location":"helm/#umbrella-chart-pattern","text":"one single root chart.yml connection to other charts via dependencies one value file to overwrite sub charts once to initiate run: helm dep up to support multiple environments or tenants you can add additional value.yaml files","title":"umbrella chart pattern"},{"location":"helm/#extend-content-of-fields","text":"It is easiert to extend a map than a list. In short, you can not extend/merge lists which is defined in a another values.yaml. A list is overriden by default. A map entry can be overriden by its name. Or the map is extended if you use a new entry field name.","title":"extend content of fields"},{"location":"helm/#local-dependecy","text":"The version and the chart name need to match event when the reference is locally. In Chart.yaml ... dependencies: - name: your-dependency-name version: \"1.2.3\" repository: \"file://../path/to/your/local/chart\"","title":"local dependecy"},{"location":"helm/#notes-on-certificates","text":"Not all cli command support ignoring or setting ca certs ( e.g. helm dependency update does not ) so you might consider to add the needed certificates system wide on os/container level.","title":"notes on certificates"},{"location":"helm/#snippets","text":"Helm template guide gives a quick overview of the flow control and the available template functions. Under the hood helm uses a mix of Go template language and Sprig template library .","title":"snippets"},{"location":"helm/#required-values","text":"If you want to check at the top of the file without rendering it to the template: {{ $_ := required \"A valid .Values.who entry required!\" .Values.who}} {{ $_ := required \"A valid .Values.who entry required!\" .Values.who2}} ... value: {{ .Values.who }} or inline value: {{ required \"A valid .Values.who entry required!\" .Values.who }}","title":"required values"},{"location":"helm/#load-file","text":"spec: inlineMultilineField: | {{ tpl (.Files.Get \"relativePath/file.json\" ) . | indent 4 }}","title":"load file"},{"location":"helm/#doc-in-valuesyaml","text":"see best practices # serverHost is the host name for the webserver serverHost: example # serverPort is the HTTP listener port for the webserver serverPort: 9191","title":"doc in values.yaml"},{"location":"helm/#range","text":"Different examples with result iterate over inline array iterate over array iterate over dictionary/map iterate over complex type // count loop {{- range until (.Value.upperBound | int) }} count: {{ . }} {{- end }} If you want to access all values (not only the looped on) inside of range, you need to access the global scope e.g. $.Values.bla","title":"range"},{"location":"helm/#compare-numbers-eq-ne-lt-gt-le-ge","text":"If you want to compare a value with a integer literal it might say that you cannot compare types. In this case add a decimal place to it. // change {{- if eq .Value.number 1 }} // to {{- if eq .Value.number 1.0 }}","title":"compare numbers (eq, ne, lt, gt, le, ge)"},{"location":"helm/#access-dotted-values","text":"# values.yaml ca.password: bla --- # template apiVersion: v1 kind: Secret metadata: # variable usage name: {{ index .Values \"ca.password\" }} ....","title":"access dotted values"},{"location":"helm/#variables","text":"# variable declaration and initiation {{- $random := ( randAlphaNum 5 | quote) }} apiVersion: v1 kind: Secret metadata: # variable usage name: name-{{ $random }} ....","title":"variables"},{"location":"helm/#runtime-values","text":"Runtime values can be accessed with the lookup function. The lookup values will only be visible when calling install. Helm template will not substitute these values. It might not work with argocd, because argo uses helm template and kubectl apply and not helm install. You can find an nice example how to lookup a password here .","title":"runtime values"},{"location":"helm/#trigger-reload-of-resource-when-connfigmap-or-secret-changes","text":"kind: Deployment spec: template: metadata: annotations: checksum/config: {{ include (print $.Template.BasePath \"/configmap.yaml\") . | sha256sum }}","title":"trigger reload of resource when connfigmap or secret changes"},{"location":"jackson/","text":"Jackson ObjectMapper OVerview ObjectMapper om = new ObjectMapper(); // create json string form object String json = om.writeValueAsString (myJavaObject) annotation @JsonProperty @JsonAlias @JsonFormat e.g. parse a Date format it is your class setter/getter overload: does not work out of the box, one need to be annotated with @JsonSetter not your class you can not annotate the field, but you have no setter for a field: annotate the getter with @JsonProperty(access = JsonProperty.Access.READ_ONLY) ( But there is a bug in it ) add @JsonIgnoreProperties(value=\"field\", allowGetters = true, allowSetters = false) to class date parsing for java time helpful java #json #mapping","title":"Jackson"},{"location":"jackson/#jackson","text":"","title":"Jackson"},{"location":"jackson/#objectmapper","text":"OVerview ObjectMapper om = new ObjectMapper(); // create json string form object String json = om.writeValueAsString (myJavaObject)","title":"ObjectMapper"},{"location":"jackson/#annotation","text":"@JsonProperty @JsonAlias @JsonFormat e.g. parse a Date format","title":"annotation"},{"location":"jackson/#it-is-your-class","text":"setter/getter overload: does not work out of the box, one need to be annotated with @JsonSetter","title":"it is your class"},{"location":"jackson/#not-your-class","text":"you can not annotate the field, but you have no setter for a field: annotate the getter with @JsonProperty(access = JsonProperty.Access.READ_ONLY) ( But there is a bug in it ) add @JsonIgnoreProperties(value=\"field\", allowGetters = true, allowSetters = false) to class","title":"not your class"},{"location":"jackson/#date-parsing-for-java-time","text":"helpful","title":"date parsing for java time"},{"location":"jackson/#java-json-mapping","text":"","title":"java #json #mapping"},{"location":"java-logging/","text":"Logging with Java overview loggers define which package and which log level you want to use appenders define how messages are logged (File, Console, RollingFile etc.) you can connect one or more appenders to one logger if no appender 'catches' the log it will be handled by the root logger (which you need to connect to a logger as well as set a log level) scafffolding use slf4j with simple-logger to print to console if you want to configure loggers and appenders use log4j2 or logback if you have no strong opinion on that use logback because it rolling filling appender config is way more convenient if you use spring boot just use slf4j loggers in code (Spring boot uses logback under the hood) if you want to configuration configure with logging.level.* in properties.yaml e.g. ogging.level.org=DEBUG or add a logback-spring.xml to resources folder if you use logback-spring.xml you can use properties from appliyation.yaml with <springProperty name=\"targetname\" source=\"spring.property.name\"/> slf4j + simple logger This setting does not need a configuration to work. slf4j + logback + spring remember it is called logback not lockback Paste listing content into src/main/resources/logback-spring.xml add app.name Property to src/main/resources/application.properties <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <!-- to load default spring layout --> <include resource=\"org/springframework/boot/logging/logback/defaults.xml\"/> <!-- to load properties from application.properties --> <property resource=\"application.properties\" /> <!-- app.name needs to be defined in application.properties (otherwise it will use '${app.name}' as folder name)--> <property name=\"APPLICATION_NAME\" value=\"${app.name}\"/> <property name=\"PATH_LOGS\" value=\"/tmp/${APPLICATION_NAME}/logs\" /> <property name=\"LAYOUT\" value=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/> <appender name=\"Console\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <!-- default spring layout --> <Pattern>${CONSOLE_LOG_PATTERN}</Pattern> </encoder> </appender> <appender name=\"RollingFile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <file>${PATH_LOGS}/${APPLICATION_NAME}.log</file> <encoder> <Pattern>${LAYOUT}</Pattern> </encoder> <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"> <fileNamePattern>${PATH_LOGS}/archived/${APPLICATION_NAME}-%d{yyyy-MM-dd}.%i.log</fileNamePattern> <maxFileSize>10MB</maxFileSize> <!-- deletes oldest files when totalSizeCap or maxHostory is reached --> <totalSizeCap>100MB</totalSizeCap> <maxHistory>30</maxHistory> </rollingPolicy> </appender> <logger name=\"de.weyrich\" level=\"debug\" additivity=\"false\"> <appender-ref ref=\"RollingFile\" /> <appender-ref ref=\"Console\" /> </logger> <root level=\"info\"> <appender-ref ref=\"RollingFile\" /> <appender-ref ref=\"Console\" /> </root> </configuration> slf4j + log4j2 does not need a configuration to funciton add log4j2.xml to resources folder (not log4j.xml!) the name of the logger defines which packages are targeted. You do not need wildcards, you just end with the package part you want to include (e.g. de.weyrich) you cann access system properties with ${sys:propertyName} Maven: <properties> <slf4j.version>1.7.5</slf4j.version> <log4j.version>2.13.3</log4j.version> </properties> ... <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${slf4j.version}</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-slf4j-impl</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>${log4j.version}</version> </dependency> log4j2.xml <Configuration status=\"info\"> <Appenders> <File name=\"FILE\" fileName=\"application.log\"> <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/> </File> <Console name=\"CONSOLE\"> <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/> </Console> </Appenders> <Loggers> <Logger name=\"de\" level=\"DEBUG\"> <AppenderRef ref=\"CONSOLE\"/> <AppenderRef ref=\"FILE\"/> </Logger> <!-- addtional loggers e.g. for used libraries--> <Root level=\"WARN\"> <AppenderRef ref=\"CONSOLE\"/> </Root> </Loggers> </Configuration>","title":"Logging with Java"},{"location":"java-logging/#logging-with-java","text":"","title":"Logging with Java"},{"location":"java-logging/#overview","text":"loggers define which package and which log level you want to use appenders define how messages are logged (File, Console, RollingFile etc.) you can connect one or more appenders to one logger if no appender 'catches' the log it will be handled by the root logger (which you need to connect to a logger as well as set a log level)","title":"overview"},{"location":"java-logging/#scafffolding","text":"use slf4j with simple-logger to print to console if you want to configure loggers and appenders use log4j2 or logback if you have no strong opinion on that use logback because it rolling filling appender config is way more convenient if you use spring boot just use slf4j loggers in code (Spring boot uses logback under the hood) if you want to configuration configure with logging.level.* in properties.yaml e.g. ogging.level.org=DEBUG or add a logback-spring.xml to resources folder if you use logback-spring.xml you can use properties from appliyation.yaml with <springProperty name=\"targetname\" source=\"spring.property.name\"/>","title":"scafffolding"},{"location":"java-logging/#slf4j-simple-logger","text":"This setting does not need a configuration to work.","title":"slf4j + simple logger"},{"location":"java-logging/#slf4j-logback-spring","text":"remember it is called logback not lockback Paste listing content into src/main/resources/logback-spring.xml add app.name Property to src/main/resources/application.properties <?xml version=\"1.0\" encoding=\"UTF-8\"?> <configuration> <!-- to load default spring layout --> <include resource=\"org/springframework/boot/logging/logback/defaults.xml\"/> <!-- to load properties from application.properties --> <property resource=\"application.properties\" /> <!-- app.name needs to be defined in application.properties (otherwise it will use '${app.name}' as folder name)--> <property name=\"APPLICATION_NAME\" value=\"${app.name}\"/> <property name=\"PATH_LOGS\" value=\"/tmp/${APPLICATION_NAME}/logs\" /> <property name=\"LAYOUT\" value=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/> <appender name=\"Console\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder> <!-- default spring layout --> <Pattern>${CONSOLE_LOG_PATTERN}</Pattern> </encoder> </appender> <appender name=\"RollingFile\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"> <file>${PATH_LOGS}/${APPLICATION_NAME}.log</file> <encoder> <Pattern>${LAYOUT}</Pattern> </encoder> <rollingPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy\"> <fileNamePattern>${PATH_LOGS}/archived/${APPLICATION_NAME}-%d{yyyy-MM-dd}.%i.log</fileNamePattern> <maxFileSize>10MB</maxFileSize> <!-- deletes oldest files when totalSizeCap or maxHostory is reached --> <totalSizeCap>100MB</totalSizeCap> <maxHistory>30</maxHistory> </rollingPolicy> </appender> <logger name=\"de.weyrich\" level=\"debug\" additivity=\"false\"> <appender-ref ref=\"RollingFile\" /> <appender-ref ref=\"Console\" /> </logger> <root level=\"info\"> <appender-ref ref=\"RollingFile\" /> <appender-ref ref=\"Console\" /> </root> </configuration>","title":"slf4j + logback + spring"},{"location":"java-logging/#slf4j-log4j2","text":"does not need a configuration to funciton add log4j2.xml to resources folder (not log4j.xml!) the name of the logger defines which packages are targeted. You do not need wildcards, you just end with the package part you want to include (e.g. de.weyrich) you cann access system properties with ${sys:propertyName} Maven: <properties> <slf4j.version>1.7.5</slf4j.version> <log4j.version>2.13.3</log4j.version> </properties> ... <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version>${slf4j.version}</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-slf4j-impl</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-api</artifactId> <version>${log4j.version}</version> </dependency> <dependency> <groupId>org.apache.logging.log4j</groupId> <artifactId>log4j-core</artifactId> <version>${log4j.version}</version> </dependency> log4j2.xml <Configuration status=\"info\"> <Appenders> <File name=\"FILE\" fileName=\"application.log\"> <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/> </File> <Console name=\"CONSOLE\"> <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/> </Console> </Appenders> <Loggers> <Logger name=\"de\" level=\"DEBUG\"> <AppenderRef ref=\"CONSOLE\"/> <AppenderRef ref=\"FILE\"/> </Logger> <!-- addtional loggers e.g. for used libraries--> <Root level=\"WARN\"> <AppenderRef ref=\"CONSOLE\"/> </Root> </Loggers> </Configuration>","title":"slf4j + log4j2"},{"location":"jdbc/","text":"jdbc db2 i highly recommend to activate jdbc traces while developing to see how the driver butchers your sql statements. maven dependecy: https://mvnrepository.com/artifact/com.ibm.db2/jcc does not like semicolons in queries you may need a space at the end of every line. New lines are ignored by the driver. complete query will executed in lowerCase. Casesensitive string compares may therefore be effected. You may want to use the lower() to workaround this issue. you may need to execute Class.forName(\"com.ibm.db2.jcc.DB2Driver\"); before calling the connection if you are using jdbc version < 4.0 example from ibm: https://www.ibm.com/support/knowledgecenter/SSEPEK_10.0.0/java/src/tpc/imjcc_cjvjdbas.html if you want to use Hibernate UUID in DB2, create it in DB with varchar36 so you can edit it by yourself (see @JdbcTypeCode(SqlTypes.VARCHAR)) druid/ avatica core official doc you can not use all druid sql language features over jdbc so you should consider using the rest api for queries instead jdbc url against broker(default :8082) or router (default :8888) : jdbc:avatica:remote:url=http://localhost:8888/druid/v2/sql/avatica/ Drivername: org.apache.calcite.avatica.remote.Driver user: if default not needed pw: if default not needed add driver to classpath: <dependency> <groupId>org.apache.calcite.avatica</groupId> <artifactId>avatica-core</artifactId> <version>1.17.0</version> </dependency> oracle thin If you want to query a table which has lower case letters in it, you need to add quotes to the query e.g.: \u00b4\u00b4\u00b4sql select + from \"flyway_schema_history\" \u00b4\u00b4\u00b4 Same goes for fieldnames. By default oracle thin driver send every table and field name in uppercase even if you wrote lowercase letter. integration into spring boot For mininal setup use spring-boot-starter-jdbc . If you want spring data like access use spring-boot-starter-data-jdbc , but then you may just use spring-data-jpa maven <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency> Config @Configuration public class JdbcConfig{ private String driverName; private String jdbcUrl; private String dbUser; private String dbPw; @Bean public Datasource datasource(){ DriverManagerDataSource dataSource = new DriverManagerDataSource(); dataSource.setDriverClassName(driverName); dataSource.setUrl(jdbcUrl); dataSource.setUsername(dbUser); dataSource.setPassword(dbPw); return dataSource; } } usage Init JdbcTemplate with Datasource @Autowired public YourDao(Datasource datasource){ this.jdbcTemplate = new JdbcTemplate(datasource); }","title":"jdbc"},{"location":"jdbc/#jdbc","text":"","title":"jdbc"},{"location":"jdbc/#db2","text":"i highly recommend to activate jdbc traces while developing to see how the driver butchers your sql statements. maven dependecy: https://mvnrepository.com/artifact/com.ibm.db2/jcc does not like semicolons in queries you may need a space at the end of every line. New lines are ignored by the driver. complete query will executed in lowerCase. Casesensitive string compares may therefore be effected. You may want to use the lower() to workaround this issue. you may need to execute Class.forName(\"com.ibm.db2.jcc.DB2Driver\"); before calling the connection if you are using jdbc version < 4.0 example from ibm: https://www.ibm.com/support/knowledgecenter/SSEPEK_10.0.0/java/src/tpc/imjcc_cjvjdbas.html if you want to use Hibernate UUID in DB2, create it in DB with varchar36 so you can edit it by yourself (see @JdbcTypeCode(SqlTypes.VARCHAR))","title":"db2"},{"location":"jdbc/#druid-avatica-core","text":"official doc you can not use all druid sql language features over jdbc so you should consider using the rest api for queries instead jdbc url against broker(default :8082) or router (default :8888) : jdbc:avatica:remote:url=http://localhost:8888/druid/v2/sql/avatica/ Drivername: org.apache.calcite.avatica.remote.Driver user: if default not needed pw: if default not needed add driver to classpath: <dependency> <groupId>org.apache.calcite.avatica</groupId> <artifactId>avatica-core</artifactId> <version>1.17.0</version> </dependency>","title":"druid/ avatica core"},{"location":"jdbc/#oracle-thin","text":"If you want to query a table which has lower case letters in it, you need to add quotes to the query e.g.: \u00b4\u00b4\u00b4sql select + from \"flyway_schema_history\" \u00b4\u00b4\u00b4 Same goes for fieldnames. By default oracle thin driver send every table and field name in uppercase even if you wrote lowercase letter.","title":"oracle thin"},{"location":"jdbc/#integration-into-spring-boot","text":"For mininal setup use spring-boot-starter-jdbc . If you want spring data like access use spring-boot-starter-data-jdbc , but then you may just use spring-data-jpa","title":"integration into spring boot"},{"location":"jdbc/#maven","text":"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-jdbc</artifactId> </dependency>","title":"maven"},{"location":"jdbc/#config","text":"@Configuration public class JdbcConfig{ private String driverName; private String jdbcUrl; private String dbUser; private String dbPw; @Bean public Datasource datasource(){ DriverManagerDataSource dataSource = new DriverManagerDataSource(); dataSource.setDriverClassName(driverName); dataSource.setUrl(jdbcUrl); dataSource.setUsername(dbUser); dataSource.setPassword(dbPw); return dataSource; } }","title":"Config"},{"location":"jdbc/#usage","text":"Init JdbcTemplate with Datasource @Autowired public YourDao(Datasource datasource){ this.jdbcTemplate = new JdbcTemplate(datasource); }","title":"usage"},{"location":"jenkins/","text":"Jenkins (pipeline) if you use a jenkins file it is called a multi branch pipeline (it uses the pipeline plugin) there is a imperative and a declarative pipeline syntax there is a vs code pipeline linter extension 'Jenkins Pipeline Linter Connector' which uses the jenkins build in pipeline linter: http://<JENKINS:PORT>/pipeline-model-converter/validate . You may need to configure the extension to ignore SSL errors. you can add (global) config files to the jenkins project like a settings.xml or property file with credentials if you use blue ocean you might not get the real error of a pipeline so you should open the console in classic view if stuff seems to be missing do no use the variable name 'package' in jenkins file. (declarative) pipeline examples official api documentation define variables Use environment block below pipeline for all stages below stage for one stage you can access the variables in script blocks like groovy variables and within declarative syntax scope in strings with \"$VARIABLE_NAME\" pipeline { agent any environment { MY_GLOBAL_VARIABLE='i am global' } stages { stage('1'){ steps { echo \"$MY_GLOBAL_VARIABLE\" } } } } build java use bat or sh before unknown commands (depends on OS of jenkins agent) save artifact save test results pipeline { agent any stages { stage('build and save artifact'){ steps { echo 'building project-a' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true } } stage('test and save results') { steps { echo 'testing project-a' sh 'mvn test' junit 'project-a/target/surefire-reports/*.xml' } } } } deploy with ssh use ssh pipeline step plugin [Github](https://github.com/jenkinsci/ssh-steps-plugin Jenkins Doku use stash/unstash to move stuff between stages if you define a variable like rmote you need to be in a script block when using withCredentials you can choose between usernamePassword or sshUserPrivateKey Prerequisite: The user must exist, needs to have the public key configured pipeline { agent any stages { stage('build and save artifact'){ steps { echo 'building project' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true stash includes: 'target/*.jar', name: 'project' } } stage('deploy artifact and run startSkript'){ steps{ echo 'deploy project' unstash 'project' // script block needed to define a variable 'remote' script { // UUID (e.g. 42fb2924-4a89-43a4-8e96-1c2e0c61e446) of the jenkins credentials entity. If you only see the name of the credential you may use html devloper tools to find its uuid // ssh credential for auth with private ssh key withCredentials([sshUserPrivateKey(credentialsId: '42fb2924-4a89-43a4-8e96-1c2e0c61e446', keyFileVariable: 'identity', passphraseVariable: 'passphrase', usernameVariable: 'userName')]) { // ssh credential for password auth // usernameVariable([usernamePassword(credentialsId: '42fb2924-4a89-43a4-8e96-1c2e0c61e446', usernameVariable: 'userName', passwordVariable: 'password') { def remote = [:] remote.user = 'jenkins' //probably not relevant when using ssh key remote.identityFile = identity remote.passphrase = passphrase remote.name = 'myhost' remote.host = 'myhost.de' remote.allowAnyHosts = true // for password auth //remote.user=userName //remote.password=password // or just: // def remote = [user = 'jenkins', identityFile = identity, passphrase = passphrase, name = 'myhost', host = 'myhost.de', allowAnyHosts = true] println remote sshCommand remote: remote, command: \"pwd\", sudo: \"false\" // sshPut needs exact name match sshPut remote: remote, from: 'target/app.jar', into: '.' sshPut remote: remote, from: './start-app.sh', into: '.' sshCommand remote: remote, command: \"sh ./start-app.sh\", sudo: \"true\" } } } } } } parallel build in monorepo parallel build use dir directive pipeline { agent any stages { stage('parallel stage'){ parallel { stage('stage project-a') { stages { stage('build project-a'){ steps { dir('./project-a/') { echo 'building project-a' } } } } } stage('stage project-b') { stages { stage('build project-b'){ steps { dir('./project-b/') { echo 'building project-b' } } } } } } } } } user interaction yes/no prompt with timeout use input step to ask user for permission set timeout option for stage so it will be finished if no user acknowledges the step stage('deploy'){ when { branch: \"master\" } options{ timeout(time:15, unit: 'MINUTES') } steps{ input 'Deploy?' script { deploy(\"$environment\", \"stash-name\" ,\"spring-profile\") } } } pipeline parameters add parameters block under pipeline asks user for input Example from official documentation pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" echo \"Biography: ${params.BIOGRAPHY}\" echo \"Toggle: ${params.TOGGLE}\" echo \"Choice: ${params.CHOICE}\" echo \"Password: ${params.PASSWORD}\" } } } } define and use function (inline) You can define funtions below the pipeline block def functionName(String paramExamle){} function can be called in steps or script blocks depending on the used in the function: functionName('myParam') pipeline { agent any stages { stage('build and save artifact'){ steps { build() } } stage('test and save results') { steps { test() } } } } def build(){ echo 'building project-a' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true } def test(){ echo 'testing project-a' sh 'mvn test' junit 'project-a/target/surefire-reports/*.xml' } define and use function (library) see create zip file pipeline { agent any stages { stage('zip whole workspace'){ steps { zip zipFile: \"../target.zip\", archive: true, dir: \".\" } } } } read xml use readFile file:'myFile.xml' this might be prohibited by jenkins and need approval of an admin (\"Manage jenkins-> In-process script approval\") Examples for groovy XmlParser You should read xml file into string before processing it otherwise jenkins might throw exceptions test.xml: \"\"\"xml Aang \"\"\" pipeline { agent any stages { stage('read from xml'){ script { def xmlContentString = readFile(\"test.xml\") // do not add root element (in this case avatar) to path println(getXmlSlurper(xmlContentString).name) //Aang } } } } @NonCPS def getXmlSlurper(String xmlContentString){ return new XmlSlurper().parseText(xmlContentString) } remove old builds use buildDiscarder and logRotator api to remove old build artifacts see pipeline { agent any options { buildDiscarder(logRotator(numToKeepStr: '30', artifactNumToKeepStr: '30'))() } stage('build and save artifact'){ steps { echo 'building project-a' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true } } } clean jenkins workspace it may be that jenkins keep you workspace between two builds and you need to delete created folder A general solution is to skip default checkout mechanism and delete everything before checking out this might slow down the build so you should consider to use custom delete logic in you pipeline pipeline { agent any options { skipDefaultCheckout(true) } stages { stage('Checkout') { steps { cleanWs() checkout scm } } stages { stage('do stuff'){ steps { echo \"do stuff\" } } } }","title":"Jenkins (pipeline)"},{"location":"jenkins/#jenkins-pipeline","text":"if you use a jenkins file it is called a multi branch pipeline (it uses the pipeline plugin) there is a imperative and a declarative pipeline syntax there is a vs code pipeline linter extension 'Jenkins Pipeline Linter Connector' which uses the jenkins build in pipeline linter: http://<JENKINS:PORT>/pipeline-model-converter/validate . You may need to configure the extension to ignore SSL errors. you can add (global) config files to the jenkins project like a settings.xml or property file with credentials if you use blue ocean you might not get the real error of a pipeline so you should open the console in classic view if stuff seems to be missing do no use the variable name 'package' in jenkins file.","title":"Jenkins (pipeline)"},{"location":"jenkins/#declarative-pipeline-examples","text":"official api documentation","title":"(declarative) pipeline examples"},{"location":"jenkins/#define-variables","text":"Use environment block below pipeline for all stages below stage for one stage you can access the variables in script blocks like groovy variables and within declarative syntax scope in strings with \"$VARIABLE_NAME\" pipeline { agent any environment { MY_GLOBAL_VARIABLE='i am global' } stages { stage('1'){ steps { echo \"$MY_GLOBAL_VARIABLE\" } } } }","title":"define variables"},{"location":"jenkins/#build-java","text":"use bat or sh before unknown commands (depends on OS of jenkins agent) save artifact save test results pipeline { agent any stages { stage('build and save artifact'){ steps { echo 'building project-a' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true } } stage('test and save results') { steps { echo 'testing project-a' sh 'mvn test' junit 'project-a/target/surefire-reports/*.xml' } } } }","title":"build java"},{"location":"jenkins/#deploy-with-ssh","text":"use ssh pipeline step plugin [Github](https://github.com/jenkinsci/ssh-steps-plugin Jenkins Doku use stash/unstash to move stuff between stages if you define a variable like rmote you need to be in a script block when using withCredentials you can choose between usernamePassword or sshUserPrivateKey Prerequisite: The user must exist, needs to have the public key configured pipeline { agent any stages { stage('build and save artifact'){ steps { echo 'building project' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true stash includes: 'target/*.jar', name: 'project' } } stage('deploy artifact and run startSkript'){ steps{ echo 'deploy project' unstash 'project' // script block needed to define a variable 'remote' script { // UUID (e.g. 42fb2924-4a89-43a4-8e96-1c2e0c61e446) of the jenkins credentials entity. If you only see the name of the credential you may use html devloper tools to find its uuid // ssh credential for auth with private ssh key withCredentials([sshUserPrivateKey(credentialsId: '42fb2924-4a89-43a4-8e96-1c2e0c61e446', keyFileVariable: 'identity', passphraseVariable: 'passphrase', usernameVariable: 'userName')]) { // ssh credential for password auth // usernameVariable([usernamePassword(credentialsId: '42fb2924-4a89-43a4-8e96-1c2e0c61e446', usernameVariable: 'userName', passwordVariable: 'password') { def remote = [:] remote.user = 'jenkins' //probably not relevant when using ssh key remote.identityFile = identity remote.passphrase = passphrase remote.name = 'myhost' remote.host = 'myhost.de' remote.allowAnyHosts = true // for password auth //remote.user=userName //remote.password=password // or just: // def remote = [user = 'jenkins', identityFile = identity, passphrase = passphrase, name = 'myhost', host = 'myhost.de', allowAnyHosts = true] println remote sshCommand remote: remote, command: \"pwd\", sudo: \"false\" // sshPut needs exact name match sshPut remote: remote, from: 'target/app.jar', into: '.' sshPut remote: remote, from: './start-app.sh', into: '.' sshCommand remote: remote, command: \"sh ./start-app.sh\", sudo: \"true\" } } } } } }","title":"deploy with ssh"},{"location":"jenkins/#parallel-build-in-monorepo","text":"parallel build use dir directive pipeline { agent any stages { stage('parallel stage'){ parallel { stage('stage project-a') { stages { stage('build project-a'){ steps { dir('./project-a/') { echo 'building project-a' } } } } } stage('stage project-b') { stages { stage('build project-b'){ steps { dir('./project-b/') { echo 'building project-b' } } } } } } } } }","title":"parallel build in monorepo"},{"location":"jenkins/#user-interaction","text":"","title":"user interaction"},{"location":"jenkins/#yesno-prompt-with-timeout","text":"use input step to ask user for permission set timeout option for stage so it will be finished if no user acknowledges the step stage('deploy'){ when { branch: \"master\" } options{ timeout(time:15, unit: 'MINUTES') } steps{ input 'Deploy?' script { deploy(\"$environment\", \"stash-name\" ,\"spring-profile\") } } }","title":"yes/no prompt with timeout"},{"location":"jenkins/#pipeline-parameters","text":"add parameters block under pipeline asks user for input Example from official documentation pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" echo \"Biography: ${params.BIOGRAPHY}\" echo \"Toggle: ${params.TOGGLE}\" echo \"Choice: ${params.CHOICE}\" echo \"Password: ${params.PASSWORD}\" } } } }","title":"pipeline parameters"},{"location":"jenkins/#define-and-use-function-inline","text":"You can define funtions below the pipeline block def functionName(String paramExamle){} function can be called in steps or script blocks depending on the used in the function: functionName('myParam') pipeline { agent any stages { stage('build and save artifact'){ steps { build() } } stage('test and save results') { steps { test() } } } } def build(){ echo 'building project-a' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true } def test(){ echo 'testing project-a' sh 'mvn test' junit 'project-a/target/surefire-reports/*.xml' }","title":"define and use function (inline)"},{"location":"jenkins/#define-and-use-function-library","text":"see","title":"define and use function (library)"},{"location":"jenkins/#create-zip-file","text":"pipeline { agent any stages { stage('zip whole workspace'){ steps { zip zipFile: \"../target.zip\", archive: true, dir: \".\" } } } }","title":"create zip file"},{"location":"jenkins/#read-xml","text":"use readFile file:'myFile.xml' this might be prohibited by jenkins and need approval of an admin (\"Manage jenkins-> In-process script approval\") Examples for groovy XmlParser You should read xml file into string before processing it otherwise jenkins might throw exceptions test.xml: \"\"\"xml Aang \"\"\" pipeline { agent any stages { stage('read from xml'){ script { def xmlContentString = readFile(\"test.xml\") // do not add root element (in this case avatar) to path println(getXmlSlurper(xmlContentString).name) //Aang } } } } @NonCPS def getXmlSlurper(String xmlContentString){ return new XmlSlurper().parseText(xmlContentString) }","title":"read xml"},{"location":"jenkins/#remove-old-builds","text":"use buildDiscarder and logRotator api to remove old build artifacts see pipeline { agent any options { buildDiscarder(logRotator(numToKeepStr: '30', artifactNumToKeepStr: '30'))() } stage('build and save artifact'){ steps { echo 'building project-a' sh 'mvn -B -DskipTests clean package' archiveArtifacts artifacts: 'target/*.jar', fingerprint: true } } }","title":"remove old builds"},{"location":"jenkins/#clean-jenkins-workspace","text":"it may be that jenkins keep you workspace between two builds and you need to delete created folder A general solution is to skip default checkout mechanism and delete everything before checking out this might slow down the build so you should consider to use custom delete logic in you pipeline pipeline { agent any options { skipDefaultCheckout(true) } stages { stage('Checkout') { steps { cleanWs() checkout scm } } stages { stage('do stuff'){ steps { echo \"do stuff\" } } } }","title":"clean jenkins workspace"},{"location":"jwt/","text":"JWT with java","title":"JWT"},{"location":"jwt/#jwt","text":"","title":"JWT"},{"location":"jwt/#_1","text":"","title":""},{"location":"jwt/#with-java","text":"","title":"with java"},{"location":"k3d/","text":"k3d use local image k3d cli allows to import local images to k3d. You need to set imagePullPolicy: IfNotPresent in the pod definition. Otherwise it tries to pull it from Dockerhub. # k3d cluster create local k3d image import IMAGENAME --cluster local","title":"k3d"},{"location":"k3d/#k3d","text":"","title":"k3d"},{"location":"k3d/#use-local-image","text":"k3d cli allows to import local images to k3d. You need to set imagePullPolicy: IfNotPresent in the pod definition. Otherwise it tries to pull it from Dockerhub. # k3d cluster create local k3d image import IMAGENAME --cluster local","title":"use local image"},{"location":"logging-stacks/","text":"Logging stacks k8s logging distinct between: Container logs Kubernetes system components logging which run on the node itself (typically only kubelet, but may be more) container log to stdout which is collected by k8s and can be viewed via kubectl container logs are stored at kubelet more details here Options for collecting container logs: logging sidecar container running inside an app\u2019s pod. using a node-level logging agent that runs on every node. push logs directly from within an application to some backend.","title":"Logging stacks"},{"location":"logging-stacks/#logging-stacks","text":"","title":"Logging stacks"},{"location":"logging-stacks/#k8s-logging","text":"distinct between: Container logs Kubernetes system components logging which run on the node itself (typically only kubelet, but may be more) container log to stdout which is collected by k8s and can be viewed via kubectl container logs are stored at kubelet more details here Options for collecting container logs: logging sidecar container running inside an app\u2019s pod. using a node-level logging agent that runs on every node. push logs directly from within an application to some backend.","title":"k8s logging"},{"location":"mac/","text":"Notes on mac usage moving from linux to mac moving to zsh macos - how does the window system work install snippets ## installm brew: https://brew.sh/ ## install oh-my-zsh: https://ohmyz.sh/#install # add brew to path echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> ~/.zprofile eval \"$(/opt/homebrew/bin/brew shellenv)\" # activate completion in zsh echo \"autoload -Uz compinit && compinit\" >> ~/.zshrc . ~/.zshrc brew install --cask firefox brew install --cask visual-studio-code # control dark mode brew install --cask nightfall # cli brew install jq brew install tfenv # nvm needs additional steps see brew low after installs brew install thefuck brew install yarn brew install nvm # python brew install pyenv pyenv install -l pyenv install 3.8.0 pyenv install 2.7.14 pyenv local 2.8.0 # docker & Kubernetes ## Option 1: Rancher Desktop brew install --cask rancher echo \"# rancher desktop\" >> ~/.zshrc echo \"export PATH=$PATH:$HOME/.rd/bin\" >> ~/.zshrc source ~/.zshrc ## Option 2: colima provides container runtime in vm brew install colima ## docker cli brew install docker ## docker mac fix # problems wth keychain when running docker login: # to fix: brew install docker-credential-helper # cloud brew install awscli brew install --cask google-cloud-sdk brew install azure-cli ~/.zprofile # for brew eval \"$(/opt/homebrew/bin/brew shellenv)\" ~/.zshrc snippets # oh my zsh stuff... # activate completion in zsh autoload -Uz compinit && compinit # add brew to path eval \"$(/opt/homebrew/bin/brew shellenv)\" # for git ssh-add --apple-use-keychain ~/.ssh/id_rsa # brew completion if type brew &>/dev/null then FPATH=\"$(brew --prefix)/share/zsh/site-functions:${FPATH}\" autoload -Uz compinit compinit fi # from brew install info export NVM_DIR=\"$HOME/.nvm\" [ -s \"/opt/homebrew/opt/nvm/nvm.sh\" ] && \\. \"/opt/homebrew/opt/nvm/nvm.sh\" # This loads nvm [ -s \"/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm\" ] && \\. \"/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm\" # This loads nvm bash_completion # gpg export GPG_TTY=$(tty) eval $(thefuck --alias) usual git stuff ## alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status ## base config MAIL_ADDRESS=\"jan.weyrich@example.de\" git config --global user.name \"Jan Weyrich\" git config --global user.email \"$MAIL_ADDRESS\" git config --global core.hooksPath .githooks ## generate ssh key ssh-keygen -t ed25519 -C \"$MAIL_ADDRESS\" # add ssh key agent for passwort prompt # see https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent ## gpg brew install gpg2 gnupg pinentry-mac gpg --full-generate-key # config for git gpg -K --keyid-format SHORT export GPG_SEC_ID=8627004B gpg --armor --export $GPG_SEC_ID > ~/gpg-public.pem git config --global user.signingkey $GPG_SEC_ID git config --global gpg.program $(which gpg) git config --global commit.gpgsign true echo 'export GPG_TTY=$(tty)' >> ~/.zshrc source ~/.zshrc # for pw prompt see here: https://unixb0y.de/blog/articles/2019-01/gpg-password-macos-keychain echo 'use-agent' > ~/.gnupg/gpg.conf echo \"pinentry-program /opt/homebrew/bin/pinentry-mac\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent # check with echo \"test\" | gpg --clearsign # if you got: gpg: signing failed: No pinentry # you may want to try with: echo \"/opt/homebrew/bin/pinentry\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent ## save passphrase for twelve hours echo \"default-cache-ttl 43200\" >> ~/.gnupg/gpg-agent.conf completion stopped working rm -f ~/.zcompdump; compinit Source colima # to debug export DEBUG=testcontainers colima start docker context use colima docker run hello-world Could not get testcontainers to work with colima. Here are some links with further information: https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226 https://stackoverflow.com/questions/70749679/how-can-i-use-testcontainer-in-mac-os-with-out-docker-desktop m1 compatibility problems multi-arch build for arm and x86 See Docker build docker build echo \"# for docker build on m1\" >> ~/.zshrc echo \"export DOCKER_BUILDKIT=0\" >> ~/.zshrc echo \"export COMPOSE_DOCKER_CLI_BUILD=0\" >> ~/.zshrc source ~/.zshrc failed to solve with frontend dockerfile.v0: failed to create LLB definition: rpc error: code = Unknown desc = error getting credentials - err: docker-credential-osxkeychain resolves to executable in current directory (./docker-credential-osxkeychain), out: `` Source terraform Provider registry.terraform.io/hashicorp/template v2.2.0 does not have a package available for your current platform, darwin_arm64. Terraform forum brew install kreuzwerker/taps/m1-terraform-provider-helper m1-terraform-provider-helper activate m1-terraform-provider-helper install hashicorp/template -v v2.2.0 extend sudoers file \u00b4\u00b4\u00b4shell sudo visudo /private/etc/sudoers add jwy ALL=(ALL) ALL \u00b4\u00b4\u00b4","title":"Notes on mac usage"},{"location":"mac/#notes-on-mac-usage","text":"","title":"Notes on mac usage"},{"location":"mac/#moving-from-linux-to-mac","text":"moving to zsh macos - how does the window system work","title":"moving from linux to mac"},{"location":"mac/#install-snippets","text":"## installm brew: https://brew.sh/ ## install oh-my-zsh: https://ohmyz.sh/#install # add brew to path echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"' >> ~/.zprofile eval \"$(/opt/homebrew/bin/brew shellenv)\" # activate completion in zsh echo \"autoload -Uz compinit && compinit\" >> ~/.zshrc . ~/.zshrc brew install --cask firefox brew install --cask visual-studio-code # control dark mode brew install --cask nightfall # cli brew install jq brew install tfenv # nvm needs additional steps see brew low after installs brew install thefuck brew install yarn brew install nvm # python brew install pyenv pyenv install -l pyenv install 3.8.0 pyenv install 2.7.14 pyenv local 2.8.0 # docker & Kubernetes ## Option 1: Rancher Desktop brew install --cask rancher echo \"# rancher desktop\" >> ~/.zshrc echo \"export PATH=$PATH:$HOME/.rd/bin\" >> ~/.zshrc source ~/.zshrc ## Option 2: colima provides container runtime in vm brew install colima ## docker cli brew install docker ## docker mac fix # problems wth keychain when running docker login: # to fix: brew install docker-credential-helper # cloud brew install awscli brew install --cask google-cloud-sdk brew install azure-cli","title":"install snippets"},{"location":"mac/#zprofile","text":"# for brew eval \"$(/opt/homebrew/bin/brew shellenv)\"","title":"~/.zprofile"},{"location":"mac/#zshrc-snippets","text":"# oh my zsh stuff... # activate completion in zsh autoload -Uz compinit && compinit # add brew to path eval \"$(/opt/homebrew/bin/brew shellenv)\" # for git ssh-add --apple-use-keychain ~/.ssh/id_rsa # brew completion if type brew &>/dev/null then FPATH=\"$(brew --prefix)/share/zsh/site-functions:${FPATH}\" autoload -Uz compinit compinit fi # from brew install info export NVM_DIR=\"$HOME/.nvm\" [ -s \"/opt/homebrew/opt/nvm/nvm.sh\" ] && \\. \"/opt/homebrew/opt/nvm/nvm.sh\" # This loads nvm [ -s \"/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm\" ] && \\. \"/opt/homebrew/opt/nvm/etc/bash_completion.d/nvm\" # This loads nvm bash_completion # gpg export GPG_TTY=$(tty) eval $(thefuck --alias)","title":"~/.zshrc snippets"},{"location":"mac/#usual-git-stuff","text":"## alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status ## base config MAIL_ADDRESS=\"jan.weyrich@example.de\" git config --global user.name \"Jan Weyrich\" git config --global user.email \"$MAIL_ADDRESS\" git config --global core.hooksPath .githooks ## generate ssh key ssh-keygen -t ed25519 -C \"$MAIL_ADDRESS\" # add ssh key agent for passwort prompt # see https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent#adding-your-ssh-key-to-the-ssh-agent ## gpg brew install gpg2 gnupg pinentry-mac gpg --full-generate-key # config for git gpg -K --keyid-format SHORT export GPG_SEC_ID=8627004B gpg --armor --export $GPG_SEC_ID > ~/gpg-public.pem git config --global user.signingkey $GPG_SEC_ID git config --global gpg.program $(which gpg) git config --global commit.gpgsign true echo 'export GPG_TTY=$(tty)' >> ~/.zshrc source ~/.zshrc # for pw prompt see here: https://unixb0y.de/blog/articles/2019-01/gpg-password-macos-keychain echo 'use-agent' > ~/.gnupg/gpg.conf echo \"pinentry-program /opt/homebrew/bin/pinentry-mac\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent # check with echo \"test\" | gpg --clearsign # if you got: gpg: signing failed: No pinentry # you may want to try with: echo \"/opt/homebrew/bin/pinentry\" > ~/.gnupg/gpg-agent.conf gpgconf --kill gpg-agent ## save passphrase for twelve hours echo \"default-cache-ttl 43200\" >> ~/.gnupg/gpg-agent.conf","title":"usual git stuff"},{"location":"mac/#completion-stopped-working","text":"rm -f ~/.zcompdump; compinit Source","title":"completion stopped working"},{"location":"mac/#colima","text":"# to debug export DEBUG=testcontainers colima start docker context use colima docker run hello-world Could not get testcontainers to work with colima. Here are some links with further information: https://github.com/testcontainers/testcontainers-java/issues/5034#issuecomment-1036433226 https://stackoverflow.com/questions/70749679/how-can-i-use-testcontainer-in-mac-os-with-out-docker-desktop","title":"colima"},{"location":"mac/#m1-compatibility-problems","text":"","title":"m1 compatibility problems"},{"location":"mac/#multi-arch-build-for-arm-and-x86","text":"See Docker build","title":"multi-arch build for arm and x86"},{"location":"mac/#docker-build","text":"echo \"# for docker build on m1\" >> ~/.zshrc echo \"export DOCKER_BUILDKIT=0\" >> ~/.zshrc echo \"export COMPOSE_DOCKER_CLI_BUILD=0\" >> ~/.zshrc source ~/.zshrc failed to solve with frontend dockerfile.v0: failed to create LLB definition: rpc error: code = Unknown desc = error getting credentials - err: docker-credential-osxkeychain resolves to executable in current directory (./docker-credential-osxkeychain), out: `` Source","title":"docker build"},{"location":"mac/#terraform","text":"Provider registry.terraform.io/hashicorp/template v2.2.0 does not have a package available for your current platform, darwin_arm64. Terraform forum brew install kreuzwerker/taps/m1-terraform-provider-helper m1-terraform-provider-helper activate m1-terraform-provider-helper install hashicorp/template -v v2.2.0","title":"terraform"},{"location":"mac/#extend-sudoers-file","text":"\u00b4\u00b4\u00b4shell sudo visudo /private/etc/sudoers","title":"extend sudoers file"},{"location":"mac/#add","text":"","title":"add"},{"location":"mac/#jwy-allall-all","text":"\u00b4\u00b4\u00b4","title":"jwy ALL=(ALL) ALL"},{"location":"maven-memory-aid/","text":"maven memory aid CLI options reference cli completion Bash completion oh-myzsh plugin for completion but it is a bit slow useful commands mvn exec:java -Dexec.mainClass=de.weyrich.MainClassName mvn help:system prints system properties and environment variables. mvn help:effective-settings print settings. mvn help:effective-pom creates a pom which is used after the interpolation, inheritance and use of active profiles. when maven ignores local m2 Sometimes maven ignores local artifacts in .m2 directory. Depending on the version it might help to delete _maven.repositories or _remote.repositories in the artifact direcotry in .m2 direcotry. Explanation find ~/.m2/repository -name _maven.repositories -exec rm -v {} \\; find ~/.m2/repository -name _remote.repositories -exec rm -v {} \\; phase vs. goal Nice graphic: https://medium.com/@yetanothersoftwareengineer/maven-lifecycle-phases-plugins-and-goals-25d8e33fa22 Default phase and goal bindings: https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Built-in_Lifecycle_Bindings maven and ci Maven revisions Stackoverflow local vs. remote Repo maven uses a local and a remote repository. By default the local repo is located under: ~/.m2/repository The default remote repo is located under: https://repo.maven.apache.org/maven2/ The remote repository can be defined in the the user ~/.m2/settings.xml or in the pom.xml or the global ${maven.home}/conf/settings.xml . The precedence of the defined repositories in a xml file is from top to down. The files have the following precedence: Global settings.xml User settings.xml Local POM Parent POMs, recursively Super POM which depends on the used maven version You can define mirrors in the settings.xml which will redirect specific or wildcard repositories to another one. The mirrorOf element contains the id of the repository you want to redirect (or a wildcard * ) and an url element which points to the target repository. repository mirrors https://maven.apache.org/guides/mini/guide-mirror-settings.html is set in settings.xml redirects traffic of an Repository via mirrorOf to another url package vs install vs deploy mvn package builds a project and save the artifacts in the target folder. mvn install calls the package command and save the artifacts in the local .m2 repo. mvn deploy calls the install command and save the artifacts in the remote repo, which is defined in the <distributionManagement> xml element in the settings.xml. In order to resolve dependencies they must exist in the local or the remote repo. The exception are dependencies on projects, which are build together. parent vs. aggregator pom Combinations of parent and aggregator poms are possible. A aggregator pom allows to build multiple projects. ... <modules> <module>artifactID-A</modules> <module>artifactID-b</modules> </modules> ... A parent pom is referenced from within a child via the <parent> element. The child project inherits the dependencies,the plugins and defined repositories. These can be overwritten in the child pom if needed. ... <parent> <groupId>parentGroupId</groupId> <artifactId>parentID</artifactId> <version>1.0.0</version> </parent> ... importing projects An alternative to a parent project is to import a project. This mechanism allows to import multiple projects instead of one as is possible via parent pom mechanism. This is accomplished by declaring a pom artifact as a dependency with a scope of \"import\" in the dependencyManagement section. ... <dependencyManagement> <dependencies> <dependency> <groupId>maven</groupId> <artifactId>A</artifactId> <version>1.0</version> <type>pom</type> <scope>import</scope> </dependency> </dependencies> </dependencyManagement> ... </project> ... dependencies vs. dependencyManagement The use of dependencyManagement only makes sense if it is used in a parent pom. It is used to manage the versions of dependencies across multiple projects. Dependencies defined in dependencyManagement are not used during a maven build. To do so they need to be included in dependencies block, but the version tag of the dependency can no be omitted in the dependencies block because it is already defined in dependencyManagement block. plugin vs. pluginManagement Works like dependencies/ dependencyManagementsee only for plugins. goals and phases There is a nice overview under: https://www.baeldung.com/maven-goals-phases mvn help:describe -Dcmd=package shows the lifecylce phases of a project and which maven plugins run when. SNAPTSHOT-version vs version use case Source: https://stackoverflow.com/questions/5901378/what-exactly-is-a-maven-snapshot-and-why-do-we-need-it A snapshot version in maven is one that has not been released. The idea is that before a 1.0 release (or any other release) is done, there exists a 1.0-SNAPSHOT. That version is what might become 1.0. It's basically \"1.0 under development\". This might be close to a real 1.0 release, or pretty far (right after the 0.9 release, for example). The difference between a \"real\" version and a snapshot version is that snapshots might get updates. That means that downloading 1.0-SNAPSHOT today might give a different file than downloading it yesterday or tomorrow. Usually, snapshot dependencies should only exist during development and no released version (i.e. no non-snapshot) should have a dependency on a snapshot version. behaviour Source: https://stackoverflow.com/questions/5901378/what-exactly-is-a-maven-snapshot-and-why-do-we-need-it When you build an application, maven will search for dependencies in the local repository. If a stable version is not found there, it will search the remote repositories (defined in settings.xml or pom.xml) to retrieve this dependency. Then, it will copy it into the local repository, to make it available for the next builds. For example, a foo-1.0.jar library is considered as a stable version, and if maven finds it in the local repository, it will use this one for the current build. Now, if you need a foo-1.0-SNAPSHOT.jar library, maven will know that this version is not stable and is subject to changes. That's why maven will try to find a newer version in the remote repositories, even if a version of this library is found on the local repository. However, this check is made only once per day. That means that if you have a foo-1.0-20110506.110000-1.jar (i.e. this library has been generated on 2011/05/06 at 11:00:00) in your local repository, and if you run the maven build again the same day, maven will not check the repositories for a newer version. known errors in with parent pom projects Maven: Failed to read artifact descriptor Cause: The parent pom project needs to be build so the pom can be uploaded into local and/or remote repo Solution: run mvn install or mvn deploy to save the pom in local or remote repo. create remote repo from m2 directory Artpie How to generate build-info.properties add spring-boot-maven-plugin with execution goal build-info <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> <version>2.0.1.RELEASE</version> <executions> <execution> <goals> <goal>build-info</goal> </goals> </execution> </executions> </plugin> run mvn spring-boot:build-info generate asciidoctor if plugin is configured properly mvn org.asciidoctor:asciidoctor-maven-plugin:process-asciidoc","title":"maven memory aid"},{"location":"maven-memory-aid/#maven-memory-aid","text":"CLI options reference","title":"maven memory aid"},{"location":"maven-memory-aid/#cli-completion","text":"Bash completion oh-myzsh plugin for completion but it is a bit slow","title":"cli completion"},{"location":"maven-memory-aid/#useful-commands","text":"mvn exec:java -Dexec.mainClass=de.weyrich.MainClassName mvn help:system prints system properties and environment variables. mvn help:effective-settings print settings. mvn help:effective-pom creates a pom which is used after the interpolation, inheritance and use of active profiles.","title":"useful commands"},{"location":"maven-memory-aid/#when-maven-ignores-local-m2","text":"Sometimes maven ignores local artifacts in .m2 directory. Depending on the version it might help to delete _maven.repositories or _remote.repositories in the artifact direcotry in .m2 direcotry. Explanation find ~/.m2/repository -name _maven.repositories -exec rm -v {} \\; find ~/.m2/repository -name _remote.repositories -exec rm -v {} \\;","title":"when maven ignores local m2"},{"location":"maven-memory-aid/#phase-vs-goal","text":"Nice graphic: https://medium.com/@yetanothersoftwareengineer/maven-lifecycle-phases-plugins-and-goals-25d8e33fa22 Default phase and goal bindings: https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Built-in_Lifecycle_Bindings","title":"phase vs. goal"},{"location":"maven-memory-aid/#maven-and-ci","text":"Maven revisions Stackoverflow","title":"maven and ci"},{"location":"maven-memory-aid/#local-vs-remote-repo","text":"maven uses a local and a remote repository. By default the local repo is located under: ~/.m2/repository The default remote repo is located under: https://repo.maven.apache.org/maven2/ The remote repository can be defined in the the user ~/.m2/settings.xml or in the pom.xml or the global ${maven.home}/conf/settings.xml . The precedence of the defined repositories in a xml file is from top to down. The files have the following precedence: Global settings.xml User settings.xml Local POM Parent POMs, recursively Super POM which depends on the used maven version You can define mirrors in the settings.xml which will redirect specific or wildcard repositories to another one. The mirrorOf element contains the id of the repository you want to redirect (or a wildcard * ) and an url element which points to the target repository.","title":"local vs. remote Repo"},{"location":"maven-memory-aid/#repository-mirrors","text":"https://maven.apache.org/guides/mini/guide-mirror-settings.html is set in settings.xml redirects traffic of an Repository via mirrorOf to another url","title":"repository mirrors"},{"location":"maven-memory-aid/#package-vs-install-vs-deploy","text":"mvn package builds a project and save the artifacts in the target folder. mvn install calls the package command and save the artifacts in the local .m2 repo. mvn deploy calls the install command and save the artifacts in the remote repo, which is defined in the <distributionManagement> xml element in the settings.xml. In order to resolve dependencies they must exist in the local or the remote repo. The exception are dependencies on projects, which are build together.","title":"package vs install vs deploy"},{"location":"maven-memory-aid/#parent-vs-aggregator-pom","text":"Combinations of parent and aggregator poms are possible. A aggregator pom allows to build multiple projects. ... <modules> <module>artifactID-A</modules> <module>artifactID-b</modules> </modules> ... A parent pom is referenced from within a child via the <parent> element. The child project inherits the dependencies,the plugins and defined repositories. These can be overwritten in the child pom if needed. ... <parent> <groupId>parentGroupId</groupId> <artifactId>parentID</artifactId> <version>1.0.0</version> </parent> ...","title":"parent vs. aggregator pom"},{"location":"maven-memory-aid/#importing-projects","text":"An alternative to a parent project is to import a project. This mechanism allows to import multiple projects instead of one as is possible via parent pom mechanism. This is accomplished by declaring a pom artifact as a dependency with a scope of \"import\" in the dependencyManagement section. ... <dependencyManagement> <dependencies> <dependency> <groupId>maven</groupId> <artifactId>A</artifactId> <version>1.0</version> <type>pom</type> <scope>import</scope> </dependency> </dependencies> </dependencyManagement> ... </project> ...","title":"importing projects"},{"location":"maven-memory-aid/#dependencies-vs-dependencymanagement","text":"The use of dependencyManagement only makes sense if it is used in a parent pom. It is used to manage the versions of dependencies across multiple projects. Dependencies defined in dependencyManagement are not used during a maven build. To do so they need to be included in dependencies block, but the version tag of the dependency can no be omitted in the dependencies block because it is already defined in dependencyManagement block.","title":"dependencies vs. dependencyManagement"},{"location":"maven-memory-aid/#plugin-vs-pluginmanagement","text":"Works like dependencies/ dependencyManagementsee only for plugins.","title":"plugin vs. pluginManagement"},{"location":"maven-memory-aid/#goals-and-phases","text":"There is a nice overview under: https://www.baeldung.com/maven-goals-phases mvn help:describe -Dcmd=package shows the lifecylce phases of a project and which maven plugins run when.","title":"goals and phases"},{"location":"maven-memory-aid/#snaptshot-version-vs-version","text":"","title":"SNAPTSHOT-version vs version"},{"location":"maven-memory-aid/#use-case","text":"Source: https://stackoverflow.com/questions/5901378/what-exactly-is-a-maven-snapshot-and-why-do-we-need-it A snapshot version in maven is one that has not been released. The idea is that before a 1.0 release (or any other release) is done, there exists a 1.0-SNAPSHOT. That version is what might become 1.0. It's basically \"1.0 under development\". This might be close to a real 1.0 release, or pretty far (right after the 0.9 release, for example). The difference between a \"real\" version and a snapshot version is that snapshots might get updates. That means that downloading 1.0-SNAPSHOT today might give a different file than downloading it yesterday or tomorrow. Usually, snapshot dependencies should only exist during development and no released version (i.e. no non-snapshot) should have a dependency on a snapshot version.","title":"use case"},{"location":"maven-memory-aid/#behaviour","text":"Source: https://stackoverflow.com/questions/5901378/what-exactly-is-a-maven-snapshot-and-why-do-we-need-it When you build an application, maven will search for dependencies in the local repository. If a stable version is not found there, it will search the remote repositories (defined in settings.xml or pom.xml) to retrieve this dependency. Then, it will copy it into the local repository, to make it available for the next builds. For example, a foo-1.0.jar library is considered as a stable version, and if maven finds it in the local repository, it will use this one for the current build. Now, if you need a foo-1.0-SNAPSHOT.jar library, maven will know that this version is not stable and is subject to changes. That's why maven will try to find a newer version in the remote repositories, even if a version of this library is found on the local repository. However, this check is made only once per day. That means that if you have a foo-1.0-20110506.110000-1.jar (i.e. this library has been generated on 2011/05/06 at 11:00:00) in your local repository, and if you run the maven build again the same day, maven will not check the repositories for a newer version.","title":"behaviour"},{"location":"maven-memory-aid/#known-errors-in-with-parent-pom-projects","text":"Maven: Failed to read artifact descriptor Cause: The parent pom project needs to be build so the pom can be uploaded into local and/or remote repo Solution: run mvn install or mvn deploy to save the pom in local or remote repo.","title":"known errors in with parent pom projects"},{"location":"maven-memory-aid/#create-remote-repo-from-m2-directory","text":"Artpie How to","title":"create remote repo from m2 directory"},{"location":"maven-memory-aid/#generate-build-infoproperties","text":"add spring-boot-maven-plugin with execution goal build-info <plugin> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-maven-plugin</artifactId> <version>2.0.1.RELEASE</version> <executions> <execution> <goals> <goal>build-info</goal> </goals> </execution> </executions> </plugin> run mvn spring-boot:build-info","title":"generate build-info.properties"},{"location":"maven-memory-aid/#generate-asciidoctor-if-plugin-is-configured-properly","text":"mvn org.asciidoctor:asciidoctor-maven-plugin:process-asciidoc","title":"generate asciidoctor if plugin is configured properly"},{"location":"medium/","text":"Medium inline code on german qwertz keyboard To create a inline code block ` + arrow right + space. But this does not work when you want to convert an exsisting textblock to inline code. To do this configure your input to use american layout and press < key. More details here","title":"Medium"},{"location":"medium/#medium","text":"","title":"Medium"},{"location":"medium/#inline-code-on-german-qwertz-keyboard","text":"To create a inline code block ` + arrow right + space. But this does not work when you want to convert an exsisting textblock to inline code. To do this configure your input to use american layout and press < key. More details here","title":"inline code on german qwertz keyboard"},{"location":"metrics/","text":"Metrics k8s components Components: metric-server - Is used for vertical and horizontal pod autoscaler and should not be used for other purposes. Only in memory, no history. kube-state-metrics (KSM) - expose metrics. Is used by prometheus. Only in memory, no history. prometheus interaction of components: metric-server (in memory) is used to collect metrics kubelet: exposes the metrics of the node (and all its pods) which can then be collected by the metric server cAdvisor is the subcomponent of the kubelet who collect the metrics from the pods metrics-server can be deployed with: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Prometheus promql snippets # list metrics {__name__!=\"\"} Alert There are two different types of rules: Alerts and Records. Alerts are used to monitor resources; when the Prometheus expression is fulfilled, an alarm is created. Records are used for pre-calculation of expressions. You need AlertReciver if you want to send an E-Mail or Notify via other way see . Example of alert: apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-rule-with-alert spec: groups: - name: general.rules rules: - alert: example annotations: description: 'Shown in OCP' summary: \"might be used to describe promql\" expr: count(kube_job_status_failed{job_name=~'.*'}==1) > 2 for: 60m labels: severity: warning Grafana See you github Repo how to use operator to install grafana and configure datasources.","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#k8s-components","text":"Components: metric-server - Is used for vertical and horizontal pod autoscaler and should not be used for other purposes. Only in memory, no history. kube-state-metrics (KSM) - expose metrics. Is used by prometheus. Only in memory, no history. prometheus interaction of components: metric-server (in memory) is used to collect metrics kubelet: exposes the metrics of the node (and all its pods) which can then be collected by the metric server cAdvisor is the subcomponent of the kubelet who collect the metrics from the pods metrics-server can be deployed with: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml","title":"k8s components"},{"location":"metrics/#prometheus","text":"","title":"Prometheus"},{"location":"metrics/#promql-snippets","text":"# list metrics {__name__!=\"\"}","title":"promql snippets"},{"location":"metrics/#alert","text":"There are two different types of rules: Alerts and Records. Alerts are used to monitor resources; when the Prometheus expression is fulfilled, an alarm is created. Records are used for pre-calculation of expressions. You need AlertReciver if you want to send an E-Mail or Notify via other way see . Example of alert: apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: prometheus-rule-with-alert spec: groups: - name: general.rules rules: - alert: example annotations: description: 'Shown in OCP' summary: \"might be used to describe promql\" expr: count(kube_job_status_failed{job_name=~'.*'}==1) > 2 for: 60m labels: severity: warning","title":"Alert"},{"location":"metrics/#grafana","text":"See you github Repo how to use operator to install grafana and configure datasources.","title":"Grafana"},{"location":"mockito-memoery-aid/","text":"Mockito memory aid rule of thumb: Do not mock types you don\u2019t own might seem obvious but do not mock the class under test. Overview of mockito annotations import static org.mockito.Mockito.*; import static org.mockito.ArgumentMatchers.*; activate mockito annotations support Junit5: add @ExtendWith(MockitoExtension.class) to class Junit4: add @RunWith(MockitoJUnitRunner.class) to class Programmatically: MockitoAnnotations.initMocks(this); mock object Inline mock an object: List mockedList = mock(List.class); static import: import static org.mockito.Mockito.*; with annotation: add @Mock on a object to mock the object use @InjectMocks to mock all fields of the annotated object. All fields of the mocked class needs to be present in the testing class and be annotated with @Mock or define behaviour: given - preconidtion when() - action to call thenReturn() - returned values example: when(mockedList.get(1)).thenReturn(\"first\"); ArgumentMatchers You can use the ArgumentMatchers in the package org.mockito.ArgumentMatchers to pass arbitrary arguments to the mocked method. Example: when(mockedList.get(any())).thenReturn(\"first\"); . When using matchers, all arguments have to be provided by matchers. If you want to give some real arguments you need to use wrappers like eq intercept object/ partially mock The real methods of a spied object are invoked but the object can still be verified and stubbed, when needed. Inline spy an object: List spyList = Mockito.spy(new ArrayList ()); static import: import static org.mockito.Mockito.*; with annotation: add @Spy on a object to spy the object use doReturn/when to mock methods of a spied object check behaviour of object Use verify() to check if certain conditions are met: was a mocked method called which parameters are passed to a mocked method how often is a mocked method called","title":"Mockito memory aid"},{"location":"mockito-memoery-aid/#mockito-memory-aid","text":"rule of thumb: Do not mock types you don\u2019t own might seem obvious but do not mock the class under test. Overview of mockito annotations import static org.mockito.Mockito.*; import static org.mockito.ArgumentMatchers.*;","title":"Mockito memory aid"},{"location":"mockito-memoery-aid/#activate-mockito-annotations-support","text":"Junit5: add @ExtendWith(MockitoExtension.class) to class Junit4: add @RunWith(MockitoJUnitRunner.class) to class Programmatically: MockitoAnnotations.initMocks(this);","title":"activate mockito annotations support"},{"location":"mockito-memoery-aid/#mock-object","text":"Inline mock an object: List mockedList = mock(List.class); static import: import static org.mockito.Mockito.*; with annotation: add @Mock on a object to mock the object use @InjectMocks to mock all fields of the annotated object. All fields of the mocked class needs to be present in the testing class and be annotated with @Mock or define behaviour: given - preconidtion when() - action to call thenReturn() - returned values example: when(mockedList.get(1)).thenReturn(\"first\");","title":"mock object"},{"location":"mockito-memoery-aid/#argumentmatchers","text":"You can use the ArgumentMatchers in the package org.mockito.ArgumentMatchers to pass arbitrary arguments to the mocked method. Example: when(mockedList.get(any())).thenReturn(\"first\"); . When using matchers, all arguments have to be provided by matchers. If you want to give some real arguments you need to use wrappers like eq","title":"ArgumentMatchers"},{"location":"mockito-memoery-aid/#intercept-object-partially-mock","text":"The real methods of a spied object are invoked but the object can still be verified and stubbed, when needed. Inline spy an object: List spyList = Mockito.spy(new ArrayList ()); static import: import static org.mockito.Mockito.*; with annotation: add @Spy on a object to spy the object use doReturn/when to mock methods of a spied object","title":"intercept object/ partially mock"},{"location":"mockito-memoery-aid/#check-behaviour-of-object","text":"Use verify() to check if certain conditions are met: was a mocked method called which parameters are passed to a mocked method how often is a mocked method called","title":"check behaviour of object"},{"location":"nginx/","text":"nginx memory aid Nice tool to generate a nginx config . You may need to generate a Diffie-Hellman parameter for DHE ciphersuites: openssl dhparam -out dhparam.pem 4096 Guide how to configure nginx as reverse proxy with tls and docker directories config: /etc/nginx logs: /var/log/nginx files which getting served: /var/www commands nginx - start server nginx -s stop - stop the server nginx -s reload - reload the config nginx -t - test config ln -s /etc/nginx/sites-available/weyrich.dev.conf /etc/nginx/sites-enabled/weyrich.dev.conf - create symlink from a conf file in sites-available to sites-enabled. Remember to use a full qualified path configuration global config /etc/nginx/nginx.conf set some standard settings and loads the config files in following two folders. Files in /etc/nginx/conf.d/ which ends with .conf Files in /etc/nginx/sites-enabled which are usually only a symlink to files in /etc/nginx/sites-available. Contains the default configuration of nginx. configuration directives Reference The guide on how config blocks are chosen by nginx is nice. The outline is: find server block with the most 'specific' ip address and port in the listen directive if multiple listen directives are equally specific (or just equal) then the server_name directive will decide which will execute it. Search for the most 'specific one' Example of reverse proxy with tls support which proxies to a non-tls server (except for the subdomain it is generated the tool ): server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name weyrich.dev; root /var/www/weyrich.dev/public; # SSL ssl_certificate /etc/letsencrypt/live/weyrich.dev/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/weyrich.dev/privkey.pem; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/weyrich.dev.access.log; error_log /var/log/nginx/weyrich.dev.error.log warn; # index.html fallback location / { try_files $uri $uri/ /index.html; } # additional config include nginxconfig.io/general.conf; } # ci subdomains proxy pass server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name ci.weyrich.dev; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/ci.weyrich.dev.access.log; error_log /var/log/nginx/ci.weyrich.dev.error.log warn; # reverse proxy location / { proxy_pass http://127.0.0.1:180/; include nginxconfig.io/proxy.conf; } } # HTTP redirect server { listen 78.47.255.193:80; listen [::]:80; server_name .weyrich.dev; return 301 https://weyrich.dev$request_uri; }","title":"nginx memory aid"},{"location":"nginx/#nginx-memory-aid","text":"Nice tool to generate a nginx config . You may need to generate a Diffie-Hellman parameter for DHE ciphersuites: openssl dhparam -out dhparam.pem 4096 Guide how to configure nginx as reverse proxy with tls and docker","title":"nginx memory aid"},{"location":"nginx/#directories","text":"config: /etc/nginx logs: /var/log/nginx files which getting served: /var/www","title":"directories"},{"location":"nginx/#commands","text":"nginx - start server nginx -s stop - stop the server nginx -s reload - reload the config nginx -t - test config ln -s /etc/nginx/sites-available/weyrich.dev.conf /etc/nginx/sites-enabled/weyrich.dev.conf - create symlink from a conf file in sites-available to sites-enabled. Remember to use a full qualified path","title":"commands"},{"location":"nginx/#configuration","text":"global config /etc/nginx/nginx.conf set some standard settings and loads the config files in following two folders. Files in /etc/nginx/conf.d/ which ends with .conf Files in /etc/nginx/sites-enabled which are usually only a symlink to files in /etc/nginx/sites-available. Contains the default configuration of nginx.","title":"configuration"},{"location":"nginx/#configuration-directives","text":"Reference The guide on how config blocks are chosen by nginx is nice. The outline is: find server block with the most 'specific' ip address and port in the listen directive if multiple listen directives are equally specific (or just equal) then the server_name directive will decide which will execute it. Search for the most 'specific one' Example of reverse proxy with tls support which proxies to a non-tls server (except for the subdomain it is generated the tool ): server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name weyrich.dev; root /var/www/weyrich.dev/public; # SSL ssl_certificate /etc/letsencrypt/live/weyrich.dev/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/weyrich.dev/privkey.pem; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/weyrich.dev.access.log; error_log /var/log/nginx/weyrich.dev.error.log warn; # index.html fallback location / { try_files $uri $uri/ /index.html; } # additional config include nginxconfig.io/general.conf; } # ci subdomains proxy pass server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name ci.weyrich.dev; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/ci.weyrich.dev.access.log; error_log /var/log/nginx/ci.weyrich.dev.error.log warn; # reverse proxy location / { proxy_pass http://127.0.0.1:180/; include nginxconfig.io/proxy.conf; } } # HTTP redirect server { listen 78.47.255.193:80; listen [::]:80; server_name .weyrich.dev; return 301 https://weyrich.dev$request_uri; }","title":"configuration directives"},{"location":"oh-my-zsh/","text":"oh-my-zsh used plugins see ~/.zshrc # ZSH_THEME=\"robbyrussell\" ZSH_THEME=\"powerlevel10k\" ... plugins=(git podman zsh-syntax-highlighting you-should-use) prerequisite # zsh sudo dnf install zsh chsh -s $(which zsh) # oh-my-zsh ##TODO or link to webpage # oh-my-zsh theme git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k # font is missing and can be installed with help of ## [GitHub - romkatv/powerlevel10k: A Zsh theme](https://github.com/romkatv/powerlevel10k?tab=readme-ov-file#meslo-nerd-font-patched-for-powerlevel10k) # # oh-my-zsh plugin install # list of automatically installed plugins https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/docker/README.md # zsh-syntax-highlight git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting ## [You-should-use Plugin](https://github.com/MichaelAquilina/zsh-you-should-use?ref=catalins.tech) git clone https://github.com/MichaelAquilina/zsh-you-should-use.git $ZSH_CUSTOM/plugins/you-should-use sudo dnf install python-pygments # usage ccat or cless to have highlighting powerlevel10k Changes quite a bit of behavior. e.g. \"Transient prompt\": removes the path after execution so it is easier to copy. Change it with p10k configure .","title":"oh-my-zsh"},{"location":"oh-my-zsh/#oh-my-zsh","text":"","title":"oh-my-zsh"},{"location":"oh-my-zsh/#used-plugins","text":"see ~/.zshrc # ZSH_THEME=\"robbyrussell\" ZSH_THEME=\"powerlevel10k\" ... plugins=(git podman zsh-syntax-highlighting you-should-use)","title":"used plugins"},{"location":"oh-my-zsh/#prerequisite","text":"# zsh sudo dnf install zsh chsh -s $(which zsh) # oh-my-zsh ##TODO or link to webpage # oh-my-zsh theme git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k # font is missing and can be installed with help of ## [GitHub - romkatv/powerlevel10k: A Zsh theme](https://github.com/romkatv/powerlevel10k?tab=readme-ov-file#meslo-nerd-font-patched-for-powerlevel10k) # # oh-my-zsh plugin install # list of automatically installed plugins https://github.com/ohmyzsh/ohmyzsh/blob/master/plugins/docker/README.md # zsh-syntax-highlight git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting ## [You-should-use Plugin](https://github.com/MichaelAquilina/zsh-you-should-use?ref=catalins.tech) git clone https://github.com/MichaelAquilina/zsh-you-should-use.git $ZSH_CUSTOM/plugins/you-should-use sudo dnf install python-pygments # usage ccat or cless to have highlighting","title":"prerequisite"},{"location":"oh-my-zsh/#powerlevel10k","text":"Changes quite a bit of behavior. e.g. \"Transient prompt\": removes the path after execution so it is easier to copy. Change it with p10k configure .","title":"powerlevel10k"},{"location":"pulumi/","text":"Pulumi see current state # show urns of resources pulumi state -u # see state of stack pulumi stack pulumi stack export # not sure if this works pulumi state -u -l work with secrets \"By default, the encryption method uses automatic, per-stack encryption keys provided by Pulumi Cloud, but you can also use a provider of your own choosing instead .\" # configure secret provider in new project pulumi stack init dev --secrets-provider passphrase # configure secret provider in existing project # see https://www.pulumi.com/docs/cli/commands/pulumi_stack_change-secrets-provider/ pulumi stack change-secrets-provider TYPE # your passphrase to unlock config/secrets export PULUMI_CONFIG_PASSPHRASE=\"STACKPASSWORD\" login to cloud provider azure Pulumi runs against the default azure subscription. If you got authentifcation errors you might want to check if you selected the right subscription. az login az account list az account set --subscription=<id> pulumi print values One option is to define output properties and then run pulumi stack output . You can define output in Typescript by exporting a variable like this export const someId = resource.id string interpolation Use pulumi.concat or pulumi.interpolate. For deails see pulumi native vs. classic In doubt use native because. it exposes the full api of a provider (beware of breaking api changes) in case of azure it only exposes the ARM api which is a (unified) subset of the rest api (see) it allows to configure the location for the whole stack: pulumi config set azure-native:location WestUS2 The classic versions are based on the terraform providers. Pulumi blog pulumi flow for pulumi native create in web ui see template of ressource azure: \"Automation\" > \"Export template\" you can copy part of the stuff to pulumi refresh state If you got diff between pulumi state and actual state. pulumi up --refresh pulumi refresh see diff pulumi preview -diff or pulumi up -diff use custom backend setup with standard login. The initial project nearly setups an backend. Run pulumi up and then switch the backend to newly created resources. otherwise here is a gist for creating the resources with a bash scripts azure gist gcp gist Login to selfmanaged backend azure: set AZURE_STORAGE_KEY and STORAGE_ACCOUNT_NAME run pulumi login azblob://<container-blob-name> gcp: set GOOGLE_PROJECT and GOOGLE_CREDENTIALS run pulumi login gs://<my-pulumi-state-bucket> access stack name within code In doubt keep stacknames short and the same size when you want to add it to the resources. So you can be sure that length requirements of resource names are fullfilled in every stack and not just in a few. let stack = pulumi.getStack(); Source: https://www.pulumi.com/docs/intro/concepts/stack/#getting-the-current-stack-programmatically stack dependant variables Add an entry under config in the config- .yaml in root. let config = new pulumi.Config(); let name = config.require(\"name\"); Source: Accessing Configuration from Code see api of existing resource azure az resource.. list az containerapp list az containerapp env list az monitor log-analytics workspace list","title":"Pulumi"},{"location":"pulumi/#pulumi","text":"","title":"Pulumi"},{"location":"pulumi/#see-current-state","text":"# show urns of resources pulumi state -u # see state of stack pulumi stack pulumi stack export # not sure if this works pulumi state -u -l","title":"see current state"},{"location":"pulumi/#work-with-secrets","text":"\"By default, the encryption method uses automatic, per-stack encryption keys provided by Pulumi Cloud, but you can also use a provider of your own choosing instead .\" # configure secret provider in new project pulumi stack init dev --secrets-provider passphrase # configure secret provider in existing project # see https://www.pulumi.com/docs/cli/commands/pulumi_stack_change-secrets-provider/ pulumi stack change-secrets-provider TYPE # your passphrase to unlock config/secrets export PULUMI_CONFIG_PASSPHRASE=\"STACKPASSWORD\"","title":"work with secrets"},{"location":"pulumi/#login-to-cloud-provider","text":"","title":"login to cloud provider"},{"location":"pulumi/#azure","text":"Pulumi runs against the default azure subscription. If you got authentifcation errors you might want to check if you selected the right subscription. az login az account list az account set --subscription=<id>","title":"azure"},{"location":"pulumi/#pulumi-print-values","text":"One option is to define output properties and then run pulumi stack output . You can define output in Typescript by exporting a variable like this export const someId = resource.id","title":"pulumi print values"},{"location":"pulumi/#string-interpolation","text":"Use pulumi.concat or pulumi.interpolate. For deails see","title":"string interpolation"},{"location":"pulumi/#pulumi-native-vs-classic","text":"In doubt use native because. it exposes the full api of a provider (beware of breaking api changes) in case of azure it only exposes the ARM api which is a (unified) subset of the rest api (see) it allows to configure the location for the whole stack: pulumi config set azure-native:location WestUS2 The classic versions are based on the terraform providers. Pulumi blog","title":"pulumi native vs. classic"},{"location":"pulumi/#pulumi-flow-for-pulumi-native","text":"create in web ui see template of ressource azure: \"Automation\" > \"Export template\" you can copy part of the stuff to pulumi","title":"pulumi flow for pulumi native"},{"location":"pulumi/#refresh-state","text":"If you got diff between pulumi state and actual state. pulumi up --refresh pulumi refresh","title":"refresh state"},{"location":"pulumi/#see-diff","text":"pulumi preview -diff or pulumi up -diff","title":"see diff"},{"location":"pulumi/#use-custom-backend","text":"setup with standard login. The initial project nearly setups an backend. Run pulumi up and then switch the backend to newly created resources. otherwise here is a gist for creating the resources with a bash scripts azure gist gcp gist Login to selfmanaged backend azure: set AZURE_STORAGE_KEY and STORAGE_ACCOUNT_NAME run pulumi login azblob://<container-blob-name> gcp: set GOOGLE_PROJECT and GOOGLE_CREDENTIALS run pulumi login gs://<my-pulumi-state-bucket>","title":"use custom backend"},{"location":"pulumi/#access-stack-name-within-code","text":"In doubt keep stacknames short and the same size when you want to add it to the resources. So you can be sure that length requirements of resource names are fullfilled in every stack and not just in a few. let stack = pulumi.getStack(); Source: https://www.pulumi.com/docs/intro/concepts/stack/#getting-the-current-stack-programmatically","title":"access stack name within code"},{"location":"pulumi/#stack-dependant-variables","text":"Add an entry under config in the config- .yaml in root. let config = new pulumi.Config(); let name = config.require(\"name\"); Source: Accessing Configuration from Code","title":"stack dependant variables"},{"location":"pulumi/#see-api-of-existing-resource","text":"","title":"see api of existing resource"},{"location":"pulumi/#azure_1","text":"az resource.. list az containerapp list az containerapp env list az monitor log-analytics workspace list","title":"azure"},{"location":"raspberry/","text":"raspberry pi installation use Etcher to create the image add a ssh file to the root of the boot partition to activate ssh like explained here start raspberry pi (and connect to lan wire so you can start right away from the remote computer) use ssh to connect to rp use sudo raspi-config to configure the rp (optional) install pip: sudo apt-get install python-pip power off and restart # shutdown alias sudo poweroff # restart alias sudo reboot # shutdown sudo shutdown -h now # restart sudo shutdown -r now install pip sudo apt-get update sudo apt-get install python-pip install docker on rp # install curl -sSL https://get.docker.com | sh # add use pi to docker group sudo usermod -aG docker pi # reboot sudo reboot # test setup docker run hello-world # hello-world might not work. In this case than use (https://stackoverflow.com/questions/52233182/docker-run-does-not-display-any-output) docker run hypriot/armhf-hello-world docker images for rp In order to run on a rp a docker image need to be compiled for arm processor architecture. You can find a list of supported cpu architectures for docker here . To find out which architecture your rp has see . E.g. a rp 2 mod B uses a ARMv7 architecture. open-jdk install ranchers k3s kubernetes on rp the cpu of the rp 2 mod B seems to be to weak to run k3s server. Always on 100% and timeout on kubectl commands Guide # install k3s on rp and start server ('systemd: Starting k3s' will take a looong time) curl -sfL https://get.k3s.io | sh - # after startup test if it runs sudo kubectl get nodes Uninstall : server node: /usr/local/bin/k3s-uninstall.sh agent node: /usr/local/bin/k3s-agent-uninstall.sh why i ditch my rp over a vls i want a public ip e.g. to use GitHub webhooks via router can be tricky via ngrok you do not get a static url many docker container wont run on arm my rp (2b+) is to weak to run k3s kubernetes","title":"raspberry pi"},{"location":"raspberry/#raspberry-pi","text":"","title":"raspberry pi"},{"location":"raspberry/#installation","text":"use Etcher to create the image add a ssh file to the root of the boot partition to activate ssh like explained here start raspberry pi (and connect to lan wire so you can start right away from the remote computer) use ssh to connect to rp use sudo raspi-config to configure the rp (optional) install pip: sudo apt-get install python-pip","title":"installation"},{"location":"raspberry/#power-off-and-restart","text":"# shutdown alias sudo poweroff # restart alias sudo reboot # shutdown sudo shutdown -h now # restart sudo shutdown -r now","title":"power off and restart"},{"location":"raspberry/#install-pip","text":"sudo apt-get update sudo apt-get install python-pip","title":"install pip"},{"location":"raspberry/#install-docker-on-rp","text":"# install curl -sSL https://get.docker.com | sh # add use pi to docker group sudo usermod -aG docker pi # reboot sudo reboot # test setup docker run hello-world # hello-world might not work. In this case than use (https://stackoverflow.com/questions/52233182/docker-run-does-not-display-any-output) docker run hypriot/armhf-hello-world","title":"install docker on rp"},{"location":"raspberry/#docker-images-for-rp","text":"In order to run on a rp a docker image need to be compiled for arm processor architecture. You can find a list of supported cpu architectures for docker here . To find out which architecture your rp has see . E.g. a rp 2 mod B uses a ARMv7 architecture. open-jdk","title":"docker images for rp"},{"location":"raspberry/#install-ranchers-k3s-kubernetes-on-rp","text":"the cpu of the rp 2 mod B seems to be to weak to run k3s server. Always on 100% and timeout on kubectl commands Guide # install k3s on rp and start server ('systemd: Starting k3s' will take a looong time) curl -sfL https://get.k3s.io | sh - # after startup test if it runs sudo kubectl get nodes Uninstall : server node: /usr/local/bin/k3s-uninstall.sh agent node: /usr/local/bin/k3s-agent-uninstall.sh","title":"install ranchers k3s kubernetes on rp"},{"location":"raspberry/#why-i-ditch-my-rp-over-a-vls","text":"i want a public ip e.g. to use GitHub webhooks via router can be tricky via ngrok you do not get a static url many docker container wont run on arm my rp (2b+) is to weak to run k3s kubernetes","title":"why i ditch my rp over a vls"},{"location":"rpm/","text":"RPMs extract RPMs without installation brew install rpm2cpio rpm2cpio bla.rpm | cpi -idmv # creates folder structure of rpm without installing create RPM Ruby fpm (Effing Package Management) fpm.ruby -s -dir -t rpm -n \"RPM_NAME\" -v \"RPM_VERSION\" -p \"RPM_PATH\" --description \"DESCRIPTION\" --prefix \"DESTINATION_PATH\" -C \"DESTINATION_PATH\" # chatgpg explanation # -s: Specifies the source of the package to be built. It is likely followed by an argument that specifies the type of source (e.g., \"dir\" for a directory). # -dir: Indicates that the package source is a directory. This option is typically used in conjunction with the -s option. # -t rpm: Specifies the target package format to be generated, which, in this case, is an RPM (Red Hat Package Manager) package. # -n \"RPM_NAME\": Specifies the name of the RPM package being created. Replace \"RPM_NAME\" with the desired name. # -v \"RPM_VERSION\": Specifies the version of the RPM package being created. Replace \"RPM_VERSION\" with the desired version. # -p \"RPM_PATH\": Specifies the path where the generated RPM package should be saved. Replace \"RPM_PATH\" with the desired file path. # --description \"DESCRIPTION\": Sets the description for the RPM package. Replace \"DESCRIPTION\" with the desired package description. # --prefix \"DESTINATION_PATH\": Specifies the prefix or base directory for the files contained within the package. Replace \"DESTINATION_PATH\" with the desired path. # -C \"DESTINATION_PATH\": Changes to the specified directory before processing the package. This option is used to ensure that the correct files are included in the package. Replace \"DESTINATION_PATH\" with the desired path.","title":"RPMs"},{"location":"rpm/#rpms","text":"","title":"RPMs"},{"location":"rpm/#extract-rpms-without-installation","text":"brew install rpm2cpio rpm2cpio bla.rpm | cpi -idmv # creates folder structure of rpm without installing","title":"extract RPMs without installation"},{"location":"rpm/#create-rpm","text":"Ruby fpm (Effing Package Management) fpm.ruby -s -dir -t rpm -n \"RPM_NAME\" -v \"RPM_VERSION\" -p \"RPM_PATH\" --description \"DESCRIPTION\" --prefix \"DESTINATION_PATH\" -C \"DESTINATION_PATH\" # chatgpg explanation # -s: Specifies the source of the package to be built. It is likely followed by an argument that specifies the type of source (e.g., \"dir\" for a directory). # -dir: Indicates that the package source is a directory. This option is typically used in conjunction with the -s option. # -t rpm: Specifies the target package format to be generated, which, in this case, is an RPM (Red Hat Package Manager) package. # -n \"RPM_NAME\": Specifies the name of the RPM package being created. Replace \"RPM_NAME\" with the desired name. # -v \"RPM_VERSION\": Specifies the version of the RPM package being created. Replace \"RPM_VERSION\" with the desired version. # -p \"RPM_PATH\": Specifies the path where the generated RPM package should be saved. Replace \"RPM_PATH\" with the desired file path. # --description \"DESCRIPTION\": Sets the description for the RPM package. Replace \"DESCRIPTION\" with the desired package description. # --prefix \"DESTINATION_PATH\": Specifies the prefix or base directory for the files contained within the package. Replace \"DESTINATION_PATH\" with the desired path. # -C \"DESTINATION_PATH\": Changes to the specified directory before processing the package. This option is used to ensure that the correct files are included in the package. Replace \"DESTINATION_PATH\" with the desired path.","title":"create RPM"},{"location":"security_scanner/","text":"Security scanner SBOM A software bill of materials (SBOM) is an industry standard mechanism of surfacing metadata about dependencies in images or applications. Source . An SBOM can be created based on the source code, binaries (language-specific artifacts or container images) or dynamically at runtime. Static SBOM attributes like package metadata can be created at build time. Dynamic data like CVE might change without a new build and need to be handled differently. formats spdx from linux foundation cyclone dx from owasp syft from/with grype SBOM tools Software Composition Analysis (SCA) cdxgen - cli to create CycloneDX for different package managers (no build binaries needed) CLI to generate spdx sbom for different package managers cyclonedx cli from owasp currently supports BOM analysis, modification, diffing, merging, format conversion, signing and verification. DependencyTrack continuous SBOM analysis platform from owasp cve-bin-tool from intel allows to check binary or sbom for CVEs spdx-to-osv cli allows to find Open Source Vulnerabilities in SBOM vulnerability scanner These scanners usually allow to scan binaries, but some also allow to start form a existing SBOM file. OWASP DependencyCheck scanner from owasp (cli or build plugins for maven, gradle, etc.) Trivy multi purpose scanner. Grype a vulnerability scanner for container images and filesystems Clair a vulnerability scanner for container images from redhat examples # generate spdx sbom from maven project spdx-sbom-generator ## creates: bom-Java-Maven.spdx # scan spdx or cyclonedx sbom for vulnerabilities trivy sbom bom-Java-Maven.spdx ## or grype sbom:/bom-Java-Maven.spdx ## generate sbom from jar syft --output cyclonedx-json name.jar > sbom-cyclonedx.json syft --output spdx-json name.jar > sbom-spdx.json ## scan jar for vulnerabilities grype name.jar vulnerability databases Common Vulnerabilities and Exposures (CVE) Open Source Vulnerabilities Database (OSV) National Vulnerability Database (NVD) Sonatype oss index Common Platform Enumeration (CPE) Is a structured naming scheme for information technology systems, software, and packages. Lists DevSecOps","title":"Security scanner"},{"location":"security_scanner/#security-scanner","text":"","title":"Security scanner"},{"location":"security_scanner/#sbom","text":"A software bill of materials (SBOM) is an industry standard mechanism of surfacing metadata about dependencies in images or applications. Source . An SBOM can be created based on the source code, binaries (language-specific artifacts or container images) or dynamically at runtime. Static SBOM attributes like package metadata can be created at build time. Dynamic data like CVE might change without a new build and need to be handled differently.","title":"SBOM"},{"location":"security_scanner/#formats","text":"spdx from linux foundation cyclone dx from owasp syft from/with grype","title":"formats"},{"location":"security_scanner/#sbom-tools","text":"Software Composition Analysis (SCA) cdxgen - cli to create CycloneDX for different package managers (no build binaries needed) CLI to generate spdx sbom for different package managers cyclonedx cli from owasp currently supports BOM analysis, modification, diffing, merging, format conversion, signing and verification. DependencyTrack continuous SBOM analysis platform from owasp cve-bin-tool from intel allows to check binary or sbom for CVEs spdx-to-osv cli allows to find Open Source Vulnerabilities in SBOM","title":"SBOM tools"},{"location":"security_scanner/#vulnerability-scanner","text":"These scanners usually allow to scan binaries, but some also allow to start form a existing SBOM file. OWASP DependencyCheck scanner from owasp (cli or build plugins for maven, gradle, etc.) Trivy multi purpose scanner. Grype a vulnerability scanner for container images and filesystems Clair a vulnerability scanner for container images from redhat","title":"vulnerability scanner"},{"location":"security_scanner/#examples","text":"# generate spdx sbom from maven project spdx-sbom-generator ## creates: bom-Java-Maven.spdx # scan spdx or cyclonedx sbom for vulnerabilities trivy sbom bom-Java-Maven.spdx ## or grype sbom:/bom-Java-Maven.spdx ## generate sbom from jar syft --output cyclonedx-json name.jar > sbom-cyclonedx.json syft --output spdx-json name.jar > sbom-spdx.json ## scan jar for vulnerabilities grype name.jar","title":"examples"},{"location":"security_scanner/#vulnerability-databases","text":"Common Vulnerabilities and Exposures (CVE) Open Source Vulnerabilities Database (OSV) National Vulnerability Database (NVD) Sonatype oss index","title":"vulnerability databases"},{"location":"security_scanner/#common-platform-enumeration-cpe","text":"Is a structured naming scheme for information technology systems, software, and packages.","title":"Common Platform Enumeration (CPE)"},{"location":"security_scanner/#lists","text":"DevSecOps","title":"Lists"},{"location":"sql-dialects/","text":"SQL Dialects Translate one dialect to another For non sensitive information use: https://www.jooq.org/translate/ Otherwise use CLI locally : mkdir jooq-cli && cd jooq-cli curl -O -J -L https://repo1.maven.org/maven2/org/jooq/jooq/3.19.7/jooq-3.19.7.jar curl -O -J -L https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar curl -O -J -L https://repo1.maven.org/maven2/io/r2dbc/r2dbc-spi/1.0.0.RELEASE/r2dbc-spi-1.0.0.RELEASE.jar java -cp jooq-3.19.7.jar:reactive-streams-1.0.3.jar:r2dbc-spi-1.0.0.RELEASE.jar org.jooq.ParserCLI -h --from-dialect=PostgresSQL --to-dialect=","title":"SQL Dialects"},{"location":"sql-dialects/#sql-dialects","text":"","title":"SQL Dialects"},{"location":"sql-dialects/#translate-one-dialect-to-another","text":"For non sensitive information use: https://www.jooq.org/translate/ Otherwise use CLI locally : mkdir jooq-cli && cd jooq-cli curl -O -J -L https://repo1.maven.org/maven2/org/jooq/jooq/3.19.7/jooq-3.19.7.jar curl -O -J -L https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar curl -O -J -L https://repo1.maven.org/maven2/io/r2dbc/r2dbc-spi/1.0.0.RELEASE/r2dbc-spi-1.0.0.RELEASE.jar java -cp jooq-3.19.7.jar:reactive-streams-1.0.3.jar:r2dbc-spi-1.0.0.RELEASE.jar org.jooq.ParserCLI -h --from-dialect=PostgresSQL --to-dialect=","title":"Translate one dialect to another"},{"location":"terminal/","text":"Terminals & Shells A terminal is a device or program that provides a user interface A shell is a command-line interpreter that processes commands. terminal My default profile MY quake profile which open on f12 warp \"sticky\" current command easy to use mouse to navigate the cursor block-based commands which can be bookmarked completion workflows/snippets caveat: login needed iterm2 Tillix","title":"Terminals & Shells"},{"location":"terminal/#terminals-shells","text":"A terminal is a device or program that provides a user interface A shell is a command-line interpreter that processes commands.","title":"Terminals &amp; Shells"},{"location":"terminal/#terminal","text":"My default profile MY quake profile which open on f12","title":"terminal"},{"location":"terminal/#warp","text":"\"sticky\" current command easy to use mouse to navigate the cursor block-based commands which can be bookmarked completion workflows/snippets caveat: login needed","title":"warp"},{"location":"terminal/#iterm2","text":"","title":"iterm2"},{"location":"terminal/#tillix","text":"","title":"Tillix"},{"location":"terraform/","text":"Terraform \"Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files.\" Example tf files official documentation official learn stuff you can also import existing infrastructure into terraform Setup Install manually or use a repository manager enable autocomplete terraform -install-autocomplete Read provider specific setup documentation on terraform registry or here Create folder, create your tf files and run: terraform init && terraform apply Wording a provider creates and manages resources. They wrap the API of a service (provider) like AWS, Azure or GCP. If you use multiple providers you can qualifiy which provider uses which resource. a resource might be a physical resource like a aws EC2 instance or a logical resource like a application. A resource has a type and a name (e.g. resource \"aws_instance\" \"example\"`{...} )and can be configured inside the curly brackets. a datasource is a way to get information about existing infrastructure (mostly useful when it is not managed by terraform or by another terraform configuration) a set of .tf files is called a terraform configuration CLI terraform init # initializes various local settings terraform fmt # format all files in directory terraform validate # validate file terraform apply # checks diff between config file and real infrastructure and creates execution plan to eliminate this diff terraform state # The state of the infrastructure is saved in terraform.tfstate file. It can be manually modified by this command. terraform destroy # completely destroys the Terraform-managed infrastructure Resource Dependencies There are explicit and implicit dependencies for creating a order of actions. explicit: with the depends_on field of a resource. implicit: e.g. usage of instance = aws_instance.example.id Resources which are not dependant on others can be build in parallel. Provisioning Only necessary if you do not use image-based infrastructure (you can create images with vagrant or packer). \"can be used to model specific actions on the local machine or on a remote machine in order to prepare servers or other infrastructure objects for service.\" is not declarative provisioner are defined inside a resource and have a type like: local-exec or remote-exec Are for bootstrapping components (on creation) not to change software on a running component. You need to destroy the infrastructure if the resource already exist, so the resource will be recreated and the bootstrapping logic of the provisioner can be done. If a resource successfully creates but fails during provisioning, Terraform will error and mark the resource as \"tainted\". Terraform tries to destroy and recreate tainted resources every time apply ist called. terraform taint <resource.id> manually marks a resource as tainted. Input Variables Official documentation How to use variable var.my_api_token #use nested var var.system.name #use list element(var.system.used_port, 0) # use map lookup(var.system.port_app, \"80\", \"default_val\") How to declare a variable # with default value and description variable \"my_api_token\" { type = string # number, bool, list(<TYPE>), map(<TYPE>) etc. description = \"An API token.\" default = \"1234-5679-123\" #sensitive=true #does not show value in logs } # nested variable ```hcl variable \"system\" { type = object({ name = string used_ports = list(string) port_app = map(string) }) default = object({ name = \"VLS\" used_ports = [\"80\",\"43\"] port_app ={\"80\":\"http\",\"43\":\"https\"} }) } Where to initialize variable: In a nutshell: preset in variables.tf file via the default field overwrite console ( -var 'var_name=var_value' ) overwrite in terraform.tfvars file overwrite in *.tfvars file and ( -var-file 'production.tfvars' ) overwrite in environmental variables (TF_VARS_) A bit more extensive: variables are defined in a variables.tf file and may be assigned a default value variables are accessed with a var.<variable_name> notation. variables can be overwritten in console: terraform apply -var 'region=us-east-2' variables can also be overwritten from file. Terraform search automatically for terraform.tfvars or .auto.tfvars files. Alternatively you can pass a file in console terraform apply -var-file 'production.tfvars' . variables can be overwritten by environment variables. They need to start with TF_VAR_ . Environment variables are limited to string-type variables (can not use List and map type variables). variables which are unspecified are asked for after executing terraform apply Variable Types Lists # define variable \"cidrs\" { type = list }` # init cidrs = [ \"10.0.0.0/16\", \"10.1.0.0/16\" ] Maps # define and init variable \"amis\" { type = \"map\" default = { \"us-east-1\" = \"ami-b374d5a5\" \"us-west-2\" = \"ami-4b32be2b\" } } # use resource \"aws_instance\" \"example\" { ami = var.amis[\"us-east-1\"] instance_type = \"t2.micro\" } Output Variables Output blocks can be pasted in any of the *.tf files. Output are printed after ``terraform output`` or ``terraform apply`` are run. In a rot module the label of the output block is showed to the user as the name of the variable. In a module the label can be used to reference the variable. You may need to prefix the variable with the terraform type like data or var`. Example: output \"ip\" { value = aws_eip.ip.public_ip } modules Modules are self-contained packages of Terraform configurations that are managed as a group. Any set of Terraform configuration files in a folder can be a module. The official modules can be downloaded from Terraform Registry . use module After adding new modules you need to rerun terraform init not only terraform apply terraform get will download the modules Modules reside under ~/.terraform/modules/ You can define from where to download the module via the source attribute of module local: ./my-module git: 'github.com/crowdsalat/examplemodule' Terraform Registry: source = 'hashicorp/consul/aws' Example for module usage via registry: module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.7.3\" # optional num_servers = \"3\" } create module A module is a bunch of .tf files in a directory The files are only for organization, terraform just merges them to one file before using The name of the module is defined by the folder Files (convention): main.tf variables.tf: input parameters for the module outputs.tf: return values of the module backend.tf: define remote backend variables.tf: used variables Remote State Storage Use a remote backend to store state-data on a server. When not configured the state is stored locally in a terraform.tfstate file There are different backends. Terraform Cloud is one of such. Google cloud backend example: backend \"gcs\" { bucket = \"existing-bucket-name\" } expressions Documentation","title":"Terraform"},{"location":"terraform/#terraform","text":"\"Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files.\" Example tf files official documentation official learn stuff you can also import existing infrastructure into terraform","title":"Terraform"},{"location":"terraform/#setup","text":"Install manually or use a repository manager enable autocomplete terraform -install-autocomplete Read provider specific setup documentation on terraform registry or here Create folder, create your tf files and run: terraform init && terraform apply","title":"Setup"},{"location":"terraform/#wording","text":"a provider creates and manages resources. They wrap the API of a service (provider) like AWS, Azure or GCP. If you use multiple providers you can qualifiy which provider uses which resource. a resource might be a physical resource like a aws EC2 instance or a logical resource like a application. A resource has a type and a name (e.g. resource \"aws_instance\" \"example\"`{...} )and can be configured inside the curly brackets. a datasource is a way to get information about existing infrastructure (mostly useful when it is not managed by terraform or by another terraform configuration) a set of .tf files is called a terraform configuration","title":"Wording"},{"location":"terraform/#cli","text":"terraform init # initializes various local settings terraform fmt # format all files in directory terraform validate # validate file terraform apply # checks diff between config file and real infrastructure and creates execution plan to eliminate this diff terraform state # The state of the infrastructure is saved in terraform.tfstate file. It can be manually modified by this command. terraform destroy # completely destroys the Terraform-managed infrastructure","title":"CLI"},{"location":"terraform/#resource-dependencies","text":"There are explicit and implicit dependencies for creating a order of actions. explicit: with the depends_on field of a resource. implicit: e.g. usage of instance = aws_instance.example.id Resources which are not dependant on others can be build in parallel.","title":"Resource Dependencies"},{"location":"terraform/#provisioning","text":"Only necessary if you do not use image-based infrastructure (you can create images with vagrant or packer). \"can be used to model specific actions on the local machine or on a remote machine in order to prepare servers or other infrastructure objects for service.\" is not declarative provisioner are defined inside a resource and have a type like: local-exec or remote-exec Are for bootstrapping components (on creation) not to change software on a running component. You need to destroy the infrastructure if the resource already exist, so the resource will be recreated and the bootstrapping logic of the provisioner can be done. If a resource successfully creates but fails during provisioning, Terraform will error and mark the resource as \"tainted\". Terraform tries to destroy and recreate tainted resources every time apply ist called. terraform taint <resource.id> manually marks a resource as tainted.","title":"Provisioning"},{"location":"terraform/#input-variables","text":"Official documentation","title":"Input Variables"},{"location":"terraform/#how-to-use-variable","text":"var.my_api_token #use nested var var.system.name #use list element(var.system.used_port, 0) # use map lookup(var.system.port_app, \"80\", \"default_val\")","title":"How to use variable"},{"location":"terraform/#how-to-declare-a-variable","text":"# with default value and description variable \"my_api_token\" { type = string # number, bool, list(<TYPE>), map(<TYPE>) etc. description = \"An API token.\" default = \"1234-5679-123\" #sensitive=true #does not show value in logs } # nested variable ```hcl variable \"system\" { type = object({ name = string used_ports = list(string) port_app = map(string) }) default = object({ name = \"VLS\" used_ports = [\"80\",\"43\"] port_app ={\"80\":\"http\",\"43\":\"https\"} }) }","title":"How to declare a variable"},{"location":"terraform/#where-to-initialize-variable","text":"In a nutshell: preset in variables.tf file via the default field overwrite console ( -var 'var_name=var_value' ) overwrite in terraform.tfvars file overwrite in *.tfvars file and ( -var-file 'production.tfvars' ) overwrite in environmental variables (TF_VARS_) A bit more extensive: variables are defined in a variables.tf file and may be assigned a default value variables are accessed with a var.<variable_name> notation. variables can be overwritten in console: terraform apply -var 'region=us-east-2' variables can also be overwritten from file. Terraform search automatically for terraform.tfvars or .auto.tfvars files. Alternatively you can pass a file in console terraform apply -var-file 'production.tfvars' . variables can be overwritten by environment variables. They need to start with TF_VAR_ . Environment variables are limited to string-type variables (can not use List and map type variables). variables which are unspecified are asked for after executing terraform apply","title":"Where to initialize variable:"},{"location":"terraform/#variable-types","text":"","title":"Variable Types"},{"location":"terraform/#lists","text":"# define variable \"cidrs\" { type = list }` # init cidrs = [ \"10.0.0.0/16\", \"10.1.0.0/16\" ]","title":"Lists"},{"location":"terraform/#maps","text":"# define and init variable \"amis\" { type = \"map\" default = { \"us-east-1\" = \"ami-b374d5a5\" \"us-west-2\" = \"ami-4b32be2b\" } } # use resource \"aws_instance\" \"example\" { ami = var.amis[\"us-east-1\"] instance_type = \"t2.micro\" }","title":"Maps"},{"location":"terraform/#output-variables","text":"Output blocks can be pasted in any of the *.tf files. Output are printed after ``terraform output`` or ``terraform apply`` are run. In a rot module the label of the output block is showed to the user as the name of the variable. In a module the label can be used to reference the variable. You may need to prefix the variable with the terraform type like data or var`. Example: output \"ip\" { value = aws_eip.ip.public_ip }","title":"Output Variables"},{"location":"terraform/#modules","text":"Modules are self-contained packages of Terraform configurations that are managed as a group. Any set of Terraform configuration files in a folder can be a module. The official modules can be downloaded from Terraform Registry .","title":"modules"},{"location":"terraform/#use-module","text":"After adding new modules you need to rerun terraform init not only terraform apply terraform get will download the modules Modules reside under ~/.terraform/modules/ You can define from where to download the module via the source attribute of module local: ./my-module git: 'github.com/crowdsalat/examplemodule' Terraform Registry: source = 'hashicorp/consul/aws' Example for module usage via registry: module \"consul\" { source = \"hashicorp/consul/aws\" version = \"0.7.3\" # optional num_servers = \"3\" }","title":"use module"},{"location":"terraform/#create-module","text":"A module is a bunch of .tf files in a directory The files are only for organization, terraform just merges them to one file before using The name of the module is defined by the folder Files (convention): main.tf variables.tf: input parameters for the module outputs.tf: return values of the module backend.tf: define remote backend variables.tf: used variables","title":"create module"},{"location":"terraform/#remote-state-storage","text":"Use a remote backend to store state-data on a server. When not configured the state is stored locally in a terraform.tfstate file There are different backends. Terraform Cloud is one of such. Google cloud backend example: backend \"gcs\" { bucket = \"existing-bucket-name\" }","title":"Remote State Storage"},{"location":"terraform/#expressions","text":"Documentation","title":"expressions"},{"location":"test-domain/","text":"How to test a HTTP/HTTPS URL with a given IP address # http only curl --header 'Host: a.example' https://x.example # https/SNI curl --head --insecure --resolve a.example:443:b.example https://a.example curl --head --insecure --resolve a.example:443:192.168.1.1 https://a.example # or curl --head --insecure --connect-to a.example:443:x.example:443 https://a.example curl --head --insecure --connect-to a.example:443:192.168.1.1:443 https://a.example Source","title":"How to test a HTTP/HTTPS URL with a given IP address"},{"location":"test-domain/#how-to-test-a-httphttps-url-with-a-given-ip-address","text":"# http only curl --header 'Host: a.example' https://x.example # https/SNI curl --head --insecure --resolve a.example:443:b.example https://a.example curl --head --insecure --resolve a.example:443:192.168.1.1 https://a.example # or curl --head --insecure --connect-to a.example:443:x.example:443 https://a.example curl --head --insecure --connect-to a.example:443:192.168.1.1:443 https://a.example Source","title":"How to test a HTTP/HTTPS URL with a given IP address"},{"location":"testcontainers/","text":"Testcontainers Podman https://podman-desktop.io/tutorial/testcontainers-with-podman Activate Docker compatibility in Podman Desktop under: Settings -> Preferences -> Docker Compability After that testcontainers should work as expected.","title":"Testcontainers"},{"location":"testcontainers/#testcontainers","text":"","title":"Testcontainers"},{"location":"testcontainers/#podman","text":"https://podman-desktop.io/tutorial/testcontainers-with-podman Activate Docker compatibility in Podman Desktop under: Settings -> Preferences -> Docker Compability After that testcontainers should work as expected.","title":"Podman"},{"location":"tools/","text":"helpful stuff rest/api test tools postman might be So here are a few alternatives: OSS bruno Insonmia allows to use with coud or offline OSS hoppscotch OSS Restfox compare tools StackShare CNCF landscape ThoughtWorks Technology Radar assisting tools Subnet calculator Unix permission bit calculator links to overview pages overview of tools/services/libraries for web development learn interactive online katacoda has multiple free interative tutorials (mainly cloud or ci/cd topics) regex crossword GitHub git learning resources Learn git branching interactivly Visualize git boot usb balena etcher screen recorder To create documentation for an ui or demonstrate bugs. Windows: Screen to gif Linux: Deepin screen recorder Peek terminal recorder ascii cinema save terminal session in a cast file which can be played in a terminal or on the website. The cast file is a text file, but it can not be used to be directly copy pasted into a terminal. The asciidoc player is needed. usage: ## install sudo apt-get install asciinema ## record and save in file asciinema rec ./file.cast # exit via crlt + D ## replay a record asciinema play ./file.cast ## dumps the content of a record on console asciinema cat ./file.cast ## record and save to web ( you need account to view it) asciinema rec # exit via crlt + D # press enter disk usage on mac space-radar","title":"helpful stuff"},{"location":"tools/#helpful-stuff","text":"","title":"helpful stuff"},{"location":"tools/#restapi-test-tools","text":"postman might be So here are a few alternatives: OSS bruno Insonmia allows to use with coud or offline OSS hoppscotch OSS Restfox","title":"rest/api test tools"},{"location":"tools/#compare-tools","text":"StackShare CNCF landscape ThoughtWorks Technology Radar","title":"compare tools"},{"location":"tools/#assisting-tools","text":"Subnet calculator Unix permission bit calculator","title":"assisting tools"},{"location":"tools/#links-to-overview-pages","text":"overview of tools/services/libraries for web development","title":"links to overview pages"},{"location":"tools/#learn-interactive-online","text":"katacoda has multiple free interative tutorials (mainly cloud or ci/cd topics) regex crossword GitHub git learning resources Learn git branching interactivly Visualize git","title":"learn interactive online"},{"location":"tools/#boot-usb","text":"balena etcher","title":"boot usb"},{"location":"tools/#screen-recorder","text":"To create documentation for an ui or demonstrate bugs. Windows: Screen to gif Linux: Deepin screen recorder Peek","title":"screen recorder"},{"location":"tools/#terminal-recorder","text":"ascii cinema save terminal session in a cast file which can be played in a terminal or on the website. The cast file is a text file, but it can not be used to be directly copy pasted into a terminal. The asciidoc player is needed. usage: ## install sudo apt-get install asciinema ## record and save in file asciinema rec ./file.cast # exit via crlt + D ## replay a record asciinema play ./file.cast ## dumps the content of a record on console asciinema cat ./file.cast ## record and save to web ( you need account to view it) asciinema rec # exit via crlt + D # press enter","title":"terminal recorder"},{"location":"tools/#disk-usage-on-mac","text":"space-radar","title":"disk usage on mac"},{"location":"tracing/","text":"Tracing TODO see you obsidian notes (2025-01-07) with spring boot Configure it: spring.application.name: micrometer-tracing # url of collector # port 4318 is http but there are more management.otlp.tracing.endpoint: http://jaeger:4318/v1/traces management.tracing.sampling.probability: 1.0 Add the following dependecies <!-- Adds the Tracing API --> <dependency> <groupId>io.micrometer</groupId> <artifactId>micrometer-tracing</artifactId> </dependency> <!-- Adds the Tracer Implementation --> <dependency> <groupId>io.micrometer</groupId> <artifactId>micrometer-tracing-bridge-otel</artifactId> </dependency> <!-- Adds an exporter to store the traces --> <dependency> <groupId>io.opentelemetry</groupId> <artifactId>opentelemetry-exporter-otlp</artifactId> </dependency> <!-- Only neeeded for autconfiguration if feign is used as rest client. Otherwise traceid is not send --> <dependency> <groupId>io.github.openfeign</groupId> <artifactId>feign-micrometer</artifactId> <version>1.0.7</version> </dependency>","title":"Tracing"},{"location":"tracing/#tracing","text":"","title":"Tracing"},{"location":"tracing/#todo","text":"see you obsidian notes (2025-01-07)","title":"TODO"},{"location":"tracing/#with-spring-boot","text":"Configure it: spring.application.name: micrometer-tracing # url of collector # port 4318 is http but there are more management.otlp.tracing.endpoint: http://jaeger:4318/v1/traces management.tracing.sampling.probability: 1.0 Add the following dependecies <!-- Adds the Tracing API --> <dependency> <groupId>io.micrometer</groupId> <artifactId>micrometer-tracing</artifactId> </dependency> <!-- Adds the Tracer Implementation --> <dependency> <groupId>io.micrometer</groupId> <artifactId>micrometer-tracing-bridge-otel</artifactId> </dependency> <!-- Adds an exporter to store the traces --> <dependency> <groupId>io.opentelemetry</groupId> <artifactId>opentelemetry-exporter-otlp</artifactId> </dependency> <!-- Only neeeded for autconfiguration if feign is used as rest client. Otherwise traceid is not send --> <dependency> <groupId>io.github.openfeign</groupId> <artifactId>feign-micrometer</artifactId> <version>1.0.7</version> </dependency>","title":"with spring boot"},{"location":"vagrant/","text":"Vagrant concept Source Vagrant uses providers, provisioners, boxes, and Vagrantfiles as building blocks of the virtual machines. Provider: Services to set up and create virtual environments (e.g. VirtualBox) Provisioner: Tools to customize the configuration of virtual environments. Vagrant has built built-in providers for uploading files, syncing directories or executing shell commands Vagrantfile: Configuration file and file name (Vagrantfile) for virtual environments. Box: Format and an extension (*.box) for virtual environments. Boxes can be downloaded from the Vagrant Cloud and copied from one machine to another in order to to replicate an environment. Vagrant file + Boxes Initially created with cli and a box name e.g: vagrant init opensuse/Tumbleweed.x86_64 . You can find public available boxes on app.vagrantup.com . reaload provosion vagrant reload --provision # or vagrant destroy -f && vagrant up SSH into vagrant box without 'vagrant ssh' # save the config to a file vagrant ssh-config > vagrant-ssh # run ssh with the file. ssh -F vagrant-ssh default Source connect with ansible to vagrant box vagrant ssh-config > vagrant-ssh.cfg You can refence the ssh config in ansible by adding the following to a host_var file: ansible_ssh_common_args: '-F ./host_vars/VAGRANT/vagrant-ssh.cfg' # must match host in vagrant-ssh.cfg ansible_ssh_host: default autocompletion vagrant autocomplete install --bash ##--zsh vagrant box list increase ram and cpu Add the following to Vagrantfile # 4 gb, 2 cores config.vm.provider \"virtualbox\" do |v| v.memory = 4000 v.cpus = 2 end increase swap size Is controled by vm image. If you want to increase the size you ned to run a provision script by adding the following to Vagrantfile: config.vm.provision \"shell\", path: \"./increase_swap_space.sh\" The content of ./increase_swap_space.sh could look like this ( Source ): #!/bin/sh # size of swapfile in megabytes swapsize=512 # does the swap file already exist? grep -q \"swapfile\" /etc/fstab # if not then create it if [ $? -ne 0 ]; then echo 'swapfile not found. Adding swapfile.' fallocate -l ${swapsize}M /swapfile chmod 600 /swapfile mkswap /swapfile swapon /swapfile echo '/swapfile none swap defaults 0 0' >> /etc/fstab else echo 'swapfile found. No changes made.' fi # output results to terminal cat /proc/swaps cat /proc/meminfo | grep Swap forward port config.vm.network \"forwarded_port\", guest: 8080, host: 8080 Copy files config.vm.provision \"file\", source: \"~/.gitconfig\", destination: \".gitconfig\"","title":"Vagrant"},{"location":"vagrant/#vagrant","text":"","title":"Vagrant"},{"location":"vagrant/#concept","text":"Source Vagrant uses providers, provisioners, boxes, and Vagrantfiles as building blocks of the virtual machines. Provider: Services to set up and create virtual environments (e.g. VirtualBox) Provisioner: Tools to customize the configuration of virtual environments. Vagrant has built built-in providers for uploading files, syncing directories or executing shell commands Vagrantfile: Configuration file and file name (Vagrantfile) for virtual environments. Box: Format and an extension (*.box) for virtual environments. Boxes can be downloaded from the Vagrant Cloud and copied from one machine to another in order to to replicate an environment.","title":"concept"},{"location":"vagrant/#vagrant-file-boxes","text":"Initially created with cli and a box name e.g: vagrant init opensuse/Tumbleweed.x86_64 . You can find public available boxes on app.vagrantup.com .","title":"Vagrant file + Boxes"},{"location":"vagrant/#reaload-provosion","text":"vagrant reload --provision # or vagrant destroy -f && vagrant up","title":"reaload provosion"},{"location":"vagrant/#ssh-into-vagrant-box-without-vagrant-ssh","text":"# save the config to a file vagrant ssh-config > vagrant-ssh # run ssh with the file. ssh -F vagrant-ssh default Source","title":"SSH into vagrant box without 'vagrant ssh'"},{"location":"vagrant/#connect-with-ansible-to-vagrant-box","text":"vagrant ssh-config > vagrant-ssh.cfg You can refence the ssh config in ansible by adding the following to a host_var file: ansible_ssh_common_args: '-F ./host_vars/VAGRANT/vagrant-ssh.cfg' # must match host in vagrant-ssh.cfg ansible_ssh_host: default","title":"connect with ansible to vagrant box"},{"location":"vagrant/#autocompletion","text":"vagrant autocomplete install --bash ##--zsh vagrant box list","title":"autocompletion"},{"location":"vagrant/#increase-ram-and-cpu","text":"Add the following to Vagrantfile # 4 gb, 2 cores config.vm.provider \"virtualbox\" do |v| v.memory = 4000 v.cpus = 2 end","title":"increase ram and cpu"},{"location":"vagrant/#increase-swap-size","text":"Is controled by vm image. If you want to increase the size you ned to run a provision script by adding the following to Vagrantfile: config.vm.provision \"shell\", path: \"./increase_swap_space.sh\" The content of ./increase_swap_space.sh could look like this ( Source ): #!/bin/sh # size of swapfile in megabytes swapsize=512 # does the swap file already exist? grep -q \"swapfile\" /etc/fstab # if not then create it if [ $? -ne 0 ]; then echo 'swapfile not found. Adding swapfile.' fallocate -l ${swapsize}M /swapfile chmod 600 /swapfile mkswap /swapfile swapon /swapfile echo '/swapfile none swap defaults 0 0' >> /etc/fstab else echo 'swapfile found. No changes made.' fi # output results to terminal cat /proc/swaps cat /proc/meminfo | grep Swap","title":"increase swap size"},{"location":"vagrant/#forward-port","text":"config.vm.network \"forwarded_port\", guest: 8080, host: 8080","title":"forward port"},{"location":"vagrant/#copy-files","text":"config.vm.provision \"file\", source: \"~/.gitconfig\", destination: \".gitconfig\"","title":"Copy files"},{"location":"vscode/","text":"VS Code Debugging Vscode expetcs a launch.json and a task.json file in .vscode directory in the root your selected vscode workspace. The preLaunchTask field in the launch.json refers to a label of a task in task.json. For debugging with firefox you need an additional vscode extension . There is an official microsoft recipes for debugging repo on github. Remote SSH and SCP Official documentation Install 'Remote - SSH' Extension Open config via the gear symbol which appears next to the 'ssh targets' batch when your mouse hovers over it over Edit this configureation text file Connect to it. First time vscode might prompt two times for your password. (Once for the ssh login and once for the start of the vs code server) Use vscode as usual. Open Folder and the integrated terminal will connect to the remote server not your local one. One for each connection: Host alias HostName hostname User user IdentityFile privateKey trouble shooting: you can restart the vs code server on the remote server with prompt > Remote-SSH: Kill VS Code SErver on Host... (CTRL + SHIFT+P). This might be necessary if you add a user to a new group. useful extensions Spellchecker for txt and md: TODO maybe LanguageTool Linter Linters: md: markdownlint Bash script Remote Connect SSH Container WSL Rainbow CSV - highlight and queries change-case - change between different cases (camel, snake etc.) Render Linebreaks","title":"VS Code"},{"location":"vscode/#vs-code","text":"","title":"VS Code"},{"location":"vscode/#debugging","text":"Vscode expetcs a launch.json and a task.json file in .vscode directory in the root your selected vscode workspace. The preLaunchTask field in the launch.json refers to a label of a task in task.json. For debugging with firefox you need an additional vscode extension . There is an official microsoft recipes for debugging repo on github.","title":"Debugging"},{"location":"vscode/#remote-ssh-and-scp","text":"Official documentation Install 'Remote - SSH' Extension Open config via the gear symbol which appears next to the 'ssh targets' batch when your mouse hovers over it over Edit this configureation text file Connect to it. First time vscode might prompt two times for your password. (Once for the ssh login and once for the start of the vs code server) Use vscode as usual. Open Folder and the integrated terminal will connect to the remote server not your local one. One for each connection: Host alias HostName hostname User user IdentityFile privateKey trouble shooting: you can restart the vs code server on the remote server with prompt > Remote-SSH: Kill VS Code SErver on Host... (CTRL + SHIFT+P). This might be necessary if you add a user to a new group.","title":"Remote SSH and SCP"},{"location":"vscode/#useful-extensions","text":"Spellchecker for txt and md: TODO maybe LanguageTool Linter Linters: md: markdownlint Bash script Remote Connect SSH Container WSL Rainbow CSV - highlight and queries change-case - change between different cases (camel, snake etc.) Render Linebreaks","title":"useful extensions"},{"location":"yaml-overview/","text":"yaml overview Nice examples Official ref card # for comments indentation to indicate nesting (2 spaces) list [] , items comma separated or hyphen (-) before list members associative array {} , items comma separated or key: value colon-centered syntax list and associative array can contain nested list and associative arrays \"---\" document separator \"...\" end document (optional)","title":"yaml overview"},{"location":"yaml-overview/#yaml-overview","text":"Nice examples Official ref card # for comments indentation to indicate nesting (2 spaces) list [] , items comma separated or hyphen (-) before list members associative array {} , items comma separated or key: value colon-centered syntax list and associative array can contain nested list and associative arrays \"---\" document separator \"...\" end document (optional)","title":"yaml overview"},{"location":"yubikey/","text":"Yubikey 5 series Guide Simple use cases setup Offical setup page # cli brew install ykman # ui ## see https://www.yubico.com/support/download/yubikey-manager/ usages save gpg key for signing git commits save ssh key for ssh login and/or login to git servers create gpg key on yubi key Source accessing gpg key on yubi key for siging commit create ssh key on yubi key accessing ssh key on yubikey","title":"Yubikey 5 series"},{"location":"yubikey/#yubikey-5-series","text":"Guide Simple use cases","title":"Yubikey 5 series"},{"location":"yubikey/#setup","text":"Offical setup page # cli brew install ykman # ui ## see https://www.yubico.com/support/download/yubikey-manager/","title":"setup"},{"location":"yubikey/#usages","text":"save gpg key for signing git commits save ssh key for ssh login and/or login to git servers","title":"usages"},{"location":"yubikey/#create-gpg-key-on-yubi-key","text":"Source","title":"create gpg key on yubi key"},{"location":"yubikey/#accessing-gpg-key-on-yubi-key-for-siging-commit","text":"","title":"accessing gpg key on yubi key for siging commit"},{"location":"yubikey/#create-ssh-key-on-yubi-key","text":"","title":"create ssh key on yubi key"},{"location":"yubikey/#accessing-ssh-key-on-yubikey","text":"","title":"accessing ssh key on yubikey"},{"location":"docker/01_docker-memory-aid/","text":"Docker CLI useful command vs code plugin for docker is pretty nice. official overview of docker cli commands . docker build . -t crowdsalat/imageName:tag creates a image based on a Dockerfile and the context/ working directory and subdirectories docker run <image> starts a container from an image -d run as daemon -it runs interactively so you can execute commands in container (-t Allocate a pseudo-tty, -i Keep STDIN open ) docker run -it --entrypoint sh crowdsalat/imageName:tag starts a container and starts a interactive shell. -p 500:1000 maps the port 1000 of the container to the port 500 of the host. Reachable under localhost on the host system. -p 192.168.178.123:1000:500 maps the port 1000 of the container to the port of the host. Reachable under localhost on the host system. Reachable under the IP address of the host system. -v /var/logs/ binds /var/logs inside the container to a unnamed volume on the mount which resides in /var/lib/docker/volumes/ -v /var/run/docker.sock:/var/run/docker.sock containers started inside of this container will be started on the host a not inside the container (sibling not a child). Useful in CI/CD pipelines which uses containers as runners see: https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/ -e VAR1=bla -e VAR2=blubb defines environmental variables in the container \u2014rm remove container after it is stopped. \u2014name add name to the container. docker history --no-trunc <image> show something similar to the original dockerfile docker exec -it container_name bash connects to a running container with the name container_name docker ps \u2212a shows all running docker container on a client docker image / container / network / volumes / system ls rm inspect docker system prune removes: all stopped containers ( docker container prune ) all networks not used by at least one container ( docker network prune ) all dangling images ( docker image prune ) all dangling build cache docker network create --driver bridge <bridgeName> create a new bridge network docker volume create <volumeName> create a new bridge volume docker cp <containerName>:<pathInConainer> <pathOnHost> copies data from the container to the host. Helpful if volume were not set up correct If you pull a image with the latest tag it will first try to download it from docker hub and only if it is not there it will use a local variant.","title":"Docker CLI useful command"},{"location":"docker/01_docker-memory-aid/#docker-cli-useful-command","text":"vs code plugin for docker is pretty nice. official overview of docker cli commands . docker build . -t crowdsalat/imageName:tag creates a image based on a Dockerfile and the context/ working directory and subdirectories docker run <image> starts a container from an image -d run as daemon -it runs interactively so you can execute commands in container (-t Allocate a pseudo-tty, -i Keep STDIN open ) docker run -it --entrypoint sh crowdsalat/imageName:tag starts a container and starts a interactive shell. -p 500:1000 maps the port 1000 of the container to the port 500 of the host. Reachable under localhost on the host system. -p 192.168.178.123:1000:500 maps the port 1000 of the container to the port of the host. Reachable under localhost on the host system. Reachable under the IP address of the host system. -v /var/logs/ binds /var/logs inside the container to a unnamed volume on the mount which resides in /var/lib/docker/volumes/ -v /var/run/docker.sock:/var/run/docker.sock containers started inside of this container will be started on the host a not inside the container (sibling not a child). Useful in CI/CD pipelines which uses containers as runners see: https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/ -e VAR1=bla -e VAR2=blubb defines environmental variables in the container \u2014rm remove container after it is stopped. \u2014name add name to the container. docker history --no-trunc <image> show something similar to the original dockerfile docker exec -it container_name bash connects to a running container with the name container_name docker ps \u2212a shows all running docker container on a client docker image / container / network / volumes / system ls rm inspect docker system prune removes: all stopped containers ( docker container prune ) all networks not used by at least one container ( docker network prune ) all dangling images ( docker image prune ) all dangling build cache docker network create --driver bridge <bridgeName> create a new bridge network docker volume create <volumeName> create a new bridge volume docker cp <containerName>:<pathInConainer> <pathOnHost> copies data from the container to the host. Helpful if volume were not set up correct If you pull a image with the latest tag it will first try to download it from docker hub and only if it is not there it will use a local variant.","title":"Docker CLI useful command"},{"location":"docker/02_dockerfile/","text":"Dockerfile Docker file reference Nice summary of run, cmd and entypoint Create image form dockerfile docker build <context> builds Docker images from a Dockerfile and a 'context'. context is the set of files located in the specified PATH or URL the context is processed recursively docker build . is used to build a image based on a dockerfile in the current directory. use -t <tageName> to add a tag to the image Simplistic docker file for java application: FROM openjdk:8-jdk-alpine COPY target/*.jar app.jar ENTRYPOINT[\"java\", \"-jar\", \"app.jar\"] shell form vs. exec form the forms can be distinguished by their syntax. they differ in their behavior. some docker commands like RUN, ENTRYPOINT or CMD can be used in either form. shell form: syntax: <instruction> <command> like calling /bin/sh -c <command> variable substitution works e.g. exec echo $path application which is started this way will not receiving interrupt signals (like crlt + c) exec form: syntax: <instruction> [\"executable\", \"arg1\", \"arg2\", ...] like calling: executable directly, so it must be a valid path (/bin/echo instead of echo) preferred for CMD and ENTRYPOINT no variable replacement ($PATH does not work) dockerfile commands RUN, CMD and Entrypoint all execute a command, but: RUN creates a new layer on the union file system CMD sets default command to run when container starts. This command can be replaced by another command by calling: docker run -it <image> <command which is used instead> ENTRYPOINT sets default command to run when container starts, but cannot be overwritten (exec and shell form). You can add a CMD without a command/executable afterwards to add further arguments to the entrypoint command which can be overwritten (only in exec form). --> Use RUN for new layer and prefer ENTRYPOINT over CMD if you do not need to change arguments. COPY and ADD copy files to an image and create a new layer, but: COPY can only be used on local files ADD can download any URL and extract tar balls --> use COPY for local files ENV sets a environment variable ( ENV path=/opt/ ) at docker build and during runtime. ARG defines parameters of a docker image which can be overridden when the images is build (e.g. $ docker build --build-arg arg_name=blubb . ). only available during build time. useful images A list of relatively small images. If present alpine images tend to be the smallest. Docker repositories mostly are organized via the tags (:8-jdk-alpine, :11-jdk-buster etc.). Java: openjdk:8-jdk-alpine openjdk:11-jdk-buster openjdk:14-jdk-slim adoptopenjdk/openjdk8-openj9:alpine adoptopenjdk/openjdk11:alpine dockerfile vs. command line -p vs EXPOSE EXPORT does not actually publish the given port. It is a documentation for the user. The user of a image needs to export the port via -p flag when running docker run. --volume vs. VOLUME VOLUME creates a anonymous volume even when docker run does not specify a --volume parameter.","title":"Dockerfile"},{"location":"docker/02_dockerfile/#dockerfile","text":"Docker file reference Nice summary of run, cmd and entypoint","title":"Dockerfile"},{"location":"docker/02_dockerfile/#create-image-form-dockerfile","text":"docker build <context> builds Docker images from a Dockerfile and a 'context'. context is the set of files located in the specified PATH or URL the context is processed recursively docker build . is used to build a image based on a dockerfile in the current directory. use -t <tageName> to add a tag to the image Simplistic docker file for java application: FROM openjdk:8-jdk-alpine COPY target/*.jar app.jar ENTRYPOINT[\"java\", \"-jar\", \"app.jar\"]","title":"Create image form dockerfile"},{"location":"docker/02_dockerfile/#shell-form-vs-exec-form","text":"the forms can be distinguished by their syntax. they differ in their behavior. some docker commands like RUN, ENTRYPOINT or CMD can be used in either form. shell form: syntax: <instruction> <command> like calling /bin/sh -c <command> variable substitution works e.g. exec echo $path application which is started this way will not receiving interrupt signals (like crlt + c) exec form: syntax: <instruction> [\"executable\", \"arg1\", \"arg2\", ...] like calling: executable directly, so it must be a valid path (/bin/echo instead of echo) preferred for CMD and ENTRYPOINT no variable replacement ($PATH does not work)","title":"shell form vs. exec form"},{"location":"docker/02_dockerfile/#dockerfile-commands","text":"RUN, CMD and Entrypoint all execute a command, but: RUN creates a new layer on the union file system CMD sets default command to run when container starts. This command can be replaced by another command by calling: docker run -it <image> <command which is used instead> ENTRYPOINT sets default command to run when container starts, but cannot be overwritten (exec and shell form). You can add a CMD without a command/executable afterwards to add further arguments to the entrypoint command which can be overwritten (only in exec form). --> Use RUN for new layer and prefer ENTRYPOINT over CMD if you do not need to change arguments. COPY and ADD copy files to an image and create a new layer, but: COPY can only be used on local files ADD can download any URL and extract tar balls --> use COPY for local files ENV sets a environment variable ( ENV path=/opt/ ) at docker build and during runtime. ARG defines parameters of a docker image which can be overridden when the images is build (e.g. $ docker build --build-arg arg_name=blubb . ). only available during build time.","title":"dockerfile commands"},{"location":"docker/02_dockerfile/#useful-images","text":"A list of relatively small images. If present alpine images tend to be the smallest. Docker repositories mostly are organized via the tags (:8-jdk-alpine, :11-jdk-buster etc.). Java: openjdk:8-jdk-alpine openjdk:11-jdk-buster openjdk:14-jdk-slim adoptopenjdk/openjdk8-openj9:alpine adoptopenjdk/openjdk11:alpine","title":"useful images"},{"location":"docker/02_dockerfile/#dockerfile-vs-command-line","text":"","title":"dockerfile vs. command line"},{"location":"docker/02_dockerfile/#-p-vs-expose","text":"EXPORT does not actually publish the given port. It is a documentation for the user. The user of a image needs to export the port via -p flag when running docker run.","title":"-p vs EXPOSE"},{"location":"docker/02_dockerfile/#-volume-vs-volume","text":"VOLUME creates a anonymous volume even when docker run does not specify a --volume parameter.","title":"--volume vs. VOLUME"},{"location":"docker/03_docker-persisting-data/","text":"Docker persisting data Docker does not persist data when a container is shutdown. In order to do so you need to define volumes or bind mounts. volumes official documentation preferred way to persist data Are managed on host under: /var/lib/docker/volumes/ can be named or anonymous create a volume: docker volume create <volumeName> -v or --volumes parameter consists of three fields, separated by colon characters (:). first field is the name of the volume path where the file or directory are mounted in the container. optional comma-separated list of options pitfall: the folder in the container will be overridden with the one from the host if it exists. When the folder on the host does not exists it will be created and the content of the folder on the docker container will be copied to the host. # create a named volume docker volume create myvolume # use a named volume on hello-world image docker run -v myvolume:/usr/share/importantPath hello-world # create a anonymous volume on hello-world image docker run -v /usr/share/importantPath hello-world Difference between defining VOLUME in a Dockerfile and adding a volume when starting a container: VOLUME creates a anonymous volume even when docker run does not specify a -v parameter stackoverflow docker forum","title":"Docker persisting data"},{"location":"docker/03_docker-persisting-data/#docker-persisting-data","text":"Docker does not persist data when a container is shutdown. In order to do so you need to define volumes or bind mounts.","title":"Docker persisting data"},{"location":"docker/03_docker-persisting-data/#volumes","text":"official documentation preferred way to persist data Are managed on host under: /var/lib/docker/volumes/ can be named or anonymous create a volume: docker volume create <volumeName> -v or --volumes parameter consists of three fields, separated by colon characters (:). first field is the name of the volume path where the file or directory are mounted in the container. optional comma-separated list of options pitfall: the folder in the container will be overridden with the one from the host if it exists. When the folder on the host does not exists it will be created and the content of the folder on the docker container will be copied to the host. # create a named volume docker volume create myvolume # use a named volume on hello-world image docker run -v myvolume:/usr/share/importantPath hello-world # create a anonymous volume on hello-world image docker run -v /usr/share/importantPath hello-world Difference between defining VOLUME in a Dockerfile and adding a volume when starting a container: VOLUME creates a anonymous volume even when docker run does not specify a -v parameter stackoverflow docker forum","title":"volumes"},{"location":"docker/04_docker-networking/","text":"Docker networking access container from your hosts ip address If you want to access a docker container by its host ip address you either add its ip address before the port mapping when starting the container or you use the host network type. If you want to use ipv6 you may need to set some further configurations. # maps port 80 of the container to localhost:8080 of host docker run -p 8080:80 # maps port 80 of the container to 192.168.178.123:8080 of host docker run -p 192.168.178.123:8080:80 network types Copied from network driver summary : User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host. Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated. Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services. Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address. Third-party network plugins allow you to integrate Docker with specialized network stacks. user-defined bridge official doc A user-defined bridge allow that two containers in this network can access each other via their container name (dns resolution). Container without a explicit network are in the default network bridge. The default bridge have no dns server so the containers need to address each other via their ip address. You can get the ip address of a container via docker inspect <containerName> . # create a new bridge network docker network create --driver bridge <bridgeName> # list networks docker network ls # start container a contgainer in the network docker run --network=<bridgeName> <imageName> # add a running container to a network docker network connect <bridgeName> <runningContainerName>","title":"Docker networking"},{"location":"docker/04_docker-networking/#docker-networking","text":"","title":"Docker networking"},{"location":"docker/04_docker-networking/#access-container-from-your-hosts-ip-address","text":"If you want to access a docker container by its host ip address you either add its ip address before the port mapping when starting the container or you use the host network type. If you want to use ipv6 you may need to set some further configurations. # maps port 80 of the container to localhost:8080 of host docker run -p 8080:80 # maps port 80 of the container to 192.168.178.123:8080 of host docker run -p 192.168.178.123:8080:80","title":"access container from your hosts ip address"},{"location":"docker/04_docker-networking/#network-types","text":"Copied from network driver summary : User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host. Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated. Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services. Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address. Third-party network plugins allow you to integrate Docker with specialized network stacks.","title":"network types"},{"location":"docker/04_docker-networking/#user-defined-bridge","text":"official doc A user-defined bridge allow that two containers in this network can access each other via their container name (dns resolution). Container without a explicit network are in the default network bridge. The default bridge have no dns server so the containers need to address each other via their ip address. You can get the ip address of a container via docker inspect <containerName> . # create a new bridge network docker network create --driver bridge <bridgeName> # list networks docker network ls # start container a contgainer in the network docker run --network=<bridgeName> <imageName> # add a running container to a network docker network connect <bridgeName> <runningContainerName>","title":"user-defined bridge"},{"location":"docker/docker-build/","text":"Docker build multi-platform on macos m1 with colima brew install docker-buildx # Follow the caveats mentioned in the install instructions: mkdir -p ~/.docker/cli-plugins ln -sfn $(which docker-buildx) ~/.docker/cli-plugins/docker-buildx # set build env docker buildx create --use colima # test docker buildx install mutli arch local on m2 docker login docker-buildx build --platform linux/amd64,linux/arm64 --push -t crowdsalat/sample:0.0.1 -t crowdsalat/sample:latest .","title":"Docker build"},{"location":"docker/docker-build/#docker-build","text":"","title":"Docker build"},{"location":"docker/docker-build/#multi-platform-on-macos-m1-with-colima","text":"brew install docker-buildx # Follow the caveats mentioned in the install instructions: mkdir -p ~/.docker/cli-plugins ln -sfn $(which docker-buildx) ~/.docker/cli-plugins/docker-buildx # set build env docker buildx create --use colima # test docker buildx install","title":"multi-platform on macos m1 with colima"},{"location":"docker/docker-build/#mutli-arch-local-on-m2","text":"docker login docker-buildx build --platform linux/amd64,linux/arm64 --push -t crowdsalat/sample:0.0.1 -t crowdsalat/sample:latest .","title":"mutli arch local on m2"},{"location":"docker/docker-compose/","text":"Docker compose is useful: when you need to start multiple containers which work together it automatically creates shared network parameters are saved in a file instead of multiple positional arguments on multiple docker run commands when your software runs on a single host and you do not want to bother with kubernetes install Offcial sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose usage cli # start container in backround docker-compose up -d # remove containers, network and (unnamed) volumes docker-compose down # remove containers, networks and volumes docker-compose down -v # show logs of services docker-compose logs -f # if a new image for a service is avaiable update it docker-compose up -d --force-recreate # or with docker-compose pull docker-compose restart usage docker-compose.yml Docker compose file reference image reference a docker image for the service build build a docker image for the service (usage instead of image) command Override the default command of the docker image of the service container_name set a specific container name (without one would be generated <imageName>-<serviceName> ) restart defaults to no . Other options are: always, on-failure, unless-stopped environment: to define a list of environment variables. Boolean values need to be quoted so the yaml parser does not interfere volumes can be defined on service level or shared for every container on root level of the yaml see .There is a long form where you can configure more things and a short form which works similar to the usual -v flag in docker run: [SOURCE:]TARGET[:MODE] . noteworthy By default Docker compose searches for a ./docker-compose.yml file - By default Compose sets up a single network for your app, so that the containers can communicate with each other by their container name. - Valid top-level sections for a Compose file are: version, services, networks, volumes, and extensions starting with \"x-\"","title":"Docker compose"},{"location":"docker/docker-compose/#docker-compose","text":"is useful: when you need to start multiple containers which work together it automatically creates shared network parameters are saved in a file instead of multiple positional arguments on multiple docker run commands when your software runs on a single host and you do not want to bother with kubernetes","title":"Docker compose"},{"location":"docker/docker-compose/#install","text":"Offcial sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.5/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose","title":"install"},{"location":"docker/docker-compose/#usage-cli","text":"# start container in backround docker-compose up -d # remove containers, network and (unnamed) volumes docker-compose down # remove containers, networks and volumes docker-compose down -v # show logs of services docker-compose logs -f # if a new image for a service is avaiable update it docker-compose up -d --force-recreate # or with docker-compose pull docker-compose restart","title":"usage cli"},{"location":"docker/docker-compose/#usage-docker-composeyml","text":"Docker compose file reference image reference a docker image for the service build build a docker image for the service (usage instead of image) command Override the default command of the docker image of the service container_name set a specific container name (without one would be generated <imageName>-<serviceName> ) restart defaults to no . Other options are: always, on-failure, unless-stopped environment: to define a list of environment variables. Boolean values need to be quoted so the yaml parser does not interfere volumes can be defined on service level or shared for every container on root level of the yaml see .There is a long form where you can configure more things and a short form which works similar to the usual -v flag in docker run: [SOURCE:]TARGET[:MODE] .","title":"usage docker-compose.yml"},{"location":"docker/docker-compose/#noteworthy","text":"By default Docker compose searches for a ./docker-compose.yml file - By default Compose sets up a single network for your app, so that the containers can communicate with each other by their container name. - Valid top-level sections for a Compose file are: version, services, networks, volumes, and extensions starting with \"x-\"","title":"noteworthy"},{"location":"docker/docker-database-examples/","text":"Examples for containerized databases MySQL Dockerhub mysql docker run -d --name mysql -e MYSQL_ROOT_PASSWORD=myql -e MYSQL_DATABASE=my-database -v mysql:/var/lib/mysql -p 3306:3306 mysql:latest Access database from within the container via: mysql -uroot -pmyql my-database . Getting Information About Databases and Tables . admin docker run -d --name sql-adminer -p 8080:8080 adminer:latest Log in with: - Server= Internal Ip of Docker Container/ container name if they are on the same user defined network - user=root - password=myql - Database: my-database PostgreSQL Postgre docker image Dockerhub Nice tutorial # create a user-defined bridge network so containers in it can address themself by the container name not only by ip address docker network create --driver bridge pgnetwork # create volume so db will be saved docker volume create --driver local --name=pgvolume # create docker container for postgres docker run -d --rm \\ --name pg-container \\ -p 5432:5432 \\ -e POSTGRES_USER=postgres \\ -e POSTGRES_PASSWORD=postgres \\ --network=pgnetwork \\ --volume=pgvolume:/var/lib/postgresql/data \\ postgres:latest # standard db name is postgres. you can set it via: # -e POSTGRES_DB=postgres NOTE : if you restart this container with new passwords you may need to clear the volume. Inside of the container you can use the psql to interact with the database. pgAdmin docker image Documentation for pgAdmin image docker run -d --rm \\ --name pgAdmin-container \\ -p 9080:80 \\ -e PGADMIN_DEFAULT_EMAIL=example@mail.com \\ -e PGADMIN_DEFAULT_PASSWORD=limoneneis \\ --network=pgnetwork \\ dpage/pgadmin4:4.20 After starting the container Open http://localhost:9080/login in browser login with your PGADMIN_DEFAULT_EMAIL and your PGADMIN_DEFAULT_PASSWORD add server with host name address: pg-container username: postgres password: postgres known issue An process named oka uses 100 of the cpu see When an application tries to connect to postgres for the second time the authentification fails because of an invalid password.","title":"Examples for containerized databases"},{"location":"docker/docker-database-examples/#examples-for-containerized-databases","text":"","title":"Examples for containerized databases"},{"location":"docker/docker-database-examples/#mysql","text":"Dockerhub","title":"MySQL"},{"location":"docker/docker-database-examples/#mysql_1","text":"docker run -d --name mysql -e MYSQL_ROOT_PASSWORD=myql -e MYSQL_DATABASE=my-database -v mysql:/var/lib/mysql -p 3306:3306 mysql:latest Access database from within the container via: mysql -uroot -pmyql my-database . Getting Information About Databases and Tables .","title":"mysql"},{"location":"docker/docker-database-examples/#admin","text":"docker run -d --name sql-adminer -p 8080:8080 adminer:latest Log in with: - Server= Internal Ip of Docker Container/ container name if they are on the same user defined network - user=root - password=myql - Database: my-database","title":"admin"},{"location":"docker/docker-database-examples/#postgresql","text":"","title":"PostgreSQL"},{"location":"docker/docker-database-examples/#postgre-docker-image","text":"Dockerhub Nice tutorial # create a user-defined bridge network so containers in it can address themself by the container name not only by ip address docker network create --driver bridge pgnetwork # create volume so db will be saved docker volume create --driver local --name=pgvolume # create docker container for postgres docker run -d --rm \\ --name pg-container \\ -p 5432:5432 \\ -e POSTGRES_USER=postgres \\ -e POSTGRES_PASSWORD=postgres \\ --network=pgnetwork \\ --volume=pgvolume:/var/lib/postgresql/data \\ postgres:latest # standard db name is postgres. you can set it via: # -e POSTGRES_DB=postgres NOTE : if you restart this container with new passwords you may need to clear the volume. Inside of the container you can use the psql to interact with the database.","title":"Postgre docker image"},{"location":"docker/docker-database-examples/#pgadmin-docker-image","text":"Documentation for pgAdmin image docker run -d --rm \\ --name pgAdmin-container \\ -p 9080:80 \\ -e PGADMIN_DEFAULT_EMAIL=example@mail.com \\ -e PGADMIN_DEFAULT_PASSWORD=limoneneis \\ --network=pgnetwork \\ dpage/pgadmin4:4.20 After starting the container Open http://localhost:9080/login in browser login with your PGADMIN_DEFAULT_EMAIL and your PGADMIN_DEFAULT_PASSWORD add server with host name address: pg-container username: postgres password: postgres","title":"pgAdmin docker image"},{"location":"docker/docker-database-examples/#known-issue","text":"An process named oka uses 100 of the cpu see When an application tries to connect to postgres for the second time the authentification fails because of an invalid password.","title":"known issue"},{"location":"docker/docker_headless_wsl/","text":"Docker in WSL2 without docker desktop motivation and overview Use docker headless instead of podman to be able to use k3d. Steps: WSL has no systemd -> Solution: Start daemon in .bashrc WSL uses newer version of ipttables which is not supported by docker to the given time -> Solution: use update-alternatives to set legacy version My system: - WSL2 - Distribution: Debian GNU/Linux 11 (bullseye) - Kernel: 4.19.104-microsoft-standard prerequisites: deinstall docker desktop restart wsl: wsl.exe --shutdown installation # INSTALL DOCKER ON DEBIAN ## source: https://docs.docker.com/engine/install/debian/ ## install packages to allow apt to use a repository over HTTP sudo apt update sudo apt install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release -y ## add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg ## set up the stable docker repository echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null ## install the latest version of Docker Engine and containerd sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io -y ## add ur user to docker group (so no sudo needed) sudo usermod -a -G docker $USER # TEST DOCKER docker run hello-world ## if it does not work check the error message of the docker daemon sudo dockerd ## ctrl + c ## if the reason has something to do with iptables this might be the solution ## https://patrickwu.space/2021/03/09/wsl-solution-to-native-docker-daemon-not-starting/ sudo update-alternatives --config iptables # choose legay: /usr/sbin/iptables-legacy ## restart wsl. In Powershell run: # wsl.exe --shutdown # START DOCKER AT STARTUP OF WSL (THERE IS NO SYSTEMD) ## source: https://blog.nillsf.com/index.php/2020/06/29/how-to-automatically-start-the-docker-daemon-on-wsl2/ ## start dockerd without being prompted for a password every time you start a terminal sudo cp /etc/sudoers /etc/sudoers.backup echo \"$USER ALL=(ALL) NOPASSWD: /usr/bin/dockerd\" | sudo tee --append /etc/sudoers sudo cat /etc/sudoers ## automatically run docker daemon echo \"\" >> ~/.bashrc echo '# Start Docker daemon automatically when logging in if not running.' >> ~/.bashrc echo 'RUNNING=`ps aux | grep dockerd | grep -v grep`' >> ~/.bashrc echo 'if [ -z \"$RUNNING\" ]; then' >> ~/.bashrc echo ' sudo dockerd > /dev/null 2>&1 &' >> ~/.bashrc echo ' disown' >> ~/.bashrc echo 'fi' >> ~/.bashrc # restart wsl wsl.exe --shutdown","title":"Docker in WSL2 without docker desktop"},{"location":"docker/docker_headless_wsl/#docker-in-wsl2-without-docker-desktop","text":"","title":"Docker in WSL2 without docker desktop"},{"location":"docker/docker_headless_wsl/#motivation-and-overview","text":"Use docker headless instead of podman to be able to use k3d. Steps: WSL has no systemd -> Solution: Start daemon in .bashrc WSL uses newer version of ipttables which is not supported by docker to the given time -> Solution: use update-alternatives to set legacy version My system: - WSL2 - Distribution: Debian GNU/Linux 11 (bullseye) - Kernel: 4.19.104-microsoft-standard","title":"motivation and overview"},{"location":"docker/docker_headless_wsl/#prerequisites","text":"deinstall docker desktop restart wsl: wsl.exe --shutdown","title":"prerequisites:"},{"location":"docker/docker_headless_wsl/#installation","text":"# INSTALL DOCKER ON DEBIAN ## source: https://docs.docker.com/engine/install/debian/ ## install packages to allow apt to use a repository over HTTP sudo apt update sudo apt install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release -y ## add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg ## set up the stable docker repository echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null ## install the latest version of Docker Engine and containerd sudo apt update sudo apt install docker-ce docker-ce-cli containerd.io -y ## add ur user to docker group (so no sudo needed) sudo usermod -a -G docker $USER # TEST DOCKER docker run hello-world ## if it does not work check the error message of the docker daemon sudo dockerd ## ctrl + c ## if the reason has something to do with iptables this might be the solution ## https://patrickwu.space/2021/03/09/wsl-solution-to-native-docker-daemon-not-starting/ sudo update-alternatives --config iptables # choose legay: /usr/sbin/iptables-legacy ## restart wsl. In Powershell run: # wsl.exe --shutdown # START DOCKER AT STARTUP OF WSL (THERE IS NO SYSTEMD) ## source: https://blog.nillsf.com/index.php/2020/06/29/how-to-automatically-start-the-docker-daemon-on-wsl2/ ## start dockerd without being prompted for a password every time you start a terminal sudo cp /etc/sudoers /etc/sudoers.backup echo \"$USER ALL=(ALL) NOPASSWD: /usr/bin/dockerd\" | sudo tee --append /etc/sudoers sudo cat /etc/sudoers ## automatically run docker daemon echo \"\" >> ~/.bashrc echo '# Start Docker daemon automatically when logging in if not running.' >> ~/.bashrc echo 'RUNNING=`ps aux | grep dockerd | grep -v grep`' >> ~/.bashrc echo 'if [ -z \"$RUNNING\" ]; then' >> ~/.bashrc echo ' sudo dockerd > /dev/null 2>&1 &' >> ~/.bashrc echo ' disown' >> ~/.bashrc echo 'fi' >> ~/.bashrc # restart wsl wsl.exe --shutdown","title":"installation"},{"location":"docker/registry/","text":"Registry trust cert of registry without trusting it Place the ca certificate inside /etc/docker/certs.d/ /ca.crt. Inlcude the port number in the path if you specify it when pulling. E.g: /etc/docker/certs.d/my-registry.example.com:5000/ca.crt Get the cert in browser or with: openssl s_client -showcerts -connect myregistry.de:443 ignore cert of registry Edit /etc/docker/daemon.json and add insecure-registries. { \"insecure-registries\" : [\"myregistry.de:443\", \"myregistry.de\"] } Restart docker: sudo systemctl restart docker","title":"Registry"},{"location":"docker/registry/#registry","text":"","title":"Registry"},{"location":"docker/registry/#trust-cert-of-registry-without-trusting-it","text":"Place the ca certificate inside /etc/docker/certs.d/ /ca.crt. Inlcude the port number in the path if you specify it when pulling. E.g: /etc/docker/certs.d/my-registry.example.com:5000/ca.crt Get the cert in browser or with: openssl s_client -showcerts -connect myregistry.de:443","title":"trust cert of registry without trusting it"},{"location":"docker/registry/#ignore-cert-of-registry","text":"Edit /etc/docker/daemon.json and add insecure-registries. { \"insecure-registries\" : [\"myregistry.de:443\", \"myregistry.de\"] } Restart docker: sudo systemctl restart docker","title":"ignore cert of registry"},{"location":"kafka/security/","text":"Kafka security on-prem create trust- and keystores #!/bin/bash validity=\"354\" keystore-pass=\"testtest\" key-pass=\"test\" distinguished-name=\"CN=myhost.de,OU=,DC=de\" hostname=\"myhost\" ca-passwort=\"testtest\" # create keystore for a broker keytool -keystore kafka.server.keystore.jks -alias localhost -keyalg RSA -validity {validity} -genkey -storepass {keystore-pass} -keypass {key-pass} -dname {distinguished-name} -ext SAN=DNS:{hostname} # private ca key openssl req -new -x509 -keyout ca-key -out ca-cert -days {validity} # create a client truststore and add ca cert to it keytool -keystore kafka.client.truststore.jks -alias CARoot -import -file ca-cert # create a server truststore and add a ca cert to it keytool -keystore kafka.server.truststore.jks -alias CARoot -importcert -file ca-cert # export the certificate from the boker keystore keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-file # sign the exportet certificate with the ca key openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password} # import the signed certificate and the ca cert to the broker keystore keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert keytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed encyption (TLS) and authentification (via mTLS) You can encrypt the traffic between brokers and the traffic between brokers and clients with TLS. If you want to use ACLs on topics you also can configure authentification via mTLS. Every participant needs to have a public/private keypair (keystore) and a list of trustet partners (truststore which holds root certificate). To configure a kafka broker follow the steps in this manual . By default hostname verification ( ssl.endpoint.identification.algorithm ) is enabled, so clients (brokers/producer/consumer) will check if the fully qualified domain name (FQDN) of the server (broker) equals the common name (CN) or the subject alternative name (SAN) of the server (broker) certificate. # listenerName:endpoint; listener=INTERNAL:localhost:9092 # the listeners which get send to zookeeper and the clients. if not set will equal to listener # advertised.listener= # The first is the listener name (e.g. INTERNAL) the second is the protocol (PLAINTEXT, SSL, ...) listener.security.protocol.map=INTERNAL:SSL If SSL protocol is activated for one listener you must locate the private/public key pair and you should set a accepted TLS Version. This can be achieved for all listeners with the ssl.* properties. If you want to adress a specific listener you need to prefix the properties with the listener.name. e.g. listener.name.internal.ssl.*. # set tls version ssl.enabled.protocols=TLSv1.3 # locate private/public key for broker ssl.keystore.location= ssl.keystore.password= ssl.key.password= If you want to activate encryption between the borkers you need to activate it and trust the certificates/private keys of the other brokers: # activate inter encryption security.inter.broker.protocol=SSL # add the certificate of the root ca to the truststore (or each broker certificate if there is no common ca) ssl.truststore.location= ssl.truststore.password= If a client needs to encrypt the connection to kafka via TLS a client need to activate encryption and trust the certificates/private keys of the brokers: # activate encryption security.protocol=SSL # add brokers certificate to truststore ssl.truststore.location= ssl.truststore.password= # to deactivate host name validation of the server certificate set it to blank # ssl.endpoint.identification.algorithm= If a client should authentificate via mTLS you need to set ssl.client.auth=requested or ssl.client.auth=required in the broker settings. Additionally to the encryption properties the client must configure a private/public key pair (which is trusted by the broker). # acitvate encryption security.protocol=SSL # add brokers certificate to truststore ssl.truststore.location= ssl.truststore.password= # add your own private/public keypair to the keystore (needs to be trusted by brokers) ssl.keystore.location= ssl.keystore.password= ssl.key.password= # to deactivate host name validation of the server certificate set it to blank # ssl.endpoint.identification.algorithm= After version 2.7 you can use inline PEMs for trusted certificates and private/public key/certificate: # encryption security.protocol=SSL # truststore replacement ssl.truststore.type=PEM ssl.truststore.certificates=-----BEGIN CERTIFICATE----- \\ <SECRET_HERE> \\ -----END CERTIFICATE----- # keystore replacement ssl.keystore.type=PEM ssl.keystore.key= ssl.keystore.certificate.chain= ACLs (autorization) With ACLs you can permit operations on resources (topic, Cluster, ..) Follow the instruction link to setup ACLs, but read the following keypoints and pitfalls before you do it: Define a Authorizer for the broker: authorizer.class.name=kafka.security.authorizer.AclAuthorizer When a client connects via mTLS the principal will be the certificate 'subject'/'distinguished name'. If you want to use only the a part of it you need to configure the ssl.principal.mapping.rules A valid ssl.principal.mapping.rules to extract the CN from a cert is: RULE: ^.*[Cc][Nn]=(([a-zA-Z0-9\\.\\-\\-]*).*$/$1/L), DEFAULT . The leading ^.* and the trailing .*$ seems to be needed. Do not forget to append th DEFAULT rule after a comma at the end If you want to debug if you Authorizer and your mapping rule work you should look into the kafka-authorizer.log . You may need to set the log level of the authorizer to DEBUG in the log4j.xml file. Use allow.everyone.if.no.acl.found=true when you want to dry run your setup. when defining a super.user you might want to set the whole 'distinguished name' as well as the CN in case your ssl.principal.mapping.rules does not work: super.user=User:CN=bla,OU=bla;User:bla . The User and the ; enclose one entry. To add ACLs you can use the kafka-acl command. If an authorizer is active you will also need an certificate for the admin user or the use you want to use when creating ACLs If you setup an authorizer you need to allow the broker of your cluster to operate on reosurce 'Cluster' or it may not function as expected Information on ACLs is stored in zookeeper (so you might want to activate ZooKeepers security as well)","title":"Kafka security"},{"location":"kafka/security/#kafka-security","text":"","title":"Kafka security"},{"location":"kafka/security/#on-prem","text":"","title":"on-prem"},{"location":"kafka/security/#create-trust-and-keystores","text":"#!/bin/bash validity=\"354\" keystore-pass=\"testtest\" key-pass=\"test\" distinguished-name=\"CN=myhost.de,OU=,DC=de\" hostname=\"myhost\" ca-passwort=\"testtest\" # create keystore for a broker keytool -keystore kafka.server.keystore.jks -alias localhost -keyalg RSA -validity {validity} -genkey -storepass {keystore-pass} -keypass {key-pass} -dname {distinguished-name} -ext SAN=DNS:{hostname} # private ca key openssl req -new -x509 -keyout ca-key -out ca-cert -days {validity} # create a client truststore and add ca cert to it keytool -keystore kafka.client.truststore.jks -alias CARoot -import -file ca-cert # create a server truststore and add a ca cert to it keytool -keystore kafka.server.truststore.jks -alias CARoot -importcert -file ca-cert # export the certificate from the boker keystore keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-file # sign the exportet certificate with the ca key openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password} # import the signed certificate and the ca cert to the broker keystore keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert keytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed","title":"create trust- and keystores"},{"location":"kafka/security/#encyption-tls-and-authentification-via-mtls","text":"You can encrypt the traffic between brokers and the traffic between brokers and clients with TLS. If you want to use ACLs on topics you also can configure authentification via mTLS. Every participant needs to have a public/private keypair (keystore) and a list of trustet partners (truststore which holds root certificate). To configure a kafka broker follow the steps in this manual . By default hostname verification ( ssl.endpoint.identification.algorithm ) is enabled, so clients (brokers/producer/consumer) will check if the fully qualified domain name (FQDN) of the server (broker) equals the common name (CN) or the subject alternative name (SAN) of the server (broker) certificate. # listenerName:endpoint; listener=INTERNAL:localhost:9092 # the listeners which get send to zookeeper and the clients. if not set will equal to listener # advertised.listener= # The first is the listener name (e.g. INTERNAL) the second is the protocol (PLAINTEXT, SSL, ...) listener.security.protocol.map=INTERNAL:SSL If SSL protocol is activated for one listener you must locate the private/public key pair and you should set a accepted TLS Version. This can be achieved for all listeners with the ssl.* properties. If you want to adress a specific listener you need to prefix the properties with the listener.name. e.g. listener.name.internal.ssl.*. # set tls version ssl.enabled.protocols=TLSv1.3 # locate private/public key for broker ssl.keystore.location= ssl.keystore.password= ssl.key.password= If you want to activate encryption between the borkers you need to activate it and trust the certificates/private keys of the other brokers: # activate inter encryption security.inter.broker.protocol=SSL # add the certificate of the root ca to the truststore (or each broker certificate if there is no common ca) ssl.truststore.location= ssl.truststore.password= If a client needs to encrypt the connection to kafka via TLS a client need to activate encryption and trust the certificates/private keys of the brokers: # activate encryption security.protocol=SSL # add brokers certificate to truststore ssl.truststore.location= ssl.truststore.password= # to deactivate host name validation of the server certificate set it to blank # ssl.endpoint.identification.algorithm= If a client should authentificate via mTLS you need to set ssl.client.auth=requested or ssl.client.auth=required in the broker settings. Additionally to the encryption properties the client must configure a private/public key pair (which is trusted by the broker). # acitvate encryption security.protocol=SSL # add brokers certificate to truststore ssl.truststore.location= ssl.truststore.password= # add your own private/public keypair to the keystore (needs to be trusted by brokers) ssl.keystore.location= ssl.keystore.password= ssl.key.password= # to deactivate host name validation of the server certificate set it to blank # ssl.endpoint.identification.algorithm= After version 2.7 you can use inline PEMs for trusted certificates and private/public key/certificate: # encryption security.protocol=SSL # truststore replacement ssl.truststore.type=PEM ssl.truststore.certificates=-----BEGIN CERTIFICATE----- \\ <SECRET_HERE> \\ -----END CERTIFICATE----- # keystore replacement ssl.keystore.type=PEM ssl.keystore.key= ssl.keystore.certificate.chain=","title":"encyption (TLS) and authentification (via mTLS)"},{"location":"kafka/security/#acls-autorization","text":"With ACLs you can permit operations on resources (topic, Cluster, ..) Follow the instruction link to setup ACLs, but read the following keypoints and pitfalls before you do it: Define a Authorizer for the broker: authorizer.class.name=kafka.security.authorizer.AclAuthorizer When a client connects via mTLS the principal will be the certificate 'subject'/'distinguished name'. If you want to use only the a part of it you need to configure the ssl.principal.mapping.rules A valid ssl.principal.mapping.rules to extract the CN from a cert is: RULE: ^.*[Cc][Nn]=(([a-zA-Z0-9\\.\\-\\-]*).*$/$1/L), DEFAULT . The leading ^.* and the trailing .*$ seems to be needed. Do not forget to append th DEFAULT rule after a comma at the end If you want to debug if you Authorizer and your mapping rule work you should look into the kafka-authorizer.log . You may need to set the log level of the authorizer to DEBUG in the log4j.xml file. Use allow.everyone.if.no.acl.found=true when you want to dry run your setup. when defining a super.user you might want to set the whole 'distinguished name' as well as the CN in case your ssl.principal.mapping.rules does not work: super.user=User:CN=bla,OU=bla;User:bla . The User and the ; enclose one entry. To add ACLs you can use the kafka-acl command. If an authorizer is active you will also need an certificate for the admin user or the use you want to use when creating ACLs If you setup an authorizer you need to allow the broker of your cluster to operate on reosurce 'Cluster' or it may not function as expected Information on ACLs is stored in zookeeper (so you might want to activate ZooKeepers security as well)","title":"ACLs (autorization)"},{"location":"kubernetes/ingress/","text":"Ingress multiple path for one domain \"In some cases, multiple paths within an Ingress will match a request. In those cases precedence will be given first to the longest matching path. If two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type.\" Source cert-manager Securing Ingress Resources with cert-manager","title":"Ingress"},{"location":"kubernetes/ingress/#ingress","text":"","title":"Ingress"},{"location":"kubernetes/ingress/#multiple-path-for-one-domain","text":"\"In some cases, multiple paths within an Ingress will match a request. In those cases precedence will be given first to the longest matching path. If two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type.\" Source","title":"multiple path for one domain"},{"location":"kubernetes/ingress/#cert-manager","text":"Securing Ingress Resources with cert-manager","title":"cert-manager"},{"location":"kubernetes/kubectl/","text":"kubectl snippets secrets # copy secret from one namespace to another kubectl get secret \"secretName\" --namespace=bla -oyaml | grep -v '^\\s*namespace:\\s' | kubectl apply --namespace=blubb -f - # download key of secret to file kubectl get secret \"secretName\" -o jsonpath='{.data.secretKey}' | base64 --decode > content.txt","title":"kubectl snippets"},{"location":"kubernetes/kubectl/#kubectl-snippets","text":"","title":"kubectl snippets"},{"location":"kubernetes/kubectl/#secrets","text":"# copy secret from one namespace to another kubectl get secret \"secretName\" --namespace=bla -oyaml | grep -v '^\\s*namespace:\\s' | kubectl apply --namespace=blubb -f - # download key of secret to file kubectl get secret \"secretName\" -o jsonpath='{.data.secretKey}' | base64 --decode > content.txt","title":"secrets"},{"location":"kubernetes/kubernetes-networking/","text":"kubernetes networking A pod has unique ip address which is reachable in the whole cluster. inside a pod between pods on a worker between pods on different workers containers inside a pod","title":"kubernetes networking"},{"location":"kubernetes/kubernetes-networking/#kubernetes-networking","text":"A pod has unique ip address which is reachable in the whole cluster.","title":"kubernetes networking"},{"location":"kubernetes/kubernetes-networking/#inside-a-pod","text":"","title":"inside a pod"},{"location":"kubernetes/kubernetes-networking/#between-pods-on-a-worker","text":"","title":"between pods on a worker"},{"location":"kubernetes/kubernetes-networking/#between-pods-on-different-workers","text":"","title":"between pods on different workers"},{"location":"kubernetes/kubernetes-networking/#_1","text":"","title":""},{"location":"kubernetes/kubernetes-networking/#containers-inside-a-pod","text":"","title":"containers inside a pod"},{"location":"kubernetes/kubernetes/","text":"Kuberentes overview Kubernetes (K8s) allows to deploy multiple containers on one or multiple host systems. K8s is essentially an API for a control loop (controller) which constantly checks if a resources is in a desired state and reconcile its state if it is not the case. The official documentation of k8s is outstanding. There is also a interactive tutorial . Outline of features: one master node and one/multiple worker nodes creates self-healing clusters allows rolling upgrade and rollback secret management and encapsulation via namespaces Reference glossary Components control plane components (manager) kube-apiserver - accepts http request of clients and can be directly accessed by a client library like kubectl etcd - saves state of workers kube-scheduler - placement of pod based on load and other factors kube-controller-manager - keep track of state on workers node components (worker) kubelet - kubernetes node agent kube-proxy - manages networking for node container runtime - containerd compliant software kubectl kubectl cli ref kubectl cheat sheet Deep dive into api structure Imperative commands Objects Kubernetes objects: represent the state of your cluster describes the desired state k8s will try to keep them alive Most have the following two fields: spec: desired state (which is described by you when creating the object) status: current state (which is updated and managed by control plane) Usually described in a .yaml file: which can be used with kubectl apply . which always contains at least the following fields: apiVersion : version of Kubernetes API you're using to create this object kind : - kind of object you want to create metadata : - Data that helps uniquely identify the object (a name string, UID, and optional namespace) spec : - desired state for the object the fields for every kind of object are described here . the yml file will get translated to a JSON which is send as a http payload to the apiserver Pods are one or multiple containers which share a unique network IP Address \"Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.\" Controllers Controllers allow: fail over scaling load balancing Kind of controllers: RepilcaSet: ensures that a specified number of pods is runnning in a node (needs another controller as wrapper like deployment) Deployment: declarative describe how to create and update instances (self-healing mechanism) DaemonsSets: ensures that all nodes run a specific copy of a pod (1 DaemonSet - 1 Pod - X nodes). Jobs: supervisor for pods which do batch jobs Services: allow communication between deployment controllers Services allows to manage how the pods can be accessed defines a logical set of pods pods are selected via a LabelSelector modes: ClusterIP (default): pods can only communicate within k8s cluster over there IP NodePort: pods can be accessed through the node ip and the so called NodePort load balancer: to expose application to the internet ExternalName: expose a service by a name Labels added to objects like pods, services, deployments key-value pairs to identify attributes of objects Selectors allows to select objects by a label equality-based: = and != set-based: IN , NOTIN , EXISTS Namespaces allows multiple virtual clusters on the same physical host there is a default namescpace install Install Docker like in the official documentation or use the install script curl -sSL https://get.docker.com | sh which might be easier Install kubectl install auto completion for kubectl : kubectl completion bash >/etc/bash_completion.d/kubectl might get permission issues with this command. Than write it to ~/kubectl and mv it with sudo to the target folder if you want to test stuff on your local machine install minikube . Note the 'Before you begin' section which shows you how to check for a supervisor. If you do not have one install one like VirtualBox. kubectl cli kubectl reference cheat sheet basic commands: - kubectl cluster-info list main nodes and worker nodes - kubectl apply -f FILENAME create/updates the defined objects in the given yaml file. - kubectl get all - kubectl get nodes - kubectl get services - kubectl get deployments - kubectl get rs list replica sets - -l <label> to load only resources with the given label update pod: - kubectl set image deployments/<POD_NAME> <POD_NAME>=<IMAGE>:<VERSIONTAG> set image of a pod to a newer version - rubectl rollout undo deployments/<POD_NAME> rollback to previous version create resources without k8s file: - kubectl run <POD_NAME> --image=<IMAGE> starts a pod with the given image - kubectl create deployment <deployment_name/POD_NAME> --image=<IMAGE> creates a deployment which starts a pod with the given image - kubectl expose <POD_NAME> --type=NodePort --port=80 creates a service on port 80 of the node. - kubectl label pod <POD_NAME> <labelKey=labelValue> creates a label on a pod - kubectl scale deployments/<POD_NAME> --replicas=4 creates a replication set debug pod: - kubectl proxy proxy into a cluster even when no services are exposed. Pods are accessible via there pod name or there internal ip address. Terminate with ctrl + c. - kubectl describe <node/pods/deployment> - kubectl kubectl logs <POD_NAME> - kubectl exec <POD_NAME> - execute a command on a container in a pod - kubectl exec -ti <POD_NAME> bash open bash in container minicube cli minicube start minicube service <POD_NAME> shows the service in the browser k8s yaml file (manifests) Overview Required fields: apiVersion: kubernetes API version you use kind: type of object (e.g. Deployment, Pod) metadata: fields that allow to uniquely identify the object (name, UID, optional namespace) spec: the state you desire for the object. The spec fields are different for ever object and can be look up under kubernetes api reference . Operators An operator is a custom controller which operates on a whole application (e.g. kafka, mysql) instead on an object (Pod, Service, ConfigMap..) An API build for running a specific application in k8s. It is build on top of k8s resource and controller api. only needed for stateful applications extends k8s API (can be used like other resources in a manifest file) The article which presented operator has a pretty good FAQ There is a maturity model for opertaors There is a currated list of Operators in this repository To build one use Operator SDK","title":"Kuberentes overview"},{"location":"kubernetes/kubernetes/#kuberentes-overview","text":"Kubernetes (K8s) allows to deploy multiple containers on one or multiple host systems. K8s is essentially an API for a control loop (controller) which constantly checks if a resources is in a desired state and reconcile its state if it is not the case. The official documentation of k8s is outstanding. There is also a interactive tutorial . Outline of features: one master node and one/multiple worker nodes creates self-healing clusters allows rolling upgrade and rollback secret management and encapsulation via namespaces Reference glossary","title":"Kuberentes overview"},{"location":"kubernetes/kubernetes/#components","text":"control plane components (manager) kube-apiserver - accepts http request of clients and can be directly accessed by a client library like kubectl etcd - saves state of workers kube-scheduler - placement of pod based on load and other factors kube-controller-manager - keep track of state on workers node components (worker) kubelet - kubernetes node agent kube-proxy - manages networking for node container runtime - containerd compliant software","title":"Components"},{"location":"kubernetes/kubernetes/#kubectl","text":"kubectl cli ref kubectl cheat sheet Deep dive into api structure Imperative commands","title":"kubectl"},{"location":"kubernetes/kubernetes/#objects","text":"Kubernetes objects: represent the state of your cluster describes the desired state k8s will try to keep them alive Most have the following two fields: spec: desired state (which is described by you when creating the object) status: current state (which is updated and managed by control plane) Usually described in a .yaml file: which can be used with kubectl apply . which always contains at least the following fields: apiVersion : version of Kubernetes API you're using to create this object kind : - kind of object you want to create metadata : - Data that helps uniquely identify the object (a name string, UID, and optional namespace) spec : - desired state for the object the fields for every kind of object are described here . the yml file will get translated to a JSON which is send as a http payload to the apiserver","title":"Objects"},{"location":"kubernetes/kubernetes/#pods","text":"are one or multiple containers which share a unique network IP Address \"Containers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.\"","title":"Pods"},{"location":"kubernetes/kubernetes/#controllers","text":"Controllers allow: fail over scaling load balancing Kind of controllers: RepilcaSet: ensures that a specified number of pods is runnning in a node (needs another controller as wrapper like deployment) Deployment: declarative describe how to create and update instances (self-healing mechanism) DaemonsSets: ensures that all nodes run a specific copy of a pod (1 DaemonSet - 1 Pod - X nodes). Jobs: supervisor for pods which do batch jobs Services: allow communication between deployment controllers","title":"Controllers"},{"location":"kubernetes/kubernetes/#services","text":"allows to manage how the pods can be accessed defines a logical set of pods pods are selected via a LabelSelector modes: ClusterIP (default): pods can only communicate within k8s cluster over there IP NodePort: pods can be accessed through the node ip and the so called NodePort load balancer: to expose application to the internet ExternalName: expose a service by a name","title":"Services"},{"location":"kubernetes/kubernetes/#labels","text":"added to objects like pods, services, deployments key-value pairs to identify attributes of objects","title":"Labels"},{"location":"kubernetes/kubernetes/#selectors","text":"allows to select objects by a label equality-based: = and != set-based: IN , NOTIN , EXISTS","title":"Selectors"},{"location":"kubernetes/kubernetes/#namespaces","text":"allows multiple virtual clusters on the same physical host there is a default namescpace","title":"Namespaces"},{"location":"kubernetes/kubernetes/#install","text":"Install Docker like in the official documentation or use the install script curl -sSL https://get.docker.com | sh which might be easier Install kubectl install auto completion for kubectl : kubectl completion bash >/etc/bash_completion.d/kubectl might get permission issues with this command. Than write it to ~/kubectl and mv it with sudo to the target folder if you want to test stuff on your local machine install minikube . Note the 'Before you begin' section which shows you how to check for a supervisor. If you do not have one install one like VirtualBox.","title":"install"},{"location":"kubernetes/kubernetes/#kubectl-cli","text":"kubectl reference cheat sheet basic commands: - kubectl cluster-info list main nodes and worker nodes - kubectl apply -f FILENAME create/updates the defined objects in the given yaml file. - kubectl get all - kubectl get nodes - kubectl get services - kubectl get deployments - kubectl get rs list replica sets - -l <label> to load only resources with the given label update pod: - kubectl set image deployments/<POD_NAME> <POD_NAME>=<IMAGE>:<VERSIONTAG> set image of a pod to a newer version - rubectl rollout undo deployments/<POD_NAME> rollback to previous version create resources without k8s file: - kubectl run <POD_NAME> --image=<IMAGE> starts a pod with the given image - kubectl create deployment <deployment_name/POD_NAME> --image=<IMAGE> creates a deployment which starts a pod with the given image - kubectl expose <POD_NAME> --type=NodePort --port=80 creates a service on port 80 of the node. - kubectl label pod <POD_NAME> <labelKey=labelValue> creates a label on a pod - kubectl scale deployments/<POD_NAME> --replicas=4 creates a replication set debug pod: - kubectl proxy proxy into a cluster even when no services are exposed. Pods are accessible via there pod name or there internal ip address. Terminate with ctrl + c. - kubectl describe <node/pods/deployment> - kubectl kubectl logs <POD_NAME> - kubectl exec <POD_NAME> - execute a command on a container in a pod - kubectl exec -ti <POD_NAME> bash open bash in container","title":"kubectl cli"},{"location":"kubernetes/kubernetes/#minicube-cli","text":"minicube start minicube service <POD_NAME> shows the service in the browser","title":"minicube cli"},{"location":"kubernetes/kubernetes/#k8s-yaml-file-manifests","text":"Overview Required fields: apiVersion: kubernetes API version you use kind: type of object (e.g. Deployment, Pod) metadata: fields that allow to uniquely identify the object (name, UID, optional namespace) spec: the state you desire for the object. The spec fields are different for ever object and can be look up under kubernetes api reference .","title":"k8s yaml file (manifests)"},{"location":"kubernetes/kubernetes/#operators","text":"An operator is a custom controller which operates on a whole application (e.g. kafka, mysql) instead on an object (Pod, Service, ConfigMap..) An API build for running a specific application in k8s. It is build on top of k8s resource and controller api. only needed for stateful applications extends k8s API (can be used like other resources in a manifest file) The article which presented operator has a pretty good FAQ There is a maturity model for opertaors There is a currated list of Operators in this repository To build one use Operator SDK","title":"Operators"},{"location":"linux%20%28debian%20based%29/cockpit/","text":"red hat cockpit cockpit server to connect via browser cockpit client which runs on you workstation and does not need a cockpit server use cockpit workstation client type \"localhost\" to connect to local machine type user@ip or user@dns to connect to remote. Allows to use ssh key as well as password. use cockpit server Runs on port 9090. Can be used on host it runs on. The Host switcher feature is deprecated and tzhe connection feature only allows username + password and not ssh. install cockpit server sudo dnf install cockpit -y sudo systemctl enable --now cockpit.socket sudo dnf install cockpit-podman -y sudo dnf install cockpit-machines -y sudo dnf install cockpit-files -y Only allows to connect to other servers via password. Does not support to connect to other server with ssh. cockpit workstation client FlatHub page","title":"red hat cockpit"},{"location":"linux%20%28debian%20based%29/cockpit/#red-hat-cockpit","text":"cockpit server to connect via browser cockpit client which runs on you workstation and does not need a cockpit server","title":"red hat cockpit"},{"location":"linux%20%28debian%20based%29/cockpit/#use-cockpit-workstation-client","text":"type \"localhost\" to connect to local machine type user@ip or user@dns to connect to remote. Allows to use ssh key as well as password.","title":"use cockpit workstation client"},{"location":"linux%20%28debian%20based%29/cockpit/#use-cockpit-server","text":"Runs on port 9090. Can be used on host it runs on. The Host switcher feature is deprecated and tzhe connection feature only allows username + password and not ssh.","title":"use cockpit server"},{"location":"linux%20%28debian%20based%29/cockpit/#install","text":"","title":"install"},{"location":"linux%20%28debian%20based%29/cockpit/#cockpit-server","text":"sudo dnf install cockpit -y sudo systemctl enable --now cockpit.socket sudo dnf install cockpit-podman -y sudo dnf install cockpit-machines -y sudo dnf install cockpit-files -y Only allows to connect to other servers via password. Does not support to connect to other server with ssh.","title":"cockpit server"},{"location":"linux%20%28debian%20based%29/cockpit/#cockpit-workstation-client","text":"FlatHub page","title":"cockpit workstation client"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/","text":"Copy&Pasta CLI installations & config create multiple direcotries with one command mkdir -p applications/my-new-app/{base,overlays/{dev,staging,prod}} The shell expands this into: applications/my-new-app/base applications/my-new-app/overlays/dev applications/my-new-app/overlays/staging applications/my-new-app/overlays/prod homebrew - add to path echo 'export PATH=$PATH:/home/linuxbrew/.linuxbrew/bin/' >> ~/.bashrc && source ~/.bashrc && brew help docker - on debian You cannot use linuxbrew to install docker engine . SO you need to install it manually. Brew can however be used to install docker cli with bashcompletion. # install docker ## you may want to check the script before running it ## source: https://docs.docker.com/engine/install/debian/#install-using-the-convenience-script curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh # allow current user to manage docker ## source: https://docs.docker.com/engine/install/debian/#install-using-the-convenience-script sudo groupadd docker sudo usermod -aG docker $USER ## log out current user docker run hello-world k8s ecosystem # kubectl brew install kubernetes-cli kubectl version # kubectl package manager brew install krew /home/linuxbrew/.linuxbrew/bin/kubectl-krew install krew echo 'export PATH=${PATH}:~/.krew/bin/' >> ~/.bashrc && source ~/.bashrc ## for changing context Usage: kubectl ctx kubectl krew install ctx ## for changing namespace Usage: kubectl ns kubectl krew install ns ## for merging k8s config files. Usage: kubectl konfig kubectl krew install konfig bash completion any binary installed with brew If the binary is installed with brew there is a chance that the bash completion script is genereted to the $(brew --prefix)/etc/bash_completion.d/ direcoty. If so brew will tell you at the end of the installation. # run once after installing brew echo 'for BREW_COMPLETION in \"$(brew --prefix)/etc/bash_completion.d/\"*; do [[ -r \"$BREW_COMPLETION\" ]] && source \"$BREW_COMPLETION\" done' \\ >>~/.bashrc && source ~/.bashrc # after brew added a new completion script just source source ~/.bashrc kubectl - bash auto-completion echo 'source <(kubectl completion bash)' >>~/.bashrc && source ~/.bashrc helm - bash auto-completion echo 'source <(helm completion bash)' >>~/.bashrc && source ~/.bashrc k3d bash auto-completion echo 'source <(k3d completion bash)' >>~/.bashrc && source ~/.bashrc Create cluster k3d cluster create Add image from local container host to k3d cluster (# prerequisite see k3d page ) k3d image import IMAGENAME --cluster local-cluster git # alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status # ssh keygen ssh-keygen -t ed25519 # remember ssh password ssh-add ~/.ssh/*_rsa","title":"Copy&Pasta CLI installations & config"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#copypasta-cli-installations-config","text":"","title":"Copy&amp;Pasta CLI installations &amp; config"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#create-multiple-direcotries-with-one-command","text":"mkdir -p applications/my-new-app/{base,overlays/{dev,staging,prod}} The shell expands this into: applications/my-new-app/base applications/my-new-app/overlays/dev applications/my-new-app/overlays/staging applications/my-new-app/overlays/prod","title":"create multiple direcotries with one command"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#homebrew-add-to-path","text":"echo 'export PATH=$PATH:/home/linuxbrew/.linuxbrew/bin/' >> ~/.bashrc && source ~/.bashrc && brew help","title":"homebrew - add to path"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#docker-on-debian","text":"You cannot use linuxbrew to install docker engine . SO you need to install it manually. Brew can however be used to install docker cli with bashcompletion. # install docker ## you may want to check the script before running it ## source: https://docs.docker.com/engine/install/debian/#install-using-the-convenience-script curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh # allow current user to manage docker ## source: https://docs.docker.com/engine/install/debian/#install-using-the-convenience-script sudo groupadd docker sudo usermod -aG docker $USER ## log out current user docker run hello-world","title":"docker - on debian"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#k8s-ecosystem","text":"# kubectl brew install kubernetes-cli kubectl version # kubectl package manager brew install krew /home/linuxbrew/.linuxbrew/bin/kubectl-krew install krew echo 'export PATH=${PATH}:~/.krew/bin/' >> ~/.bashrc && source ~/.bashrc ## for changing context Usage: kubectl ctx kubectl krew install ctx ## for changing namespace Usage: kubectl ns kubectl krew install ns ## for merging k8s config files. Usage: kubectl konfig kubectl krew install konfig","title":"k8s ecosystem"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#bash-completion","text":"","title":"bash completion"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#any-binary-installed-with-brew","text":"If the binary is installed with brew there is a chance that the bash completion script is genereted to the $(brew --prefix)/etc/bash_completion.d/ direcoty. If so brew will tell you at the end of the installation. # run once after installing brew echo 'for BREW_COMPLETION in \"$(brew --prefix)/etc/bash_completion.d/\"*; do [[ -r \"$BREW_COMPLETION\" ]] && source \"$BREW_COMPLETION\" done' \\ >>~/.bashrc && source ~/.bashrc # after brew added a new completion script just source source ~/.bashrc","title":"any binary installed with brew"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#kubectl-bash-auto-completion","text":"echo 'source <(kubectl completion bash)' >>~/.bashrc && source ~/.bashrc","title":"kubectl - bash auto-completion"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#helm-bash-auto-completion","text":"echo 'source <(helm completion bash)' >>~/.bashrc && source ~/.bashrc","title":"helm - bash auto-completion"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#k3d","text":"bash auto-completion echo 'source <(k3d completion bash)' >>~/.bashrc && source ~/.bashrc Create cluster k3d cluster create Add image from local container host to k3d cluster (# prerequisite see k3d page ) k3d image import IMAGENAME --cluster local-cluster","title":"k3d"},{"location":"linux%20%28debian%20based%29/copypasta_for_cli/#git","text":"# alias git config --global alias.co checkout git config --global alias.br branch git config --global alias.ci commit git config --global alias.st status # ssh keygen ssh-keygen -t ed25519 # remember ssh password ssh-add ~/.ssh/*_rsa","title":"git"},{"location":"linux%20%28debian%20based%29/desktop-files/","text":"Application Interface create .desktop files .desktop files put applications in the desktop menu. The files are placed in /usr/share/applications and ~/.local/share/applications . If the path to executable contains spaces it needs to be put in double quotes. Single quotes do not work. A nice tool to edit and create such files is MenuLibre : Install via sudo apt-get install menulibre afterwards just launch it from command line menulibre and create e new entry for menulibre itself so it will appear in Applauncher. default application The MimeType field in the .desktop file tells the os which files the application can handle. The default application for a MimeType are configured in ~/.config/mimeapps.list vs code issue (super+e) Vs code opens when you press super+e instead of the file browser. To resolve the issue add inode/directory = dde-file-manager.desktop;code.desktop; to ~/.config/mimeapps.list.","title":"Application Interface"},{"location":"linux%20%28debian%20based%29/desktop-files/#application-interface","text":"","title":"Application Interface"},{"location":"linux%20%28debian%20based%29/desktop-files/#create-desktop-files","text":".desktop files put applications in the desktop menu. The files are placed in /usr/share/applications and ~/.local/share/applications . If the path to executable contains spaces it needs to be put in double quotes. Single quotes do not work. A nice tool to edit and create such files is MenuLibre : Install via sudo apt-get install menulibre afterwards just launch it from command line menulibre and create e new entry for menulibre itself so it will appear in Applauncher.","title":"create .desktop files"},{"location":"linux%20%28debian%20based%29/desktop-files/#default-application","text":"The MimeType field in the .desktop file tells the os which files the application can handle. The default application for a MimeType are configured in ~/.config/mimeapps.list","title":"default application"},{"location":"linux%20%28debian%20based%29/desktop-files/#vs-code-issue-supere","text":"Vs code opens when you press super+e instead of the file browser. To resolve the issue add inode/directory = dde-file-manager.desktop;code.desktop; to ~/.config/mimeapps.list.","title":"vs code issue (super+e)"},{"location":"linux%20%28debian%20based%29/linux_filesystem/","text":"Linux filesystem disk suage # show disk usage ncdu # show disk usage du -sch symbolic links vs. hard links symbolic links (/soft links/symlinks) and hard links # soft link (symlink) ln -s source.file link.file # hard link ln source.file link.file hierarchy disk physical volume volume group logical volume filesystem / mount point mount a logical vol create logical volume on volume group check where it is created under /dev/ formate the logical volume mount volume (can be anyhere is not restricted to root / level of filesystem) (optional) add mount to fstab file so it will recreated at startup # 1. sudo lvcreate -L 100G -n <new logical volume> <volume group> # 2. sudo lvscan # 3. path can be found via step 2 sudo mkfs.ext4 /dev/<volume group>/<logical volume> #4 sudo mount /dev/<volume group>/<logical volume> /<directory_which_exists> extend a logical volume check how much space is left on volume group check where the loigcal volume is located under /dev/ (not /dev/mapping but the real path) extend logical volume check if successful # 1. vgs # 2. lvscan # 3. path can be found via step 2 # -L50G is target size 50 GB lvextend -L50G /dev/<volume group>/<logical volume> # 4. lvscan fstab /etc/fstab is responsible for mounting volumes on startup. always create a backup : sudo cp /etc/fstab /etc/fstab.backup before restarting check if fstab is ok. If there are no returned messages everything is okay: sudo mount -a mount a mount point is just a or folder/directory (e.g. mkdir -p /data ) you do not need to use the /mnt directory. It is just a convention to use it if you do not have a specific directory to mount you can mount formatted devices under /dev to any mountpoint to make a mount persistant after restart you need to add it to /etc/fstab List stuff df -h du -g # list physical volumes pvs # list volume groups vgs # list logical volume lvs","title":"Linux filesystem"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#linux-filesystem","text":"","title":"Linux filesystem"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#disk-suage","text":"# show disk usage ncdu # show disk usage du -sch","title":"disk suage"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#symbolic-links-vs-hard-links","text":"symbolic links (/soft links/symlinks) and hard links # soft link (symlink) ln -s source.file link.file # hard link ln source.file link.file","title":"symbolic links vs. hard links"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#hierarchy","text":"disk physical volume volume group logical volume filesystem / mount point","title":"hierarchy"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#mount-a-logical-vol","text":"create logical volume on volume group check where it is created under /dev/ formate the logical volume mount volume (can be anyhere is not restricted to root / level of filesystem) (optional) add mount to fstab file so it will recreated at startup # 1. sudo lvcreate -L 100G -n <new logical volume> <volume group> # 2. sudo lvscan # 3. path can be found via step 2 sudo mkfs.ext4 /dev/<volume group>/<logical volume> #4 sudo mount /dev/<volume group>/<logical volume> /<directory_which_exists>","title":"mount a logical vol"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#extend-a-logical-volume","text":"check how much space is left on volume group check where the loigcal volume is located under /dev/ (not /dev/mapping but the real path) extend logical volume check if successful # 1. vgs # 2. lvscan # 3. path can be found via step 2 # -L50G is target size 50 GB lvextend -L50G /dev/<volume group>/<logical volume> # 4. lvscan","title":"extend a logical volume"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#fstab","text":"/etc/fstab is responsible for mounting volumes on startup. always create a backup : sudo cp /etc/fstab /etc/fstab.backup before restarting check if fstab is ok. If there are no returned messages everything is okay: sudo mount -a","title":"fstab"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#mount","text":"a mount point is just a or folder/directory (e.g. mkdir -p /data ) you do not need to use the /mnt directory. It is just a convention to use it if you do not have a specific directory to mount you can mount formatted devices under /dev to any mountpoint to make a mount persistant after restart you need to add it to /etc/fstab","title":"mount"},{"location":"linux%20%28debian%20based%29/linux_filesystem/#list-stuff","text":"df -h du -g # list physical volumes pvs # list volume groups vgs # list logical volume lvs","title":"List stuff"},{"location":"linux%20%28debian%20based%29/linux_operations/","text":"linux operations (without containers) misc find file anywhere: sudo locate filename (index is updated once a day. recreate with sudo updatedb ) find a file which contains a string : grep -nr 'search*' /opt/app/ find where a on the path is saved: which execuableName check hardware lshw - cpu cores and frequency and ram lsblk - check disk size permissions Source of image Unix permission bit calculator not binary format Roles: u = user g = group o = other Permission: r = read w = write x = execute You can also use binary format three numbers (e.g. 755) first: user second: group third: other value of permissions read: 4 write: 2 execute: 1 default permission bits are defined by the umask You can set the owner user and the owner group of a file with chown . # set user and group as ownder of the direcoty and all its content sudo chown -R user:group /var/myfolder To handle the rights of a file you can use chmod . # add execute rights for the (owner) user sudo chmod u+x /opt/myapp/config/start.sh # remove execute rights for others sudo chmod o-x /opt/myapp/config/start.sh # remove write and read rights from the group of a file sudo chmod g-rw /opt/myapp/config/config.properties # remove write and read rights from the group of a file for every file in the directory sudo chmod -R g-rw /opt/myapp/config/ root sudo su su - changes environment (e.g ~/bashrc, ) install application Assumes that you need to use tar balls. If you use a package manager most or all of the steps will be done automatically depending on the package manager you used. add a user and a group download tar and extract it to /opt/ configure application to save data and log outside of the installation folder define a service for the application user management Add user # list users cat /etc/passwd # create user sudo useradd username Add user to group (e.g. to sudo group): # list groups cat /etc/group # list groups for user id username # create group sudo groupadd groupname # add user to group sudo usermod -aG groupname username # For the changes to take effect you need to logout the user.OR it may suffice to run one of the following commands: # newgrp - groupname # sudo su - $USER # when using vscode remote kill the remote vs code server # add to sudo group (sudoers) sudo usermod -aG sudo newuser # you may want to take a look into the file /etc/sudoers if the group sudo does not exist Add public ssh key to user e.g. for connecting remotely to this user with the corresponding private key # create a ~/.ssh/ folder if it does not exist mkdir -p ~/.ssh/ printf \"<content of pub key>\" >> ~/.ssh/authorized_keys chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys # ALTERNATIVE: Execute from remote to add your own key to the server (works only if you have a password or other authentification mechanism yet) ssh-copy-id -i ~/.ssh/mykey user@host Add private ssh key to user e.g. for authentication on another machine where the public/private key pair is authorized ssh-keygen -t ed25519 Allow user to use sudo # either in /etc/sudoers.d/local # or just add to a special group which might be sudo sudo adduser $USER sudo # or another special group like operator or such # to disable password prompt when using sudo: ## opens /etc/sudoers but with syntax check sudo visudo tar - unzip files You have the following tar file: softwarename-1.2.3.tar.gz Unzip with wildcard (e.g. to omit version): tar xzvf softwarename-*.tar.gz -> folder: softwarename-1.2.3 Unzip to a target folder: tar xzvf softwarename-1.2.3.tar.gz -C /opt -> folder: /opt/softwarename-1.2.3 Unzip to a target folder and set a new folder name: tar xzvf softwarename-1.2.3.tar.gz && mv softwarename-1.2.3/* /opt/softwarename -> folder: /opt/softwarename systemd - run an application as a service A service is a way to start a application as a daemon process. How to create a service depends on which linux you use. The old system is called system V (\"system five\") (initd). The following assumes that you use the newer systemd. why? systemd systemd unit files how create a service in systemd you need to create a so called unit file: in direcotry /etc/systemd/system/ File with ending '.service' containes '[Unit]' and '[Service]' and '[Install]' block Interact with a service: Status of service: systemctl start Start service: systemctl start Stop service: systemctl stop Restart service: systemctl restart At system startup: systemctl enable At system startup: systemctl disable memomory consumption # memory usage in kb ps aux --sort -rss | head","title":"linux operations (without containers)"},{"location":"linux%20%28debian%20based%29/linux_operations/#linux-operations-without-containers","text":"","title":"linux operations (without containers)"},{"location":"linux%20%28debian%20based%29/linux_operations/#misc","text":"find file anywhere: sudo locate filename (index is updated once a day. recreate with sudo updatedb ) find a file which contains a string : grep -nr 'search*' /opt/app/ find where a on the path is saved: which execuableName","title":"misc"},{"location":"linux%20%28debian%20based%29/linux_operations/#check-hardware","text":"lshw - cpu cores and frequency and ram lsblk - check disk size","title":"check hardware"},{"location":"linux%20%28debian%20based%29/linux_operations/#permissions","text":"Source of image Unix permission bit calculator not binary format Roles: u = user g = group o = other Permission: r = read w = write x = execute You can also use binary format three numbers (e.g. 755) first: user second: group third: other value of permissions read: 4 write: 2 execute: 1 default permission bits are defined by the umask You can set the owner user and the owner group of a file with chown . # set user and group as ownder of the direcoty and all its content sudo chown -R user:group /var/myfolder To handle the rights of a file you can use chmod . # add execute rights for the (owner) user sudo chmod u+x /opt/myapp/config/start.sh # remove execute rights for others sudo chmod o-x /opt/myapp/config/start.sh # remove write and read rights from the group of a file sudo chmod g-rw /opt/myapp/config/config.properties # remove write and read rights from the group of a file for every file in the directory sudo chmod -R g-rw /opt/myapp/config/","title":"permissions"},{"location":"linux%20%28debian%20based%29/linux_operations/#root","text":"sudo su su - changes environment (e.g ~/bashrc, )","title":"root"},{"location":"linux%20%28debian%20based%29/linux_operations/#install-application","text":"Assumes that you need to use tar balls. If you use a package manager most or all of the steps will be done automatically depending on the package manager you used. add a user and a group download tar and extract it to /opt/ configure application to save data and log outside of the installation folder define a service for the application","title":"install application"},{"location":"linux%20%28debian%20based%29/linux_operations/#user-management","text":"","title":"user management"},{"location":"linux%20%28debian%20based%29/linux_operations/#add-user","text":"# list users cat /etc/passwd # create user sudo useradd username","title":"Add user"},{"location":"linux%20%28debian%20based%29/linux_operations/#add-user-to-group-eg-to-sudo-group","text":"# list groups cat /etc/group # list groups for user id username # create group sudo groupadd groupname # add user to group sudo usermod -aG groupname username # For the changes to take effect you need to logout the user.OR it may suffice to run one of the following commands: # newgrp - groupname # sudo su - $USER # when using vscode remote kill the remote vs code server # add to sudo group (sudoers) sudo usermod -aG sudo newuser # you may want to take a look into the file /etc/sudoers if the group sudo does not exist","title":"Add user to group (e.g. to sudo group):"},{"location":"linux%20%28debian%20based%29/linux_operations/#add-public-ssh-key-to-user","text":"e.g. for connecting remotely to this user with the corresponding private key # create a ~/.ssh/ folder if it does not exist mkdir -p ~/.ssh/ printf \"<content of pub key>\" >> ~/.ssh/authorized_keys chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys # ALTERNATIVE: Execute from remote to add your own key to the server (works only if you have a password or other authentification mechanism yet) ssh-copy-id -i ~/.ssh/mykey user@host","title":"Add public ssh key to user"},{"location":"linux%20%28debian%20based%29/linux_operations/#add-private-ssh-key-to-user","text":"e.g. for authentication on another machine where the public/private key pair is authorized ssh-keygen -t ed25519","title":"Add private ssh key to user"},{"location":"linux%20%28debian%20based%29/linux_operations/#allow-user-to-use-sudo","text":"# either in /etc/sudoers.d/local # or just add to a special group which might be sudo sudo adduser $USER sudo # or another special group like operator or such # to disable password prompt when using sudo: ## opens /etc/sudoers but with syntax check sudo visudo","title":"Allow user to use sudo"},{"location":"linux%20%28debian%20based%29/linux_operations/#tar-unzip-files","text":"You have the following tar file: softwarename-1.2.3.tar.gz Unzip with wildcard (e.g. to omit version): tar xzvf softwarename-*.tar.gz -> folder: softwarename-1.2.3 Unzip to a target folder: tar xzvf softwarename-1.2.3.tar.gz -C /opt -> folder: /opt/softwarename-1.2.3 Unzip to a target folder and set a new folder name: tar xzvf softwarename-1.2.3.tar.gz && mv softwarename-1.2.3/* /opt/softwarename -> folder: /opt/softwarename","title":"tar - unzip files"},{"location":"linux%20%28debian%20based%29/linux_operations/#systemd-run-an-application-as-a-service","text":"A service is a way to start a application as a daemon process. How to create a service depends on which linux you use. The old system is called system V (\"system five\") (initd). The following assumes that you use the newer systemd.","title":"systemd - run an application as a service"},{"location":"linux%20%28debian%20based%29/linux_operations/#why","text":"systemd systemd unit files","title":"why?"},{"location":"linux%20%28debian%20based%29/linux_operations/#how","text":"create a service in systemd you need to create a so called unit file: in direcotry /etc/systemd/system/ File with ending '.service' containes '[Unit]' and '[Service]' and '[Install]' block Interact with a service: Status of service: systemctl start Start service: systemctl start Stop service: systemctl stop Restart service: systemctl restart At system startup: systemctl enable At system startup: systemctl disable","title":"how"},{"location":"linux%20%28debian%20based%29/linux_operations/#memomory-consumption","text":"# memory usage in kb ps aux --sort -rss | head","title":"memomory consumption"},{"location":"linux%20%28debian%20based%29/network_capture_traffic/","text":"Capture network traffic # list interfaces sudo tcpdump -D # capture traffic in asci format on port 80. -n disables name resolution sudo tcpdump -i eth0 -A -n port 80","title":"Capture network traffic"},{"location":"linux%20%28debian%20based%29/network_capture_traffic/#capture-network-traffic","text":"# list interfaces sudo tcpdump -D # capture traffic in asci format on port 80. -n disables name resolution sudo tcpdump -i eth0 -A -n port 80","title":"Capture network traffic"},{"location":"linux%20%28debian%20based%29/network_ports/","text":"Ports check open ports on local machine ## all ports sudo ss -tulnp # or sudo netstat -tulnp # or sudo lsof -Pin ## specific port sudo lsof -i :8080 windows: tnc google.com -port 80 # find and kill aplication which occupies port (windows) netstat -ano | findstr :8080 taskkill /PID 13744 /F access non http traffic on a port nc -vz IP PORT scan for open ports from remote nc -zv -w 1 TARGET_PORT TARGET_PORT # remote machine: check for open tcp ports (scans the first 1000 ports) nmap <ip_address> # remote machine: check for open tcp ports (scans all ports) nmap -p- <ip_address> # remote machine: check if tcp port 8080 is open nmap -p 8080 <ip_address> start webserver on port # start webserver on port mkdir tmp && cd tmp && echo \"Hello, you reached Jans webserver\" > index.html python3 -m http.server 8080 cd -","title":"Ports"},{"location":"linux%20%28debian%20based%29/network_ports/#ports","text":"","title":"Ports"},{"location":"linux%20%28debian%20based%29/network_ports/#check-open-ports-on-local-machine","text":"## all ports sudo ss -tulnp # or sudo netstat -tulnp # or sudo lsof -Pin ## specific port sudo lsof -i :8080 windows: tnc google.com -port 80 # find and kill aplication which occupies port (windows) netstat -ano | findstr :8080 taskkill /PID 13744 /F","title":"check open ports on local machine"},{"location":"linux%20%28debian%20based%29/network_ports/#access-non-http-traffic-on-a-port","text":"nc -vz IP PORT","title":"access non http traffic on a port"},{"location":"linux%20%28debian%20based%29/network_ports/#scan-for-open-ports-from-remote","text":"nc -zv -w 1 TARGET_PORT TARGET_PORT # remote machine: check for open tcp ports (scans the first 1000 ports) nmap <ip_address> # remote machine: check for open tcp ports (scans all ports) nmap -p- <ip_address> # remote machine: check if tcp port 8080 is open nmap -p 8080 <ip_address>","title":"scan for open ports from remote"},{"location":"linux%20%28debian%20based%29/network_ports/#start-webserver-on-port","text":"# start webserver on port mkdir tmp && cd tmp && echo \"Hello, you reached Jans webserver\" > index.html python3 -m http.server 8080 cd -","title":"start webserver on port"},{"location":"linux%20%28debian%20based%29/network_ssh/","text":"SSH generate ssh key The -i flag for connection can be omitted if the standard file names id_ALGORITHM (private key) and id_ALGORITHM.pub (public key) are used. # generate a private and public key pair in ~/.ssh/ folder ssh-keygen -t ed25519 # in specifig folder ssh-keygen -t ed25519 -f /tmp/ # needed permisions of files. Normally created automatically chmod 700 ~/.ssh chmod 644 ~/.ssh/id_ed25519.pub chmod 600 ~/.ssh/id_ed25519 connect via ssh # add -v for verbose mode # connect ssh <user>@<ip or url> # connect on non standard port ssh -p 12345 <user>@<ip or url> # connect use rsa private key saved under ~/.ssh/ instead of password ssh -i ~/.ssh/id_rsa <user>@<ip or url> ssh tunnel nice explanation of ssh tunnels # ssh tunnel which connects my local port 1025 with the remote port 4000 over the standard ssh port 22 ssh -L1025:192.168.x.x:4000 -v <user>@<ip or url> # if ssh is forwarded it can be used via -p flag ssh -p1025 <user>@192.168.x.x handle identity with password For .bashrc or .zshrc to cache password on startup if not cached already: if [ -z \"$SSH_AUTH_SOCK\" ]; then eval \"$(ssh-agent -s)\" > /dev/null fi # Function to check if the key is already added check_key_added() { ssh-add -l | grep -q \"$(ssh-keygen -lf ~/.ssh/id_ed25519 | awk '{print $2}')\" } # Add the key if it's not already added if ! check_key_added; then echo \"Add ssh key for 8 hours\" ssh-add -t 28800 ~/.ssh/id_ed25519 # 8 hours timeout fi trust ssh key on remote server # your user: copy your public key to the authorized_keys file in your home directory on a remote machine # allows access with your private key instead of a password ssh-copy-id -i ~/.ssh/id_rsa user@host # other user: copy your public key to the authorized_keys file in another home direcotry cat ~/id_rsa.pub | ssh your_user@remote.server.com \u201csudo tee -a /USER/.ssh/authorized_keys\u201d location of known_hosts Known hosts are saved in following locations - as entry in /etc/ssh/ssh_known_hosts - as entry in /etc/ssh/ssh_config - as *.pub file in /etc/ssh/ folder add and remove entries from known_hosts ```shell remove old entry from known_host ssh-keygen -R SERVER_NAME -f ~/.ssh/known_hosts add new key to known_hosts ssh-keyscan SERVER_NAME >> ~/.ssh/known_hosts","title":"SSH"},{"location":"linux%20%28debian%20based%29/network_ssh/#ssh","text":"","title":"SSH"},{"location":"linux%20%28debian%20based%29/network_ssh/#generate-ssh-key","text":"The -i flag for connection can be omitted if the standard file names id_ALGORITHM (private key) and id_ALGORITHM.pub (public key) are used. # generate a private and public key pair in ~/.ssh/ folder ssh-keygen -t ed25519 # in specifig folder ssh-keygen -t ed25519 -f /tmp/ # needed permisions of files. Normally created automatically chmod 700 ~/.ssh chmod 644 ~/.ssh/id_ed25519.pub chmod 600 ~/.ssh/id_ed25519","title":"generate ssh key"},{"location":"linux%20%28debian%20based%29/network_ssh/#connect-via-ssh","text":"# add -v for verbose mode # connect ssh <user>@<ip or url> # connect on non standard port ssh -p 12345 <user>@<ip or url> # connect use rsa private key saved under ~/.ssh/ instead of password ssh -i ~/.ssh/id_rsa <user>@<ip or url>","title":"connect via ssh"},{"location":"linux%20%28debian%20based%29/network_ssh/#ssh-tunnel","text":"nice explanation of ssh tunnels # ssh tunnel which connects my local port 1025 with the remote port 4000 over the standard ssh port 22 ssh -L1025:192.168.x.x:4000 -v <user>@<ip or url> # if ssh is forwarded it can be used via -p flag ssh -p1025 <user>@192.168.x.x","title":"ssh tunnel"},{"location":"linux%20%28debian%20based%29/network_ssh/#handle-identity-with-password","text":"For .bashrc or .zshrc to cache password on startup if not cached already: if [ -z \"$SSH_AUTH_SOCK\" ]; then eval \"$(ssh-agent -s)\" > /dev/null fi # Function to check if the key is already added check_key_added() { ssh-add -l | grep -q \"$(ssh-keygen -lf ~/.ssh/id_ed25519 | awk '{print $2}')\" } # Add the key if it's not already added if ! check_key_added; then echo \"Add ssh key for 8 hours\" ssh-add -t 28800 ~/.ssh/id_ed25519 # 8 hours timeout fi","title":"handle identity with password"},{"location":"linux%20%28debian%20based%29/network_ssh/#trust-ssh-key-on-remote-server","text":"# your user: copy your public key to the authorized_keys file in your home directory on a remote machine # allows access with your private key instead of a password ssh-copy-id -i ~/.ssh/id_rsa user@host # other user: copy your public key to the authorized_keys file in another home direcotry cat ~/id_rsa.pub | ssh your_user@remote.server.com \u201csudo tee -a /USER/.ssh/authorized_keys\u201d","title":"trust ssh key on remote server"},{"location":"linux%20%28debian%20based%29/network_ssh/#location-of-known_hosts","text":"Known hosts are saved in following locations - as entry in /etc/ssh/ssh_known_hosts - as entry in /etc/ssh/ssh_config - as *.pub file in /etc/ssh/ folder","title":"location of known_hosts"},{"location":"linux%20%28debian%20based%29/network_ssh/#add-and-remove-entries-from-known_hosts","text":"```shell","title":"add and remove entries from known_hosts"},{"location":"linux%20%28debian%20based%29/network_ssh/#remove-old-entry-from-known_host","text":"ssh-keygen -R SERVER_NAME -f ~/.ssh/known_hosts","title":"remove old entry from known_host"},{"location":"linux%20%28debian%20based%29/network_ssh/#add-new-key-to-known_hosts","text":"ssh-keyscan SERVER_NAME >> ~/.ssh/known_hosts","title":"add new key to known_hosts"},{"location":"linux%20%28debian%20based%29/network_utility_scripts/","text":"Network shell snipptes generate a certificate openssl req -x509 -newkey rsa:4096 -keyout ./key.pem -out ./cert.pem -days 365 check firewall #!/bin/bash servers=( \u201ebeispiel\u201c ) echo \"Script startetd from: $(hostname -f)\" for server in \"${servers[@]}\" do target_url=https://echo.example.de echo -n \"Try to call $target_url from $server. \" command=\"curl --max-time 5 -w 'HTTP Code %{http_code}\\n' -o /dev/null --silent $target_url\" ssh \"$server\" \"$command\" done remove old ssh key of host for multiple users #!/bin/bash echo \"Current user: $(whoami)\" HOST=\"myhost.example.de\" USERS=( \"user1\" \"user2\" ) echo \"Reset $HOST SSH Hostkey\" for USER in \"${USERS[@]}\" do echo \"Reset SSH Hostkey for user $USER\" HOME_DIR=\"/home1/users/$USER\" echo \"Remove $HOST from ${HOME_DIR}/.ssh/known_hosts\" sudo ssh-keygen -R \"$HOST\" -f ${HOME_DIR}/.ssh/known_hosts echo \"Add $HOST to ${HOME_DIR}/.ssh/known_hosts\" sudo ssh-keyscan \"$HOST\" | sudo tee --append ${HOME_DIR}/.ssh/known_hosts done","title":"Network shell snipptes"},{"location":"linux%20%28debian%20based%29/network_utility_scripts/#network-shell-snipptes","text":"","title":"Network shell snipptes"},{"location":"linux%20%28debian%20based%29/network_utility_scripts/#generate-a-certificate","text":"openssl req -x509 -newkey rsa:4096 -keyout ./key.pem -out ./cert.pem -days 365","title":"generate a certificate"},{"location":"linux%20%28debian%20based%29/network_utility_scripts/#check-firewall","text":"#!/bin/bash servers=( \u201ebeispiel\u201c ) echo \"Script startetd from: $(hostname -f)\" for server in \"${servers[@]}\" do target_url=https://echo.example.de echo -n \"Try to call $target_url from $server. \" command=\"curl --max-time 5 -w 'HTTP Code %{http_code}\\n' -o /dev/null --silent $target_url\" ssh \"$server\" \"$command\" done","title":"check firewall"},{"location":"linux%20%28debian%20based%29/network_utility_scripts/#remove-old-ssh-key-of-host-for-multiple-users","text":"#!/bin/bash echo \"Current user: $(whoami)\" HOST=\"myhost.example.de\" USERS=( \"user1\" \"user2\" ) echo \"Reset $HOST SSH Hostkey\" for USER in \"${USERS[@]}\" do echo \"Reset SSH Hostkey for user $USER\" HOME_DIR=\"/home1/users/$USER\" echo \"Remove $HOST from ${HOME_DIR}/.ssh/known_hosts\" sudo ssh-keygen -R \"$HOST\" -f ${HOME_DIR}/.ssh/known_hosts echo \"Add $HOST to ${HOME_DIR}/.ssh/known_hosts\" sudo ssh-keyscan \"$HOST\" | sudo tee --append ${HOME_DIR}/.ssh/known_hosts done","title":"remove old ssh key of host for multiple users"},{"location":"linux%20%28debian%20based%29/proxy/","text":"Proxy Windows In order to be used by cmd as well as the browsers you need to set the environmental variables: HTTP_PROXY HTTPS_PROXY NO_PROXY Set it via the environmental variables settings ui or via cmd with setx. # show current proxies netsh winhttp show proxy # set env vars setx HTTP_PROXY myproxy.de setx HTTPS_PROXY myproxy.de setx NO_PROXY \".mycompanydomain.de,localhost,127.0.0.*\" Linux export HTTP_PROXY=myproxy.de export HTTPS_PROXY=myproxy.de export NO_PROXY=\".mycompanydomain.de,localhost,127.0.0.*\" Proxy with authentification","title":"Proxy"},{"location":"linux%20%28debian%20based%29/proxy/#proxy","text":"","title":"Proxy"},{"location":"linux%20%28debian%20based%29/proxy/#windows","text":"In order to be used by cmd as well as the browsers you need to set the environmental variables: HTTP_PROXY HTTPS_PROXY NO_PROXY Set it via the environmental variables settings ui or via cmd with setx. # show current proxies netsh winhttp show proxy # set env vars setx HTTP_PROXY myproxy.de setx HTTPS_PROXY myproxy.de setx NO_PROXY \".mycompanydomain.de,localhost,127.0.0.*\"","title":"Windows"},{"location":"linux%20%28debian%20based%29/proxy/#linux","text":"export HTTP_PROXY=myproxy.de export HTTPS_PROXY=myproxy.de export NO_PROXY=\".mycompanydomain.de,localhost,127.0.0.*\"","title":"Linux"},{"location":"linux%20%28debian%20based%29/proxy/#proxy-with-authentification","text":"","title":"Proxy with authentification"},{"location":"linux%20%28debian%20based%29/python/","text":"Python install using pyenv # install and configure path brew install pyenv echo \"$(pyenv init --path)\" >> ~/.zshrc # list installable versions pyenv install --list # install pyenv install 3.9.16 pyenv install 2.7.18 # list installed versions pyenv versions # set path for python executables pyenv global 3.9.16 2.7.18 # check versions python2 --version python3 --version python --version create minimal webserver with python # serve current dir on port 8000 python3 -m http.server # serve current dir on custom port python3 -m http.server 9999 # serve custom dir on custom port python3 -m http.server 8001 --directory /opt/oh/my/content set python 3 as default The last number is the priority. Greater number is higher priority. sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 2 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 1 If you want to check the current config run sudo update-alternatives --config python install pip for python3 sudo apt install python3-pip","title":"Python"},{"location":"linux%20%28debian%20based%29/python/#python","text":"","title":"Python"},{"location":"linux%20%28debian%20based%29/python/#install-using-pyenv","text":"# install and configure path brew install pyenv echo \"$(pyenv init --path)\" >> ~/.zshrc # list installable versions pyenv install --list # install pyenv install 3.9.16 pyenv install 2.7.18 # list installed versions pyenv versions # set path for python executables pyenv global 3.9.16 2.7.18 # check versions python2 --version python3 --version python --version","title":"install using pyenv"},{"location":"linux%20%28debian%20based%29/python/#create-minimal-webserver-with-python","text":"# serve current dir on port 8000 python3 -m http.server # serve current dir on custom port python3 -m http.server 9999 # serve custom dir on custom port python3 -m http.server 8001 --directory /opt/oh/my/content","title":"create minimal webserver with python"},{"location":"linux%20%28debian%20based%29/python/#set-python-3-as-default","text":"The last number is the priority. Greater number is higher priority. sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 2 sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 1 If you want to check the current config run sudo update-alternatives --config python","title":"set python 3 as default"},{"location":"linux%20%28debian%20based%29/python/#install-pip-for-python3","text":"sudo apt install python3-pip","title":"install pip for python3"},{"location":"linux%20%28debian%20based%29/sar/","text":"Linux performance measurement minial memory consuption while true do current_time=$(date \"+%Y%m%d-%H:%M:%S\") memory_usage=$(free -m | awk 'NR==2{printf \"%.2f%%\\t\\t\", $3*100/$2}') echo \"${current_time} ${memory_usage}\" | tee -a ~/memory_usage.txt sleep 5 done sar # sar ## show cpu for whole day sar ## cpu: every second for the next 5 intervals/seconds sar -u 1 5 ## ram ### %commit - memory usage (inclduing swap) ### %memusage memory including virtual memory sar -r 1 5 ## disk ### %util - approximate usage of disks. Not reliable for ssds or raid systems sar -d 1 5 ## network sar -n ALL 1 5 ## combined sar -urb 1 5 sar -urbn ALL 1 5 # save in file sar -urb 1 5 -o ./sarout.file # save to file and run in backround sar -urb 1 5 -o ./sarout.file >/dev/null 2>&1 & # sadf ## csv # sadf -dh -- <sar command> sadf -dh -- -urb ## json # sadf -j -- <sar command> sadf -j -- -urb ## create sar file and read it into csv sar -urb 1 5 -o ./sarout.file sadf -dh sarout.file > sarout.csv ## create sar file and read it into csv in backround sar -urb 1 5 -o ./sarout.file >/dev/null 2>&1 & sadf -dh sarout.file > sarout.csv &","title":"Linux performance measurement"},{"location":"linux%20%28debian%20based%29/sar/#linux-performance-measurement","text":"","title":"Linux performance measurement"},{"location":"linux%20%28debian%20based%29/sar/#minial-memory-consuption","text":"while true do current_time=$(date \"+%Y%m%d-%H:%M:%S\") memory_usage=$(free -m | awk 'NR==2{printf \"%.2f%%\\t\\t\", $3*100/$2}') echo \"${current_time} ${memory_usage}\" | tee -a ~/memory_usage.txt sleep 5 done","title":"minial memory consuption"},{"location":"linux%20%28debian%20based%29/sar/#sar","text":"# sar ## show cpu for whole day sar ## cpu: every second for the next 5 intervals/seconds sar -u 1 5 ## ram ### %commit - memory usage (inclduing swap) ### %memusage memory including virtual memory sar -r 1 5 ## disk ### %util - approximate usage of disks. Not reliable for ssds or raid systems sar -d 1 5 ## network sar -n ALL 1 5 ## combined sar -urb 1 5 sar -urbn ALL 1 5 # save in file sar -urb 1 5 -o ./sarout.file # save to file and run in backround sar -urb 1 5 -o ./sarout.file >/dev/null 2>&1 & # sadf ## csv # sadf -dh -- <sar command> sadf -dh -- -urb ## json # sadf -j -- <sar command> sadf -j -- -urb ## create sar file and read it into csv sar -urb 1 5 -o ./sarout.file sadf -dh sarout.file > sarout.csv ## create sar file and read it into csv in backround sar -urb 1 5 -o ./sarout.file >/dev/null 2>&1 & sadf -dh sarout.file > sarout.csv &","title":"sar"},{"location":"linux%20%28debian%20based%29/screen/","text":"screen # create screen screen -S bla # leave/detach screen # press: STRG+A D # or when the screen is running elsewhere: screen -d bla # open screen screen -x bla # execute command in a new screen with the name blubb #TODO execute command in a new screen, keep it open when the process is ended screen -S blubb -dm echo \"hello\"; exec bash # kill screen screen -S blubb -X quit # or connect to it and # press STRG+a and type :quit #TODO connect screen to running process","title":"screen"},{"location":"linux%20%28debian%20based%29/screen/#screen","text":"# create screen screen -S bla # leave/detach screen # press: STRG+A D # or when the screen is running elsewhere: screen -d bla # open screen screen -x bla # execute command in a new screen with the name blubb #TODO execute command in a new screen, keep it open when the process is ended screen -S blubb -dm echo \"hello\"; exec bash # kill screen screen -S blubb -X quit # or connect to it and # press STRG+a and type :quit #TODO connect screen to running process","title":"screen"},{"location":"linux%20%28debian%20based%29/set-path/","text":"set path variable Source: https://stackabuse.com/how-to-permanently-set-path-in-linux/ alternativ move executables on existing path e.g. /usr/local/bin decision variable needed system wide : edit /etc/profile variable needed user only : edit ~/profile variables temporary for terminal session: run export PATH=\"$PATH:<path to new binary>\" how to edit The following procedure is ok for all files in the table, except for the /etc/enviroment file. export is not needed necessarily. It just defines the variable for subprocesses as well. Add Path with: export PATH=\"$PATH:<path to new binary>:<..another path>\" for example: export PATH=\"$PATH:/home/tomahawk/tools/jdk1.8.0_92/bin\" and reload the file with the source command afterwards. For example like this: source ~/.profile Exception is /etc/enviroment file which does not get executed. Here you need to add the path manually into the string. overview","title":"set path variable"},{"location":"linux%20%28debian%20based%29/set-path/#set-path-variable","text":"Source: https://stackabuse.com/how-to-permanently-set-path-in-linux/","title":"set path variable"},{"location":"linux%20%28debian%20based%29/set-path/#alternativ","text":"move executables on existing path e.g. /usr/local/bin","title":"alternativ"},{"location":"linux%20%28debian%20based%29/set-path/#decision","text":"variable needed system wide : edit /etc/profile variable needed user only : edit ~/profile variables temporary for terminal session: run export PATH=\"$PATH:<path to new binary>\"","title":"decision"},{"location":"linux%20%28debian%20based%29/set-path/#how-to-edit","text":"The following procedure is ok for all files in the table, except for the /etc/enviroment file. export is not needed necessarily. It just defines the variable for subprocesses as well. Add Path with: export PATH=\"$PATH:<path to new binary>:<..another path>\" for example: export PATH=\"$PATH:/home/tomahawk/tools/jdk1.8.0_92/bin\" and reload the file with the source command afterwards. For example like this: source ~/.profile Exception is /etc/enviroment file which does not get executed. Here you need to add the path manually into the string.","title":"how to edit"},{"location":"linux%20%28debian%20based%29/set-path/#overview","text":"","title":"overview"},{"location":"linux%20%28debian%20based%29/shell_script/","text":"shell script explainshell.com general To use a variable: ${VARIABLE}, $VARIABLE By convention variables names use uppercase snake case (EXAMPLE_VARIABLE). You should not use dash ('-') because it leads to errors when executing the script. quotes: single quotes ' mark literal values. ( echo '$path' #print $path ) In double quotes \" varriables will be replaced. ( echo \"$path\" # prints the content of the path variable ) To print commands executed in a shell script add set -o xtrace to the script or run the script with bash -x <scriptname.sh> . Really helpful for debugging. to save the return value of a command suraound the command in a dollar and braces e.g result=$(l -al) if you want to print a variable add quotes around it: echo \"$VARIABLE\" you can access the command-arguments/positional-parameter of a script by (incomplete list) $1 , $2 , ... ( $0 is the script name) $argv[1] , $argv[2] \"$@\" array of all positional parameters, {$1, $2, $3 ...}. $# number of positional parameters. $$ pid of the current shell (not subshell). $0 name of the shell or shell script. handy set relative paths Sometimes it should be irrelevant from where you start you script. If the script uses relative references to files you first need to find out in which directory the script is saved and add this directory as a prefix to every relative file reference (which makes it an absolute path). Otherwise the working directory of the shell is used as reference. #!/bin/bash # to debug script set -o xtrace BASEDIR=\"$(dirname \"$0\")\" Filter lists and pipe input # delete pods which contains example in name kubectl get pod | grep \"example\" | awk '{print $1}' | xargs kubectl delete pod JSON parsing parse a field from a json via grep, awk oder sed grep -Po '\"${JSON_FIELD_NAME}\": *\"\\K[^\"]*' ${JSON_FILE_PATH} or better yet use jq curl To send a json file via curl you need to set content type: -H 'Content-Type:application/json' add the file -d @RELATIVE/OR/FULLPATH/example.json or just a string variable -d @\"${CONTENT} \". Under windows you need to escape the double quotes when using a string variable. complete exmaple: curl -X POST -H 'Content-Type:application/json' -d @RELATIVE/OR/FULLPATH/example.json $URL to show error but no progress bar run curl: curl -sS http:// . -s for silent mode (suppress progress bar and error) and -S for explicitly showing error. to download stuff follow redirects and set output name: curl -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -o /usr/bin/jq Check firewall: curl --connect-timeout 10 --show-error bla.de . Connection error 28 indicates that a firewall blocks the call string replacement to replace the first occurrence of a pattern in a string use: \"${stringVarible/patternValue/$replaceValueVariable}\" to replace all occurrences of a pattern in a string use: \"${stringVarible//patternValue/$replaceValueVariable}\"","title":"shell script"},{"location":"linux%20%28debian%20based%29/shell_script/#shell-script","text":"explainshell.com","title":"shell script"},{"location":"linux%20%28debian%20based%29/shell_script/#general","text":"To use a variable: ${VARIABLE}, $VARIABLE By convention variables names use uppercase snake case (EXAMPLE_VARIABLE). You should not use dash ('-') because it leads to errors when executing the script. quotes: single quotes ' mark literal values. ( echo '$path' #print $path ) In double quotes \" varriables will be replaced. ( echo \"$path\" # prints the content of the path variable ) To print commands executed in a shell script add set -o xtrace to the script or run the script with bash -x <scriptname.sh> . Really helpful for debugging. to save the return value of a command suraound the command in a dollar and braces e.g result=$(l -al) if you want to print a variable add quotes around it: echo \"$VARIABLE\" you can access the command-arguments/positional-parameter of a script by (incomplete list) $1 , $2 , ... ( $0 is the script name) $argv[1] , $argv[2] \"$@\" array of all positional parameters, {$1, $2, $3 ...}. $# number of positional parameters. $$ pid of the current shell (not subshell). $0 name of the shell or shell script.","title":"general"},{"location":"linux%20%28debian%20based%29/shell_script/#handy","text":"","title":"handy"},{"location":"linux%20%28debian%20based%29/shell_script/#set-relative-paths","text":"Sometimes it should be irrelevant from where you start you script. If the script uses relative references to files you first need to find out in which directory the script is saved and add this directory as a prefix to every relative file reference (which makes it an absolute path). Otherwise the working directory of the shell is used as reference. #!/bin/bash # to debug script set -o xtrace BASEDIR=\"$(dirname \"$0\")\"","title":"set relative paths"},{"location":"linux%20%28debian%20based%29/shell_script/#filter-lists-and-pipe-input","text":"# delete pods which contains example in name kubectl get pod | grep \"example\" | awk '{print $1}' | xargs kubectl delete pod","title":"Filter lists and pipe input"},{"location":"linux%20%28debian%20based%29/shell_script/#json-parsing","text":"parse a field from a json via grep, awk oder sed grep -Po '\"${JSON_FIELD_NAME}\": *\"\\K[^\"]*' ${JSON_FILE_PATH} or better yet use jq","title":"JSON parsing"},{"location":"linux%20%28debian%20based%29/shell_script/#curl","text":"To send a json file via curl you need to set content type: -H 'Content-Type:application/json' add the file -d @RELATIVE/OR/FULLPATH/example.json or just a string variable -d @\"${CONTENT} \". Under windows you need to escape the double quotes when using a string variable. complete exmaple: curl -X POST -H 'Content-Type:application/json' -d @RELATIVE/OR/FULLPATH/example.json $URL to show error but no progress bar run curl: curl -sS http:// . -s for silent mode (suppress progress bar and error) and -S for explicitly showing error. to download stuff follow redirects and set output name: curl -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 -o /usr/bin/jq Check firewall: curl --connect-timeout 10 --show-error bla.de . Connection error 28 indicates that a firewall blocks the call","title":"curl"},{"location":"linux%20%28debian%20based%29/shell_script/#string-replacement","text":"to replace the first occurrence of a pattern in a string use: \"${stringVarible/patternValue/$replaceValueVariable}\" to replace all occurrences of a pattern in a string use: \"${stringVarible//patternValue/$replaceValueVariable}\"","title":"string replacement"},{"location":"linux%20%28debian%20based%29/terminal_config/","text":"Terminal/Shell config alias # Linux version of OSX pbcopy and pbpaste. alias pbcopy=\u2019xsel \u2014 clipboard \u2014 input\u2019 alias pbpaste=\u2019xsel \u2014 clipboard \u2014 output\u2019 # copy last command to clipboard alias copyLastCmd='fc -ln -1 | awk '\\''{$1=$1}1'\\'' ORS='\\'''\\'' | pbcopy' Terminal config iterm2 Shell config Bash Zsh Fsh copy from terminal mac Use pbcopy and pbpaste. # Copy Hello\\n to clipboard echo \"Hello\" | pbcopy # copy last command to clipboard alias copyLastCmd='fc -ln -1 | awk '\\''{$1=$1}1'\\'' ORS='\\'''\\'' | pbcopy' copyLastCmd linux Use pbcopy and pbpaste. # Linux version of OSX pbcopy and pbpaste. alias pbcopy=\u2019xsel \u2014 clipboard \u2014 input\u2019 alias pbpaste=\u2019xsel \u2014 clipboard \u2014 output\u2019 # Copy Hello\\n to clipboard echo \"Hello\" | pbcopy # copy last command to clipboard alias copyLastCmd='fc -ln -1 | awk '\\''{$1=$1}1'\\'' ORS='\\'''\\'' | pbcopy' copyLastCmd Sources StackExchange","title":"Terminal/Shell config"},{"location":"linux%20%28debian%20based%29/terminal_config/#terminalshell-config","text":"","title":"Terminal/Shell config"},{"location":"linux%20%28debian%20based%29/terminal_config/#alias","text":"# Linux version of OSX pbcopy and pbpaste. alias pbcopy=\u2019xsel \u2014 clipboard \u2014 input\u2019 alias pbpaste=\u2019xsel \u2014 clipboard \u2014 output\u2019 # copy last command to clipboard alias copyLastCmd='fc -ln -1 | awk '\\''{$1=$1}1'\\'' ORS='\\'''\\'' | pbcopy'","title":"alias"},{"location":"linux%20%28debian%20based%29/terminal_config/#terminal-config","text":"","title":"Terminal config"},{"location":"linux%20%28debian%20based%29/terminal_config/#iterm2","text":"","title":"iterm2"},{"location":"linux%20%28debian%20based%29/terminal_config/#shell-config","text":"","title":"Shell config"},{"location":"linux%20%28debian%20based%29/terminal_config/#bash","text":"","title":"Bash"},{"location":"linux%20%28debian%20based%29/terminal_config/#zsh","text":"","title":"Zsh"},{"location":"linux%20%28debian%20based%29/terminal_config/#fsh","text":"","title":"Fsh"},{"location":"linux%20%28debian%20based%29/terminal_config/#copy-from-terminal","text":"","title":"copy from terminal"},{"location":"linux%20%28debian%20based%29/terminal_config/#mac","text":"Use pbcopy and pbpaste. # Copy Hello\\n to clipboard echo \"Hello\" | pbcopy # copy last command to clipboard alias copyLastCmd='fc -ln -1 | awk '\\''{$1=$1}1'\\'' ORS='\\'''\\'' | pbcopy' copyLastCmd","title":"mac"},{"location":"linux%20%28debian%20based%29/terminal_config/#linux","text":"Use pbcopy and pbpaste. # Linux version of OSX pbcopy and pbpaste. alias pbcopy=\u2019xsel \u2014 clipboard \u2014 input\u2019 alias pbpaste=\u2019xsel \u2014 clipboard \u2014 output\u2019 # Copy Hello\\n to clipboard echo \"Hello\" | pbcopy # copy last command to clipboard alias copyLastCmd='fc -ln -1 | awk '\\''{$1=$1}1'\\'' ORS='\\'''\\'' | pbcopy' copyLastCmd","title":"linux"},{"location":"linux%20%28debian%20based%29/terminal_config/#sources","text":"StackExchange","title":"Sources"},{"location":"linux%20%28debian%20based%29/deepin/beep/","text":"Disable hardware beep completely # temporarly until restart sudo rmmod pcspkr # to disable loading of moduke at startup echo \"blacklist pcspkr\" >> /etc/modprobe.d/blacklist firefox only - Open about:config - Set accessibility.typeaheadfind.enablesound` to false","title":"Disable hardware beep"},{"location":"linux%20%28debian%20based%29/deepin/beep/#disable-hardware-beep","text":"","title":"Disable hardware beep"},{"location":"linux%20%28debian%20based%29/deepin/beep/#completely","text":"# temporarly until restart sudo rmmod pcspkr # to disable loading of moduke at startup echo \"blacklist pcspkr\" >> /etc/modprobe.d/blacklist","title":"completely"},{"location":"linux%20%28debian%20based%29/deepin/beep/#firefox-only","text":"- Open about:config - Set accessibility.typeaheadfind.enablesound` to false","title":"firefox only"},{"location":"linux%20%28debian%20based%29/deepin/file-watchers/","text":"not enough files watchers Jetbrains and VS Code complain about problems with the file watchers on Deepin. Info from jetbrains To solve this issue do: sudo nano /etc/sysctl.conf # and add: fs.inotify.max_user_watches = 524288 # to load it afterwards. sudo sysctl -p --system","title":"not enough files watchers"},{"location":"linux%20%28debian%20based%29/deepin/file-watchers/#not-enough-files-watchers","text":"Jetbrains and VS Code complain about problems with the file watchers on Deepin. Info from jetbrains To solve this issue do: sudo nano /etc/sysctl.conf # and add: fs.inotify.max_user_watches = 524288 # to load it afterwards. sudo sysctl -p --system","title":"not enough files watchers"},{"location":"linux%20%28debian%20based%29/deepin/remove-firefox-titlebar/","text":"remove titlebar form firefox There is a bug in deepin that the titlebar in firefox wont disappear even if it is turned off. It is described in this issue . To fix this it is sufficient to create a modified .desktop file for firefox, which activates client side decoration : Exec=env MOZ_GTK_TITLEBAR_DECORATION=client /opt/firefox/firefox %u [Desktop Entry] Comment=Browse the World Wide Web GenericName=Web Browser X-GNOME-FullName=Firefox Web Browser Exec=env MOZ_GTK_TITLEBAR_DECORATION=client /opt/firefox/firefox %u Terminal=false X-MultipleArgs=false Type=Application Icon=/opt/firefox/browser/chrome/icons/default/default128.png Categories=Network;WebBrowser; MimeType=text/html;text/xml;application/xhtml+xml;application/xml;application/vnd.mozilla.xul+xml;application/rss+xml;application/rdf+xml;image/gif;image/jpeg;image/png;x-scheme-handler/http;x-scheme-handler/https; StartupWMClass=Firefox StartupNotify=true Reason is Client-side decoration .","title":"remove titlebar form firefox"},{"location":"linux%20%28debian%20based%29/deepin/remove-firefox-titlebar/#remove-titlebar-form-firefox","text":"There is a bug in deepin that the titlebar in firefox wont disappear even if it is turned off. It is described in this issue . To fix this it is sufficient to create a modified .desktop file for firefox, which activates client side decoration : Exec=env MOZ_GTK_TITLEBAR_DECORATION=client /opt/firefox/firefox %u [Desktop Entry] Comment=Browse the World Wide Web GenericName=Web Browser X-GNOME-FullName=Firefox Web Browser Exec=env MOZ_GTK_TITLEBAR_DECORATION=client /opt/firefox/firefox %u Terminal=false X-MultipleArgs=false Type=Application Icon=/opt/firefox/browser/chrome/icons/default/default128.png Categories=Network;WebBrowser; MimeType=text/html;text/xml;application/xhtml+xml;application/xml;application/vnd.mozilla.xul+xml;application/rss+xml;application/rdf+xml;image/gif;image/jpeg;image/png;x-scheme-handler/http;x-scheme-handler/https; StartupWMClass=Firefox StartupNotify=true Reason is Client-side decoration .","title":"remove titlebar form firefox"},{"location":"linux%20%28debian%20based%29/deepin/remove-vscode-titlebar/","text":"remove titlebar form vscode To remove the titlebar in vscode: go to settings set window.titleBarStyle to custom Reason is Client-side decoration .","title":"remove titlebar form vscode"},{"location":"linux%20%28debian%20based%29/deepin/remove-vscode-titlebar/#remove-titlebar-form-vscode","text":"To remove the titlebar in vscode: go to settings set window.titleBarStyle to custom Reason is Client-side decoration .","title":"remove titlebar form vscode"},{"location":"linux%20%28debian%20based%29/deepin/remove_ulauncher_black_frame/","text":"remove black frame from ulauncher black Frame Open Issue If window effect is disabled there is the chance that there is a black frame around the Ulauncher window. Probably because the compositor ist somehow affected or disabled by the disabling of the window effect in deepin. In [https://github.com/Ulauncher/Ulauncher/issues/212] someone proposed a workaround which works: add \u2018margin: -20px;\u2019 to the css of a theme. Documentation on how to create an own Theme Extended white theme: .app { box-shadow: 0 0 5px @window_shadow; background-color: @window_bg; border: 1px solid @window_border_color; border-radius: 4px; margin: -20px; /*override black area when window effect is disabled*/ } Extended dark theme: .app { background-color: @window_bg; border-color: @window_border_color; margin: -20px; /*override black area when window effect is disabled*/ }","title":"remove black frame from ulauncher"},{"location":"linux%20%28debian%20based%29/deepin/remove_ulauncher_black_frame/#remove-black-frame-from-ulauncher","text":"","title":"remove black frame from ulauncher"},{"location":"linux%20%28debian%20based%29/deepin/remove_ulauncher_black_frame/#black-frame","text":"Open Issue If window effect is disabled there is the chance that there is a black frame around the Ulauncher window. Probably because the compositor ist somehow affected or disabled by the disabling of the window effect in deepin. In [https://github.com/Ulauncher/Ulauncher/issues/212] someone proposed a workaround which works: add \u2018margin: -20px;\u2019 to the css of a theme. Documentation on how to create an own Theme Extended white theme: .app { box-shadow: 0 0 5px @window_shadow; background-color: @window_bg; border: 1px solid @window_border_color; border-radius: 4px; margin: -20px; /*override black area when window effect is disabled*/ } Extended dark theme: .app { background-color: @window_bg; border-color: @window_border_color; margin: -20px; /*override black area when window effect is disabled*/ }","title":"black Frame"},{"location":"my-server/drone_ci/","text":"setup drone ci pitfalls drone needs a public ip or domain name so the git repository webhooks are working DRONE_SERVER_PROTO and DRONE_SERVER_HOST are used for the creation of the webhook so the external protocol and public ip/ domain name are needed you may want to set DRONE_LOGS_DEBUG=true and DRONE_LOGS_PRETTY=true to true until your server is completly running. troubleshooting build does not start troubleshooting build stuck in pending troubleshooting other stuff discourse forum . install Follow the official install guide . The following code snippets might be helpful for copypasta, but you might want to read the offical install guide before. If the server is running but the builds do not get triggered check this trouble shooting guide . For asking questions or searching for one use the discourse forum . # 1. generate DRONE_RPC_SECRET with: openssl rand -hex 16 # 2. create a bridge network so the contains can address each other with their name docker network create drone # 3. start drone master docker run \\ --volume=/var/lib/drone:/data \\ --env=DRONE_GITHUB_CLIENT_ID=<FROM_GITHUB> \\ --env=DRONE_GITHUB_CLIENT_SECRET=<FROM_GITHUB_SECRET> \\ --env=DRONE_RPC_SECRET=<GENERATED_SECRET_FROM_STEP_1> \\ --env=DRONE_SERVER_HOST=ci.weyrich.dev \\ --env=DRONE_SERVER_PROTO=https \\ --env=DRONE_USER_FILTER=crowdsalat \\ --env=DRONE_USER_CREATE=username:crowdsalat,admin:true \\ --env=DRONE_LOGS_PRETTY=true \\ --env=DRONE_LOGS_COLOR=true \\ --env=DRONE_LOGS_DEBUG=true \\ --publish=180:80 \\ --restart=always \\ --detach=true \\ --name=drone \\ --network drone \\ drone/drone:1 # 4. start drone docker runner: docker run -d \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e DRONE_RPC_PROTO=HTTP \\ -e DRONE_RPC_HOST=drone:80 \\ -e DRONE_RPC_SECRET=<GENERATED_SECRET_FROM_STEP_1> \\ -e DRONE_RUNNER_CAPACITY=2 \\ -e DRONE_RUNNER_NAME=drone_docker_runner \\ -e DRONE_LOGS_DEBUG=true \\ -p 3000:3000 \\ --restart always \\ --name drone_runner \\ --network drone \\ drone/drone-runner-docker:1 start containers on host it is important that the drone docker runner is started with the -v /var/run/docker.sock:/var/run/docker.sock argument. Every pipeline step starts a new container. Because of the mount of the docker.sock the stated container will run on the host not inside the docker container of the runner. If you want to build or start containers on the docker host you need to add a host volume which points to var/run/docker.sock on the host and reference it in the pipline steps where docker is used. cache stuff To cache downloaded libraries between build you can create a host volume and use map it in the build step to the directory e.g. /root/.m2/repository. Note that a admin user needs to mark the repository in which the pipelines runs to be 'trusted'. variable resolution default environmental variables You can us variables as following: ${VARIABLE_NAME} noteworthy stuff by convention drone build file is called .drone.yml by convention drone resources are saved in .drone folder parallel pipelines are separated by a document separator --- in the .drone.yml dependent pipelines can be defined with the depends_on directive steps in a pipeline share a (build-) temporal volume which is called workspace and allows to share state between the steps eventthought they are running in different containers. In order to use the drone cli you must configure the drone server address and a admin user access token. For convenience you can save two environment variables which are described when you run drone without parameters.","title":"setup drone ci"},{"location":"my-server/drone_ci/#setup-drone-ci","text":"","title":"setup drone ci"},{"location":"my-server/drone_ci/#pitfalls","text":"drone needs a public ip or domain name so the git repository webhooks are working DRONE_SERVER_PROTO and DRONE_SERVER_HOST are used for the creation of the webhook so the external protocol and public ip/ domain name are needed you may want to set DRONE_LOGS_DEBUG=true and DRONE_LOGS_PRETTY=true to true until your server is completly running.","title":"pitfalls"},{"location":"my-server/drone_ci/#troubleshooting","text":"build does not start troubleshooting build stuck in pending troubleshooting other stuff discourse forum .","title":"troubleshooting"},{"location":"my-server/drone_ci/#install","text":"Follow the official install guide . The following code snippets might be helpful for copypasta, but you might want to read the offical install guide before. If the server is running but the builds do not get triggered check this trouble shooting guide . For asking questions or searching for one use the discourse forum . # 1. generate DRONE_RPC_SECRET with: openssl rand -hex 16 # 2. create a bridge network so the contains can address each other with their name docker network create drone # 3. start drone master docker run \\ --volume=/var/lib/drone:/data \\ --env=DRONE_GITHUB_CLIENT_ID=<FROM_GITHUB> \\ --env=DRONE_GITHUB_CLIENT_SECRET=<FROM_GITHUB_SECRET> \\ --env=DRONE_RPC_SECRET=<GENERATED_SECRET_FROM_STEP_1> \\ --env=DRONE_SERVER_HOST=ci.weyrich.dev \\ --env=DRONE_SERVER_PROTO=https \\ --env=DRONE_USER_FILTER=crowdsalat \\ --env=DRONE_USER_CREATE=username:crowdsalat,admin:true \\ --env=DRONE_LOGS_PRETTY=true \\ --env=DRONE_LOGS_COLOR=true \\ --env=DRONE_LOGS_DEBUG=true \\ --publish=180:80 \\ --restart=always \\ --detach=true \\ --name=drone \\ --network drone \\ drone/drone:1 # 4. start drone docker runner: docker run -d \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -e DRONE_RPC_PROTO=HTTP \\ -e DRONE_RPC_HOST=drone:80 \\ -e DRONE_RPC_SECRET=<GENERATED_SECRET_FROM_STEP_1> \\ -e DRONE_RUNNER_CAPACITY=2 \\ -e DRONE_RUNNER_NAME=drone_docker_runner \\ -e DRONE_LOGS_DEBUG=true \\ -p 3000:3000 \\ --restart always \\ --name drone_runner \\ --network drone \\ drone/drone-runner-docker:1","title":"install"},{"location":"my-server/drone_ci/#start-containers-on-host","text":"it is important that the drone docker runner is started with the -v /var/run/docker.sock:/var/run/docker.sock argument. Every pipeline step starts a new container. Because of the mount of the docker.sock the stated container will run on the host not inside the docker container of the runner. If you want to build or start containers on the docker host you need to add a host volume which points to var/run/docker.sock on the host and reference it in the pipline steps where docker is used.","title":"start containers on host"},{"location":"my-server/drone_ci/#cache-stuff","text":"To cache downloaded libraries between build you can create a host volume and use map it in the build step to the directory e.g. /root/.m2/repository. Note that a admin user needs to mark the repository in which the pipelines runs to be 'trusted'.","title":"cache stuff"},{"location":"my-server/drone_ci/#variable-resolution","text":"default environmental variables You can us variables as following: ${VARIABLE_NAME}","title":"variable resolution"},{"location":"my-server/drone_ci/#noteworthy-stuff","text":"by convention drone build file is called .drone.yml by convention drone resources are saved in .drone folder parallel pipelines are separated by a document separator --- in the .drone.yml dependent pipelines can be defined with the depends_on directive steps in a pipeline share a (build-) temporal volume which is called workspace and allows to share state between the steps eventthought they are running in different containers. In order to use the drone cli you must configure the drone server address and a admin user access token. For convenience you can save two environment variables which are described when you run drone without parameters.","title":"noteworthy stuff"},{"location":"my-server/server-infra/","text":"my server setup setup public infrastructure like vls, domain name and dns records setup server infrastructure like letsencrypt certificates, docker, kubernetes, nginx reverse proxy setup applications like ci pipeline setup public infrastructure get virtual linux server (vls) with public ip address reserve domain name setup dns record to point domain name to vls ip concrete steps create cx11 vls in hetzner cloud buy weyrich.dev domain at namecheap (14 \u20ac/year) signup at cloudflare and create a dns record. Cloudflare is used because it is free and it supports multiple letsencrypt renewal clients . set the two cloudflare dns server addresses to be used by namecheap Here is a how-to create A and AAAA entries in cloudflare which map the domain name to the vls ipv4 and ipv6 address (optional) create CNAME entry for every subdomain you want to use. cost vls: 35.52 \u20ac/year domain name: 14 \u20ac/year dns record: free wildcard certificate: free your dashboards hetzner cloud console namecheap dashboard cloudflare dashboard server infrastructure get wildcard certificate for domain and its subdomains with letsencrypt get letsencrypt wildcard certificate In order to use letsencrypt you need a acme compliant client. Certbot is the recommended one. So setup certbot on your vls with dns validation (not http validation see the differences ) so you can create a wildcard vertificate for your domain. You can either install the cerbot on you host system or you use docker to start it. setup certbot via install # install certbot sudo apt-get install certbot python-certbot-nginx # install plugin for the dns provider cloudflare sudo apt-get install python3-certbot-dns-cloudflare # setup cloudflare credentials for certbot - https://certbot-dns-cloudflare.readthedocs.io/en/stable/ ## create a api token in cloudflare dashboard with the: ### Zone:Zone:Read and Zone:DNS:Edit permissions for all zones ## save api your api key, mail address and api token in a file touch ~/.certbot/cloudflare.ini ##add to the file: #dns_cloudflare_email=<mail address> #dns_cloudflare_api_key=<api key> #dns_cloudflare_api_token=<created api token> ## acquire wildcard certificate for weyrich.dev, save the certs under /etc/letsencrypt/live/weyrich.dev/ and start a nginx on port certbot \\ --dns-cloudflare \\ --dns-cloudflare-credentials ~/.certbot/cloudflare.ini \\ -i nginx \\ -d *.weyrich.dev # test automatic renewal sudo certbot renew --dry-run # check your domain with ssllab: https://www.ssllabs.com/ssltest/ # reload config sudo nginx -s reload # renew certificate certbot renew\\ --dns-cloudflare \\ --dns-cloudflare-credentials ~/.certbot/cloudflare.ini \\ -d *.weyrich.dev # generate certificate certbot certonly\\ --dns-cloudflare \\ --dns-cloudflare-credentials ~/.certbot/cloudflare.ini \\ -d *.weyrich.dev setup certbot via docker Blog entry which explains hot wo setup a letsencrypt client with docker. The linuxserver/letsencrypt docker image is used and configured to generate a wildcard certificate via dns validation. setup docker curl -sSL https://get.docker.com | sh setup nginx as reverse proxy Base config is created with this tool from digitalocean . Afterwards for every subdomain a proxy_pass is configured. See the page nginx memory aid for an overview of nginx config. And remember to create a CNAME in the dns for every subdomain. The browser will heavily use caching when serving static files so remember to delete the caches after changing proxy settings.","title":"my server setup"},{"location":"my-server/server-infra/#my-server-setup","text":"setup public infrastructure like vls, domain name and dns records setup server infrastructure like letsencrypt certificates, docker, kubernetes, nginx reverse proxy setup applications like ci pipeline","title":"my server setup"},{"location":"my-server/server-infra/#setup-public-infrastructure","text":"get virtual linux server (vls) with public ip address reserve domain name setup dns record to point domain name to vls ip","title":"setup public infrastructure"},{"location":"my-server/server-infra/#concrete-steps","text":"create cx11 vls in hetzner cloud buy weyrich.dev domain at namecheap (14 \u20ac/year) signup at cloudflare and create a dns record. Cloudflare is used because it is free and it supports multiple letsencrypt renewal clients . set the two cloudflare dns server addresses to be used by namecheap Here is a how-to create A and AAAA entries in cloudflare which map the domain name to the vls ipv4 and ipv6 address (optional) create CNAME entry for every subdomain you want to use.","title":"concrete steps"},{"location":"my-server/server-infra/#cost","text":"vls: 35.52 \u20ac/year domain name: 14 \u20ac/year dns record: free wildcard certificate: free","title":"cost"},{"location":"my-server/server-infra/#your-dashboards","text":"hetzner cloud console namecheap dashboard cloudflare dashboard","title":"your dashboards"},{"location":"my-server/server-infra/#server-infrastructure","text":"get wildcard certificate for domain and its subdomains with letsencrypt","title":"server infrastructure"},{"location":"my-server/server-infra/#get-letsencrypt-wildcard-certificate","text":"In order to use letsencrypt you need a acme compliant client. Certbot is the recommended one. So setup certbot on your vls with dns validation (not http validation see the differences ) so you can create a wildcard vertificate for your domain. You can either install the cerbot on you host system or you use docker to start it.","title":"get letsencrypt wildcard certificate"},{"location":"my-server/server-infra/#setup-certbot-via-install","text":"# install certbot sudo apt-get install certbot python-certbot-nginx # install plugin for the dns provider cloudflare sudo apt-get install python3-certbot-dns-cloudflare # setup cloudflare credentials for certbot - https://certbot-dns-cloudflare.readthedocs.io/en/stable/ ## create a api token in cloudflare dashboard with the: ### Zone:Zone:Read and Zone:DNS:Edit permissions for all zones ## save api your api key, mail address and api token in a file touch ~/.certbot/cloudflare.ini ##add to the file: #dns_cloudflare_email=<mail address> #dns_cloudflare_api_key=<api key> #dns_cloudflare_api_token=<created api token> ## acquire wildcard certificate for weyrich.dev, save the certs under /etc/letsencrypt/live/weyrich.dev/ and start a nginx on port certbot \\ --dns-cloudflare \\ --dns-cloudflare-credentials ~/.certbot/cloudflare.ini \\ -i nginx \\ -d *.weyrich.dev # test automatic renewal sudo certbot renew --dry-run # check your domain with ssllab: https://www.ssllabs.com/ssltest/ # reload config sudo nginx -s reload # renew certificate certbot renew\\ --dns-cloudflare \\ --dns-cloudflare-credentials ~/.certbot/cloudflare.ini \\ -d *.weyrich.dev # generate certificate certbot certonly\\ --dns-cloudflare \\ --dns-cloudflare-credentials ~/.certbot/cloudflare.ini \\ -d *.weyrich.dev","title":"setup certbot via install"},{"location":"my-server/server-infra/#setup-certbot-via-docker","text":"Blog entry which explains hot wo setup a letsencrypt client with docker. The linuxserver/letsencrypt docker image is used and configured to generate a wildcard certificate via dns validation.","title":"setup certbot via docker"},{"location":"my-server/server-infra/#setup-docker","text":"curl -sSL https://get.docker.com | sh","title":"setup docker"},{"location":"my-server/server-infra/#setup-nginx-as-reverse-proxy","text":"Base config is created with this tool from digitalocean . Afterwards for every subdomain a proxy_pass is configured. See the page nginx memory aid for an overview of nginx config. And remember to create a CNAME in the dns for every subdomain. The browser will heavily use caching when serving static files so remember to delete the caches after changing proxy settings.","title":"setup nginx as reverse proxy"},{"location":"patterns/adr/","text":"Architecture decision records Michael Nygards blog article Overview with helpful tools and templates Github repository with templates adr parts title decision status context consequences alternatives docs as code Save adr in git (e.g. as markdown) and get consent of the team with reviewed pullrequests. Enumerate the adrs to get a log of decisions e.g. adr-XXXX-searchable-title.md s Use something like github pages to publish your ads.","title":"Architecture decision records"},{"location":"patterns/adr/#architecture-decision-records","text":"Michael Nygards blog article Overview with helpful tools and templates Github repository with templates","title":"Architecture decision records"},{"location":"patterns/adr/#adr-parts","text":"title decision status context consequences alternatives","title":"adr parts"},{"location":"patterns/adr/#docs-as-code","text":"Save adr in git (e.g. as markdown) and get consent of the team with reviewed pullrequests. Enumerate the adrs to get a log of decisions e.g. adr-XXXX-searchable-title.md s Use something like github pages to publish your ads.","title":"docs as code"},{"location":"patterns/aws/","text":"AWS patterns Reference Architecture Diagrams Solution library","title":"AWS patterns"},{"location":"patterns/aws/#aws-patterns","text":"Reference Architecture Diagrams Solution library","title":"AWS patterns"},{"location":"patterns/eda/","text":"event driven architecture saga pattern orchestration vs. choreography event sourcing Event Sourcing is a pattern in which there is not a single state (which gets updated), but a stream of events which can be folded to a state (for every given point in time) Event Sourcing is not CQRS CQRS (Command-Query-Responsibility-Segregation) is a database pattern that separates read and write models (you may have multiple read models). Event sourcing is often used for the write model of a CQRS System. A read model might be one that shows the current state (the folded events) and another one might show the state aggregated for every day. Command Query Segregation is a programming pattern which says that methods either have a return value but no side-effects or should create/update data (and no return value). links small bites, nicely visualized Martin Fowler Martin Fowler GOTO; Confluent Event notification: Just a notification that something (some Data with the ID) changed. More information need to be queued in the source system Pro: decoupling the source for the notification (e.g. change) is admitted as 'business data' itself Con: hard to find out whats going on in the whole system Event-carried state transfer: Enough information that the downstream system do not need to ask more questions (call the services of the source system) Pro: decoupling reduce load to source system Con: Eventual consistency (replicated data) Event Sourcing: you keep a log of changes which can be used to create the state (e.g. accounting ledgers, source code version control) Pro: Alternative state Auditing Con:","title":"event driven architecture"},{"location":"patterns/eda/#event-driven-architecture","text":"","title":"event driven architecture"},{"location":"patterns/eda/#saga-pattern","text":"","title":"saga pattern"},{"location":"patterns/eda/#orchestration-vs-choreography","text":"","title":"orchestration vs. choreography"},{"location":"patterns/eda/#event-sourcing","text":"Event Sourcing is a pattern in which there is not a single state (which gets updated), but a stream of events which can be folded to a state (for every given point in time) Event Sourcing is not CQRS CQRS (Command-Query-Responsibility-Segregation) is a database pattern that separates read and write models (you may have multiple read models). Event sourcing is often used for the write model of a CQRS System. A read model might be one that shows the current state (the folded events) and another one might show the state aggregated for every day. Command Query Segregation is a programming pattern which says that methods either have a return value but no side-effects or should create/update data (and no return value).","title":"event sourcing"},{"location":"patterns/eda/#links","text":"small bites, nicely visualized Martin Fowler Martin Fowler GOTO; Confluent Event notification: Just a notification that something (some Data with the ID) changed. More information need to be queued in the source system Pro: decoupling the source for the notification (e.g. change) is admitted as 'business data' itself Con: hard to find out whats going on in the whole system Event-carried state transfer: Enough information that the downstream system do not need to ask more questions (call the services of the source system) Pro: decoupling reduce load to source system Con: Eventual consistency (replicated data) Event Sourcing: you keep a log of changes which can be used to create the state (e.g. accounting ledgers, source code version control) Pro: Alternative state Auditing Con:","title":"links"},{"location":"patterns/event-sourcing/","text":"Notes on event sourcing Event Sourcing is a pattern in which there is not a single state (which gets updated), but a stream of events which can be folded to a state (for every given point in time) Event Sourcing is not CQRS CQRS (Command-Query-Responsibility-Segregation) is a database pattern that separates read and write models (you may have multiple read models). Event sourcing is often used for the write model of a CQRS System. A read model might be one that shows the current state (the folded events) and another one might show the state aggregated for every day. Command Query Segregation is a programming pattern which says that methods either have a return value but no side-effects or should create/update data (and no return value).","title":"Notes on event sourcing"},{"location":"patterns/event-sourcing/#notes-on-event-sourcing","text":"Event Sourcing is a pattern in which there is not a single state (which gets updated), but a stream of events which can be folded to a state (for every given point in time) Event Sourcing is not CQRS CQRS (Command-Query-Responsibility-Segregation) is a database pattern that separates read and write models (you may have multiple read models). Event sourcing is often used for the write model of a CQRS System. A read model might be one that shows the current state (the folded events) and another one might show the state aggregated for every day. Command Query Segregation is a programming pattern which says that methods either have a return value but no side-effects or should create/update data (and no return value).","title":"Notes on event sourcing"},{"location":"publiccloud/aws/","text":"AWS orientation information on service: Look into the FAQ general health of service: AWS Health Dashboard if accessed without login it allows to filter resources if accessed while logged in it shows health based on your resources automatic analysis of resources: AWS Trusted Advisor create graphic: IaC: Cloudformation use import to create a cloudformation template for an existing ressoure audit and see erros in deployment: Monitor | Activity log terms (AWS account) root user is created by default when an aws account is created. Should not be used directly for creating services. (IAM) Users is an entity that represents an person or an application within your organization. Consist of name and credentials Groups only contain iam users, but not other groups. Users can belong to 0,1,n Groups. IAM identities: users, groups of users, or roles Policies : JSON documents which describe permissions for users, groups or roles (IAM) Role is intended to be assumable by anyone who needs it (users or applications/services). It has no credentials of its own. You get temporary credentials when you assume the role. access keys can be used for programmatic access Access Key ID ~= username Secret Access Key ~= password Region Availabilty Zone # login ## ~/.aws/config ## ~/.aws/credentials aws configure # whoami (awsaccount) aws sts get-caller-identity login to ecr The token is valid for 12 hours. Example: aws ecr get-login-password --region eu-central-1 | docker login --username AWS --password-stdin 934387437486.dkr.ecr.eu-central-1.amazonaws.com lambda invoke patterns Source Synchronous Invokes Asynchronous Invokes Poll-based Invokes services overview/noteworthy global vs. region base services Identity and Access Management (IAM) Route 53 (DNS service) CloudFront (Content Delivery Network) WAF (Web Application Firewall) region based audit security IAM Access Advisor (user): list service permissions of a user and when they were last used IAM Credentials Report (account): list user and credential status EC2 An EC2 Instance is based on a Amazon Machine Image (AMI) User data only run once at the instance first start of an instance (can be used to execute scripts) EC2 Instance Connect allows to connect to an EC2 Instance in the browser (if Amazon Linux is used) but port 22 still needs to be open ECS2 Instace role is a link to a iam role Placement groups allows to control where EC2 Instance are started (cluster, spread, (network) partition) Storage: instance store (hardware) faster than EBS and EFS but ephemeral storage which loses state after restart network attached Elastic block storage (EBS) - device only every EC2 instance has a root EBS which by default gets deleted on EC2 Instance deletion fixed size onyl accessible in the same AZ and by one EC2 instance (except for mutli-attach EBS which allows n in the same AZ) newly attached EBS need to be formatted (except for root EBS) when encrypted at rest the transit to EC2 is enrypted as well Elastic file storage (EFS) - posix filesytem can be accessed by multiple EC2 Instances scales automatically bound to AZ, but accessible in many regions generally cheaper and faster than efs security groups (firewall rules for EC2, EFS, ELB): control how traffic is allowed into or out of EC2 instances only contain allow rules (everything else is blocked) can reference (source/targets) by IP, security group id or prefix lists inbound traffic is blocked by default outbound traffic is authorised by default Distinction between dedicated purchasing options: dedicated host: gives you control over hardware to comply to software licenses. Also adds AWS License Manager service. dedicated instances: old variant and less control than dedicated host Spot instances: create a Spot Instance Request and if your demand can be met you get an EC2 instance, but it can be killed any time you can set a max price you are willing to pay for a Spot Instance Request. But if the average price rises above your max price you will loose all you spot instances. with Spot Block you can reserve an spot instance for a time range (1h-6h) a Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify. service catalog (curated IaC) AWS Service Catalog lets you centrally manage your cloud resources to achieve governance at scale of your infrastructure as code (IaC) templates, written in CloudFormation or Terraform. Aurora Serverless (v2) vs. Aurora XX-Compatible It is the same service in AWS RDS (relational database service), but a different confiuration. Aurora Serverless automatically scales storage and computing resources. The prices is mostly driven by used ACU (aurora compute units). One ACU is roughly 2GB of used RAM. The minimum amount in IDLE is 0,5 ACU and the scale down takes at least 2 Minutes after scaling up. If you want to have regional failover instances you can add additional \"serverless\" instances in other AZs or regions. Aurora v1 vs v2","title":"AWS"},{"location":"publiccloud/aws/#aws","text":"","title":"AWS"},{"location":"publiccloud/aws/#orientation","text":"information on service: Look into the FAQ general health of service: AWS Health Dashboard if accessed without login it allows to filter resources if accessed while logged in it shows health based on your resources automatic analysis of resources: AWS Trusted Advisor create graphic: IaC: Cloudformation use import to create a cloudformation template for an existing ressoure audit and see erros in deployment: Monitor | Activity log","title":"orientation"},{"location":"publiccloud/aws/#terms","text":"(AWS account) root user is created by default when an aws account is created. Should not be used directly for creating services. (IAM) Users is an entity that represents an person or an application within your organization. Consist of name and credentials Groups only contain iam users, but not other groups. Users can belong to 0,1,n Groups. IAM identities: users, groups of users, or roles Policies : JSON documents which describe permissions for users, groups or roles (IAM) Role is intended to be assumable by anyone who needs it (users or applications/services). It has no credentials of its own. You get temporary credentials when you assume the role. access keys can be used for programmatic access Access Key ID ~= username Secret Access Key ~= password Region Availabilty Zone # login ## ~/.aws/config ## ~/.aws/credentials aws configure # whoami (awsaccount) aws sts get-caller-identity","title":"terms"},{"location":"publiccloud/aws/#login-to-ecr","text":"The token is valid for 12 hours. Example: aws ecr get-login-password --region eu-central-1 | docker login --username AWS --password-stdin 934387437486.dkr.ecr.eu-central-1.amazonaws.com","title":"login to ecr"},{"location":"publiccloud/aws/#lambda-invoke-patterns","text":"Source Synchronous Invokes Asynchronous Invokes Poll-based Invokes","title":"lambda invoke patterns"},{"location":"publiccloud/aws/#services-overviewnoteworthy","text":"","title":"services overview/noteworthy"},{"location":"publiccloud/aws/#global-vs-region-base-services","text":"Identity and Access Management (IAM) Route 53 (DNS service) CloudFront (Content Delivery Network) WAF (Web Application Firewall) region based","title":"global vs. region base services"},{"location":"publiccloud/aws/#audit-security","text":"IAM Access Advisor (user): list service permissions of a user and when they were last used IAM Credentials Report (account): list user and credential status","title":"audit security"},{"location":"publiccloud/aws/#ec2","text":"An EC2 Instance is based on a Amazon Machine Image (AMI) User data only run once at the instance first start of an instance (can be used to execute scripts) EC2 Instance Connect allows to connect to an EC2 Instance in the browser (if Amazon Linux is used) but port 22 still needs to be open ECS2 Instace role is a link to a iam role Placement groups allows to control where EC2 Instance are started (cluster, spread, (network) partition) Storage: instance store (hardware) faster than EBS and EFS but ephemeral storage which loses state after restart network attached Elastic block storage (EBS) - device only every EC2 instance has a root EBS which by default gets deleted on EC2 Instance deletion fixed size onyl accessible in the same AZ and by one EC2 instance (except for mutli-attach EBS which allows n in the same AZ) newly attached EBS need to be formatted (except for root EBS) when encrypted at rest the transit to EC2 is enrypted as well Elastic file storage (EFS) - posix filesytem can be accessed by multiple EC2 Instances scales automatically bound to AZ, but accessible in many regions generally cheaper and faster than efs security groups (firewall rules for EC2, EFS, ELB): control how traffic is allowed into or out of EC2 instances only contain allow rules (everything else is blocked) can reference (source/targets) by IP, security group id or prefix lists inbound traffic is blocked by default outbound traffic is authorised by default Distinction between dedicated purchasing options: dedicated host: gives you control over hardware to comply to software licenses. Also adds AWS License Manager service. dedicated instances: old variant and less control than dedicated host Spot instances: create a Spot Instance Request and if your demand can be met you get an EC2 instance, but it can be killed any time you can set a max price you are willing to pay for a Spot Instance Request. But if the average price rises above your max price you will loose all you spot instances. with Spot Block you can reserve an spot instance for a time range (1h-6h) a Spot Fleet is a set of Spot Instances and optionally On-Demand Instances that is launched based on criteria that you specify.","title":"EC2"},{"location":"publiccloud/aws/#_1","text":"","title":""},{"location":"publiccloud/aws/#service-catalog-curated-iac","text":"AWS Service Catalog lets you centrally manage your cloud resources to achieve governance at scale of your infrastructure as code (IaC) templates, written in CloudFormation or Terraform.","title":"service catalog (curated IaC)"},{"location":"publiccloud/aws/#aurora-serverless-v2-vs-aurora-xx-compatible","text":"It is the same service in AWS RDS (relational database service), but a different confiuration. Aurora Serverless automatically scales storage and computing resources. The prices is mostly driven by used ACU (aurora compute units). One ACU is roughly 2GB of used RAM. The minimum amount in IDLE is 0,5 ACU and the scale down takes at least 2 Minutes after scaling up. If you want to have regional failover instances you can add additional \"serverless\" instances in other AZs or regions. Aurora v1 vs v2","title":"Aurora Serverless (v2) vs. Aurora XX-Compatible"},{"location":"publiccloud/azure/","text":"Azure orientation information on service: Look into the FAQ general health of service: Service Health availability of service automatic analysis of resources: advisor recommendations create graphic: Resource Visualizer (part of Resource Group) IaC: Azure Resource Manager templates (ARM templates) can be used to be copie to pulumi azure native provider UI Azure Portal: in most resources under 'Export template' CLI az: az <service> list <resourcename> az <service> show <resourcename> sometimes as yaml: az <service> export <resourcename> audit and see erros in deployment: Monitor | Activity log AWS to Azure services comparison terms directory: company tenant id belongs to directory subscription: part of a directory account: me and belongs to one or more directory and has access to one or more subscriptions Object Id = principal id Object Id != client id application \u2248 managed identity \u2248 service principal az login az account list e --subscription=<id> scopes RBAC and Policies can be applied to: management groups subscriptions resource groups individual resources. Lower levels inherit settings from higher levels. management groups Management groups provide a governance scope above subscriptions. You organize subscriptions into management groups; the governance conditions you apply cascade by inheritance to all associated subscriptions. generate graphics Use https://github.com/microsoft/ARI azure policy vs. azure rbac (governence) Azure RBAC - manages who has access to Azure resources, what areas they have access to and what they can do with those resources. Azure Policy \u2013 focus on resource properties during deployment and for already existing resources. They are able to block or allow specific values for the properties or add properties like tags after deployment. Source: Governance 101: The Difference Between RBAC and Policies collect logging in resource group If you want to access all logs on resource group level you need to create a log analytic workspace. Container apps vs container instances Use container apps when you need autoscaling want easy loadbalancer integration (each app separately) with azure domain (or custom domain) Source Postgres single server vs flex Flexible server recommended service according to azure portal newer postgres versions available zone replication Source blob vs container Blob are grouped in containers. Containers belong to storage accounts. list region/location codes Run az account list-locations -o table or look here create service account for pulumi/terraform in pipeline: See here or: #!/usr/bin/env bash set -e SUBSCRIPTION= NAME=XXXX-pipeline-service-principal YEARS=10 az ad sp create-for-rbac --name=\"${NAME}\" --role=\"Contributor\" --scopes=\"/subscriptions/${SUBSCRIPTION}\" --years=$YEARS --sdk-auth > .localsecret_pipeline ## to get information on user # az ad sp list --display-name=${NAME} ## reset password and print it (if old password is lost) # az ad sp credential reset --name=${NAME} ## delete service principal # az ad sp delete --id 00000000-0000-0000-0000-00000000000000000 managed identities Managed identities are an alternative to credentials. They are bound to a resource and allow the resource to interact with other resources with the rights of the identity (similar to aws roles). System-assigned: identities are created by azure and the lifecycle is bound to the resource in which it was created (e.g. the container app) User-assigned: configure a service principal for the resource. Lifecycle is controled by user. azure exctensions Some commands az containerapp do not run out of the box. You need to install the extension to use it. # list extensions and see if they are installed az extension list-available --output table # add containerapp extension az extension add --name containerapp Official doc see arm template of resource with az az containerapp connection list app registration vs enterprise app / service principal blog article Create service principal in UI Container apps comunication Online as well as communication between to container apps need to use the ingress. You can choose if ingress is public available or only for other container apps. The url is: CONTAINER_APP_NAME.CONTAINER_APP_ENV_NAME.REGION_NAME.azurecontainerapps.io E.g. can be look like this: app.kindflower-050d0634.germanywestcentral.azurecontainerapps.io Every container app revision got an URL as well. This URL contains the revision name as subdomain instead of the conainter app name. container app debugging It may happen that a container is killed but you got no log entry other than \"killed\". Troubleshooting:s set the memory and cpu to max (or check in metrics if the memory metric is too high before it collapesesz pipelines user for az and pulumi How to create and use a service principal az ad sp create-for-rbac --name=\"myapp-service-principal\" --role=\"Contributor\" --scopes=\"/subscriptions/BLA\" --years=10 --sdk-auth Create sp with --sdk-auth flag so your output looks something like this. Action does not work with the normal outoput of four properties. { \"clientId\": \"\", \"clientSecret\": \"\", \"subscriptionId\": \"\", \"tenantId\": \"\", \"activeDirectoryEndpointUrl\": \"\", \"resourceManagerEndpointUrl\": \"\", \"activeDirectoryGraphResourceId\": \"\", \"sqlManagementEndpointUrl\": \"\", \"galleryEndpointUrl\": \"\", \"managementEndpointUrl\": \"\" } ``` ### example for az Add the following to a Action Secret `AZURE_CREDENTIALS`: Pipeline example: ```yaml name: Deploy Backend feat on: push: branches: - azure-env paths: - 'graphql_backend/**/*' - '.github/workflows/azure-feat-backend.yml' env: AZURE_REGISTRY_USERNAME: myapp REGISTRY: myapp.azurecr.io REPOSITORY: myapp-2834-repo IMAGE_TAG: backend-${{github.sha }} AZURE_RESOURCE_GROUP: dev-myapp322577c2 AZURE_CONTAINER_APP_NAME: dev-myapp-app-backend jobs: build: name: Build Backend runs-on: ubuntu-latest environment: dev steps: - name: Checkout uses: actions/checkout@v3 - uses: azure/docker-login@v1 with: login-server: ${{ env.REGISTRY }} username: ${{ env.AZURE_REGISTRY_USERNAME }} password: ${{ secrets.AZURE_REGISTRY_PASSWORD }} - name: Build, tag, and push Backend container registry id: build-image-backend run: | docker build -t ${{ env.REGISTRY }}/${{ env.REPOSITORY}}:${{ env.IMAGE_TAG }} graphql_backend docker push ${{ env.REGISTRY }}/${{ env.REPOSITORY}}:${{ env.IMAGE_TAG }} echo \"::set-output name=image::${{ env.REGISTRY }}/${{ env.REPOSITORY}}:${{ env.IMAGE_TAG }}\" deploy: name: Deploy Backend runs-on: ubuntu-latest environment: dev needs: build steps: - name: Azure Login uses: azure/login@v1 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Deploy to containerapp uses: azure/CLI@v1 with: inlineScript: | echo \"Installing containerapp extension\" az extension add --name containerapp az config set extension.use_dynamic_install=yes_without_prompt # note needed when access is already created echo \"Configure registry\" az containerapp registry set -n ${{ env.AZURE_CONTAINER_APP_NAME }} -g ${{ env.AZURE_RESOURCE_GROUP }} --server ${{ env.REGISTRY }} --username ${{ env.AZURE_REGISTRY_USERNAME }} --password ${{ secrets.AZURE_REGISTRY_PASSWORD }} echo \"Deploy ${{ env.REGISTRY }}/${{ env.IMAGE_TAG }}:${{ github.sha }}\" az containerapp update -n ${{ env.AZURE_CONTAINER_APP_NAME }} -g ${{ env.AZURE_RESOURCE_GROUP }} --image ${{ env.REGISTRY }}/${{ env.IMAGE_TAG }} example for pulumi If you want to use a service principal with pulumi you need to configure the following configs Source Depends on backend. If you use multiple backends e.g. azure and azure-native you need to fullfill all login requirements. pulumi config set azure-native:clientId \"\" pulumi config set azure-native:clientSecret \"\" --secret pulumi config set azure-native:tenantId \"\" pulumi config set azure-native:subscriptionId \"\" name: Pulumi on: - pull_request jobs: preview: name: Preview runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v3 with: node-version-file: .nvmrc - run: npm install # see https://github.com/pulumi/actions # or for full parameter overview https://github.com/pulumi/actions/blob/master/action.yml - uses: pulumi/actions@v3 with: command: preview stack-name: staging comment-on-pr: true cloud-url: azblob://fairmanager-pulumistate #github-token: ${{ secrets.GITHUB_TOKEN }} env: PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }} AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }} AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }} azure logs KQL reference Examples: # container app ## app log ContainerAppConsoleLogs_CL | project TimeGenerated, Log_s, RevisionName_s, ContainerImage_s | sort by TimeGenerated desc ## kubernetes events/logs ContainerAppConsoleLogs_CL | project TimeGenerated, Log_s, RevisionName_s, ContainerImage_s | sort by TimeGenerated desc ## example timerange relative let endDateTime = now(); let startDateTime = ago(1h); ContainerAppConsoleLogs_CL | where TimeGenerated >= startDateTime | where TimeGenerated < endDateTime | sort by TimeGenerated desc delete old images in a repo on-demand task + az acr purge brew + az + completion # workaround no az completion in zsh # https://medium.com/@MyDiemHo/enable-azure-cli-autocomplete-in-oh-my-zsh-93e79019a20d autoload -U +X bashcompinit && bashcompinitsource source /opt/homebrew/etc/bash_completion.d/az pulumi examples Official List work with userAssignedIdentities in typescript As long as this issue is not implemented you may need to workaround limitations of dynamic properties in typescript in conjunction with pulumi.Output type. const userIdentity = new managedidentity.UserAssignedIdentity(\"uai\", { resourceGroupName: resourceGroup.name, location: resourceGroup.location, }); // userIdentity.id is from type pulumi.Output<String> // this does not work ... userAssignedIdentities: { [userIdentity.id]: {} } ... // this does ... identity: { type: containerinstance.ResourceIdentityType.UserAssigned, userAssignedIdentities: userIdentity.id.apply(id => { const dict: { [key: string] : object } = {}; dict[id] = {}; return dict; }), }, ... // translates to this: userAssignedIdentities: { /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}: {} } add firewall rule for azure service To confgure IaC as the azure portal checkbox \"Allow public access from any Azure service within Azure to this serve\" does you need to add the firewall rule with start ip and end ip address 0.0.0.0 See example in ARM: { \"type\": \"Microsoft.DBforPostgreSQL/flexibleServers/firewallRules\", \"apiVersion\": \"2022-01-20-preview\", \"name\": \"[concat(parameters('flexibleServers_dev_fairmanager_postgres_name'), '/allow_all_azure_resources')]\", \"dependsOn\": [ \"[resourceId('Microsoft.DBforPostgreSQL/flexibleServers', parameters('flexibleServers_dev_fairmanager_postgres_name'))]\" ], \"properties\": { \"startIpAddress\": \"0.0.0.0\", \"endIpAddress\": \"0.0.0.0\" } }, or pulumi import * as dbforpostgresql from \"@pulumi/azure-native/dbforpostgresql\"; const pgFirewallRules = [ { firewallRuleName: \"AllowAllAzureServicesAndResourcesWithinAzureIps\", startIpAddress: \"0.0.0.0\", endIpAddress: \"0.0.0.0\" }, ] pgFirewallRules.forEach((rule, index) => { new dbforpostgresql.FirewallRule(`${stack}-postgres-firewall-${index}`, { resourceGroupName: resourceGroup.name, serverName: pgServer.name, firewallRuleName: rule.firewallRuleName, endIpAddress: rule.endIpAddress, startIpAddress: rule.startIpAddress, }); }); application gateway If you can, avoid this resource at all. It is tedious complex to configure. Use frontdoor instead if possible. Pitfall: If you got multiple Level 7 routing mechanism e.g. application gateway which points on container apps you need to ensure that the host header is correct otherwise you get a 404 on the gateway probes. In doubt override the host name in the backend setting of the application gateway to the exact dns name of the target application. Here is a gist which contains pulumi code to create an application gateway with TLS and the necessary certificate generation as well as a vault to save the certificate. At this point it is not possible to import a certificate to a vault via pulumi. See this issue. . pitfalls Reason: You gave azure an resource group id but it expected a resoure group name. Message: error: cannot check existence of resource '/subscriptions/SUBID/resourceGroups/%2Fsubscriptions%2FSUBID%2FresourceGroups%2FRGNAME/providers/Microsoft.Network/applicationGateways/staging-fairmanager-gateway': status code 400, {\"error\":{\"code\":\"InvalidApiVersionParameter\",\"message\":\"The api-version '2020-11-01' is invalid. The supported versions are '2022-09-01,2022-06-01,2022-05-01,2022-03-01-preview,2022-01-01,2021-04-01,2021-01-01,2020-10-01,2020-09-01,2020-08-01,2020-07-01,2020-06-01,2020-05-01,2020-01-01,2019-11-01,2019-10-01,2019-09-01,2019-08-01,2019-07-01,2019-06-01,2019-05-10,2019-05-01,2019-03-01,2018-11-01,2018-09-01,2018-08-01,2018-07-01,2018-06-01,2018-05-01,2018-02-01,2018-01-01,2017-12-01,2017-08-01,2017-06-01,2017-05-10,2017-05-01,2017-03-01,2016-09-01,2016-07-01,2016-06-01,2016-02-01,2015-11-01,2015-01-01,2014-04-01-preview,2014-04-01,2014-01-01,2013-03-01,2014-02-26,2014-04'.\"}} container instances They change their IP Adresse. To expose them in a application gateway you need to: set dns-name-label when they have public ip addresses. if they are in a private vnet you can add a private dns zone and configure a init container which adds its ip adress to the dns Policies Overview of Azure Policies - Azure Policy is a service in Azure which allows you create polices which enforce and control the properties of a resource An Azure initiative is a collection of Azure policy definitions that are grouped together towards a specific goal or purpose in mind. private endpoints The domains of a private endpoint have a public DNS entry and a public IP. If you want to use the endpoint you must override this on the client machiines. If you are inside an Azure vnet you can use a private DNS zone. Otherwise you can follow this guide . private dns zones Are linked to one or more vnet. They are account global and can be linked to vnets in different ressource groups or subscriptions. The main features are auto registration of VMs in the network, forward and reverse lookup for all VMs which are in the connected Vnets. VPN A Virtual Network Gateway is the Azure-side component of a VPN connection. It acts as a gateway for connecting your Azure virtual network to one or more on-premises networks. A Local Network Gateway represents the on-premises network that you want to connect to your Azure virtual network. A Connection in Azure represents the actual link or tunnel between a Virtual Network Gateway (VNG) and a Local Network Gateway (LNG). services to regularly check security center advisor policy","title":"Azure"},{"location":"publiccloud/azure/#azure","text":"","title":"Azure"},{"location":"publiccloud/azure/#orientation","text":"information on service: Look into the FAQ general health of service: Service Health availability of service automatic analysis of resources: advisor recommendations create graphic: Resource Visualizer (part of Resource Group) IaC: Azure Resource Manager templates (ARM templates) can be used to be copie to pulumi azure native provider UI Azure Portal: in most resources under 'Export template' CLI az: az <service> list <resourcename> az <service> show <resourcename> sometimes as yaml: az <service> export <resourcename> audit and see erros in deployment: Monitor | Activity log AWS to Azure services comparison","title":"orientation"},{"location":"publiccloud/azure/#terms","text":"directory: company tenant id belongs to directory subscription: part of a directory account: me and belongs to one or more directory and has access to one or more subscriptions Object Id = principal id Object Id != client id application \u2248 managed identity \u2248 service principal az login az account list e --subscription=<id>","title":"terms"},{"location":"publiccloud/azure/#scopes","text":"RBAC and Policies can be applied to: management groups subscriptions resource groups individual resources. Lower levels inherit settings from higher levels.","title":"scopes"},{"location":"publiccloud/azure/#management-groups","text":"Management groups provide a governance scope above subscriptions. You organize subscriptions into management groups; the governance conditions you apply cascade by inheritance to all associated subscriptions.","title":"management groups"},{"location":"publiccloud/azure/#generate-graphics","text":"Use https://github.com/microsoft/ARI","title":"generate graphics"},{"location":"publiccloud/azure/#azure-policy-vs-azure-rbac-governence","text":"Azure RBAC - manages who has access to Azure resources, what areas they have access to and what they can do with those resources. Azure Policy \u2013 focus on resource properties during deployment and for already existing resources. They are able to block or allow specific values for the properties or add properties like tags after deployment. Source: Governance 101: The Difference Between RBAC and Policies","title":"azure policy vs. azure rbac (governence)"},{"location":"publiccloud/azure/#collect-logging-in-resource-group","text":"If you want to access all logs on resource group level you need to create a log analytic workspace.","title":"collect logging in resource group"},{"location":"publiccloud/azure/#container-apps-vs-container-instances","text":"Use container apps when you need autoscaling want easy loadbalancer integration (each app separately) with azure domain (or custom domain) Source","title":"Container apps vs container instances"},{"location":"publiccloud/azure/#postgres-single-server-vs-flex","text":"Flexible server recommended service according to azure portal newer postgres versions available zone replication Source","title":"Postgres single server vs flex"},{"location":"publiccloud/azure/#blob-vs-container","text":"Blob are grouped in containers. Containers belong to storage accounts.","title":"blob vs container"},{"location":"publiccloud/azure/#list-regionlocation-codes","text":"Run az account list-locations -o table or look here","title":"list region/location codes"},{"location":"publiccloud/azure/#create-service-account-for-pulumiterraform-in-pipeline","text":"See here or: #!/usr/bin/env bash set -e SUBSCRIPTION= NAME=XXXX-pipeline-service-principal YEARS=10 az ad sp create-for-rbac --name=\"${NAME}\" --role=\"Contributor\" --scopes=\"/subscriptions/${SUBSCRIPTION}\" --years=$YEARS --sdk-auth > .localsecret_pipeline ## to get information on user # az ad sp list --display-name=${NAME} ## reset password and print it (if old password is lost) # az ad sp credential reset --name=${NAME} ## delete service principal # az ad sp delete --id 00000000-0000-0000-0000-00000000000000000","title":"create service account for pulumi/terraform in pipeline:"},{"location":"publiccloud/azure/#managed-identities","text":"Managed identities are an alternative to credentials. They are bound to a resource and allow the resource to interact with other resources with the rights of the identity (similar to aws roles). System-assigned: identities are created by azure and the lifecycle is bound to the resource in which it was created (e.g. the container app) User-assigned: configure a service principal for the resource. Lifecycle is controled by user.","title":"managed identities"},{"location":"publiccloud/azure/#azure-exctensions","text":"Some commands az containerapp do not run out of the box. You need to install the extension to use it. # list extensions and see if they are installed az extension list-available --output table # add containerapp extension az extension add --name containerapp Official doc","title":"azure exctensions"},{"location":"publiccloud/azure/#see-arm-template-of-resource-with-az","text":"az containerapp connection list","title":"see arm template of resource with az"},{"location":"publiccloud/azure/#app-registration-vs-enterprise-app-service-principal","text":"blog article Create service principal in UI","title":"app registration vs enterprise app / service principal"},{"location":"publiccloud/azure/#container-apps-comunication","text":"Online as well as communication between to container apps need to use the ingress. You can choose if ingress is public available or only for other container apps. The url is: CONTAINER_APP_NAME.CONTAINER_APP_ENV_NAME.REGION_NAME.azurecontainerapps.io E.g. can be look like this: app.kindflower-050d0634.germanywestcentral.azurecontainerapps.io Every container app revision got an URL as well. This URL contains the revision name as subdomain instead of the conainter app name.","title":"Container apps comunication"},{"location":"publiccloud/azure/#container-app-debugging","text":"It may happen that a container is killed but you got no log entry other than \"killed\". Troubleshooting:s set the memory and cpu to max (or check in metrics if the memory metric is too high before it collapesesz","title":"container app debugging"},{"location":"publiccloud/azure/#pipelines-user-for-az-and-pulumi","text":"How to create and use a service principal az ad sp create-for-rbac --name=\"myapp-service-principal\" --role=\"Contributor\" --scopes=\"/subscriptions/BLA\" --years=10 --sdk-auth Create sp with --sdk-auth flag so your output looks something like this. Action does not work with the normal outoput of four properties. { \"clientId\": \"\", \"clientSecret\": \"\", \"subscriptionId\": \"\", \"tenantId\": \"\", \"activeDirectoryEndpointUrl\": \"\", \"resourceManagerEndpointUrl\": \"\", \"activeDirectoryGraphResourceId\": \"\", \"sqlManagementEndpointUrl\": \"\", \"galleryEndpointUrl\": \"\", \"managementEndpointUrl\": \"\" } ``` ### example for az Add the following to a Action Secret `AZURE_CREDENTIALS`: Pipeline example: ```yaml name: Deploy Backend feat on: push: branches: - azure-env paths: - 'graphql_backend/**/*' - '.github/workflows/azure-feat-backend.yml' env: AZURE_REGISTRY_USERNAME: myapp REGISTRY: myapp.azurecr.io REPOSITORY: myapp-2834-repo IMAGE_TAG: backend-${{github.sha }} AZURE_RESOURCE_GROUP: dev-myapp322577c2 AZURE_CONTAINER_APP_NAME: dev-myapp-app-backend jobs: build: name: Build Backend runs-on: ubuntu-latest environment: dev steps: - name: Checkout uses: actions/checkout@v3 - uses: azure/docker-login@v1 with: login-server: ${{ env.REGISTRY }} username: ${{ env.AZURE_REGISTRY_USERNAME }} password: ${{ secrets.AZURE_REGISTRY_PASSWORD }} - name: Build, tag, and push Backend container registry id: build-image-backend run: | docker build -t ${{ env.REGISTRY }}/${{ env.REPOSITORY}}:${{ env.IMAGE_TAG }} graphql_backend docker push ${{ env.REGISTRY }}/${{ env.REPOSITORY}}:${{ env.IMAGE_TAG }} echo \"::set-output name=image::${{ env.REGISTRY }}/${{ env.REPOSITORY}}:${{ env.IMAGE_TAG }}\" deploy: name: Deploy Backend runs-on: ubuntu-latest environment: dev needs: build steps: - name: Azure Login uses: azure/login@v1 with: creds: ${{ secrets.AZURE_CREDENTIALS }} - name: Deploy to containerapp uses: azure/CLI@v1 with: inlineScript: | echo \"Installing containerapp extension\" az extension add --name containerapp az config set extension.use_dynamic_install=yes_without_prompt # note needed when access is already created echo \"Configure registry\" az containerapp registry set -n ${{ env.AZURE_CONTAINER_APP_NAME }} -g ${{ env.AZURE_RESOURCE_GROUP }} --server ${{ env.REGISTRY }} --username ${{ env.AZURE_REGISTRY_USERNAME }} --password ${{ secrets.AZURE_REGISTRY_PASSWORD }} echo \"Deploy ${{ env.REGISTRY }}/${{ env.IMAGE_TAG }}:${{ github.sha }}\" az containerapp update -n ${{ env.AZURE_CONTAINER_APP_NAME }} -g ${{ env.AZURE_RESOURCE_GROUP }} --image ${{ env.REGISTRY }}/${{ env.IMAGE_TAG }}","title":"pipelines user for az and pulumi"},{"location":"publiccloud/azure/#example-for-pulumi","text":"If you want to use a service principal with pulumi you need to configure the following configs Source Depends on backend. If you use multiple backends e.g. azure and azure-native you need to fullfill all login requirements. pulumi config set azure-native:clientId \"\" pulumi config set azure-native:clientSecret \"\" --secret pulumi config set azure-native:tenantId \"\" pulumi config set azure-native:subscriptionId \"\" name: Pulumi on: - pull_request jobs: preview: name: Preview runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-node@v3 with: node-version-file: .nvmrc - run: npm install # see https://github.com/pulumi/actions # or for full parameter overview https://github.com/pulumi/actions/blob/master/action.yml - uses: pulumi/actions@v3 with: command: preview stack-name: staging comment-on-pr: true cloud-url: azblob://fairmanager-pulumistate #github-token: ${{ secrets.GITHUB_TOKEN }} env: PULUMI_CONFIG_PASSPHRASE: ${{ secrets.PULUMI_CONFIG_PASSPHRASE }} AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }} AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}","title":"example for pulumi"},{"location":"publiccloud/azure/#azure-logs","text":"KQL reference Examples: # container app ## app log ContainerAppConsoleLogs_CL | project TimeGenerated, Log_s, RevisionName_s, ContainerImage_s | sort by TimeGenerated desc ## kubernetes events/logs ContainerAppConsoleLogs_CL | project TimeGenerated, Log_s, RevisionName_s, ContainerImage_s | sort by TimeGenerated desc ## example timerange relative let endDateTime = now(); let startDateTime = ago(1h); ContainerAppConsoleLogs_CL | where TimeGenerated >= startDateTime | where TimeGenerated < endDateTime | sort by TimeGenerated desc","title":"azure logs"},{"location":"publiccloud/azure/#delete-old-images-in-a-repo","text":"on-demand task + az acr purge","title":"delete old images in a repo"},{"location":"publiccloud/azure/#brew-az-completion","text":"# workaround no az completion in zsh # https://medium.com/@MyDiemHo/enable-azure-cli-autocomplete-in-oh-my-zsh-93e79019a20d autoload -U +X bashcompinit && bashcompinitsource source /opt/homebrew/etc/bash_completion.d/az","title":"brew + az + completion"},{"location":"publiccloud/azure/#pulumi-examples","text":"Official List","title":"pulumi examples"},{"location":"publiccloud/azure/#work-with-userassignedidentities-in-typescript","text":"As long as this issue is not implemented you may need to workaround limitations of dynamic properties in typescript in conjunction with pulumi.Output type. const userIdentity = new managedidentity.UserAssignedIdentity(\"uai\", { resourceGroupName: resourceGroup.name, location: resourceGroup.location, }); // userIdentity.id is from type pulumi.Output<String> // this does not work ... userAssignedIdentities: { [userIdentity.id]: {} } ... // this does ... identity: { type: containerinstance.ResourceIdentityType.UserAssigned, userAssignedIdentities: userIdentity.id.apply(id => { const dict: { [key: string] : object } = {}; dict[id] = {}; return dict; }), }, ... // translates to this: userAssignedIdentities: { /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.ManagedIdentity/userAssignedIdentities/{identityName}: {} }","title":"work with userAssignedIdentities in typescript"},{"location":"publiccloud/azure/#add-firewall-rule-for-azure-service","text":"To confgure IaC as the azure portal checkbox \"Allow public access from any Azure service within Azure to this serve\" does you need to add the firewall rule with start ip and end ip address 0.0.0.0 See example in ARM: { \"type\": \"Microsoft.DBforPostgreSQL/flexibleServers/firewallRules\", \"apiVersion\": \"2022-01-20-preview\", \"name\": \"[concat(parameters('flexibleServers_dev_fairmanager_postgres_name'), '/allow_all_azure_resources')]\", \"dependsOn\": [ \"[resourceId('Microsoft.DBforPostgreSQL/flexibleServers', parameters('flexibleServers_dev_fairmanager_postgres_name'))]\" ], \"properties\": { \"startIpAddress\": \"0.0.0.0\", \"endIpAddress\": \"0.0.0.0\" } }, or pulumi import * as dbforpostgresql from \"@pulumi/azure-native/dbforpostgresql\"; const pgFirewallRules = [ { firewallRuleName: \"AllowAllAzureServicesAndResourcesWithinAzureIps\", startIpAddress: \"0.0.0.0\", endIpAddress: \"0.0.0.0\" }, ] pgFirewallRules.forEach((rule, index) => { new dbforpostgresql.FirewallRule(`${stack}-postgres-firewall-${index}`, { resourceGroupName: resourceGroup.name, serverName: pgServer.name, firewallRuleName: rule.firewallRuleName, endIpAddress: rule.endIpAddress, startIpAddress: rule.startIpAddress, }); });","title":"add firewall rule for azure service"},{"location":"publiccloud/azure/#application-gateway","text":"If you can, avoid this resource at all. It is tedious complex to configure. Use frontdoor instead if possible. Pitfall: If you got multiple Level 7 routing mechanism e.g. application gateway which points on container apps you need to ensure that the host header is correct otherwise you get a 404 on the gateway probes. In doubt override the host name in the backend setting of the application gateway to the exact dns name of the target application. Here is a gist which contains pulumi code to create an application gateway with TLS and the necessary certificate generation as well as a vault to save the certificate. At this point it is not possible to import a certificate to a vault via pulumi. See this issue. .","title":"application gateway"},{"location":"publiccloud/azure/#pitfalls","text":"Reason: You gave azure an resource group id but it expected a resoure group name. Message: error: cannot check existence of resource '/subscriptions/SUBID/resourceGroups/%2Fsubscriptions%2FSUBID%2FresourceGroups%2FRGNAME/providers/Microsoft.Network/applicationGateways/staging-fairmanager-gateway': status code 400, {\"error\":{\"code\":\"InvalidApiVersionParameter\",\"message\":\"The api-version '2020-11-01' is invalid. The supported versions are '2022-09-01,2022-06-01,2022-05-01,2022-03-01-preview,2022-01-01,2021-04-01,2021-01-01,2020-10-01,2020-09-01,2020-08-01,2020-07-01,2020-06-01,2020-05-01,2020-01-01,2019-11-01,2019-10-01,2019-09-01,2019-08-01,2019-07-01,2019-06-01,2019-05-10,2019-05-01,2019-03-01,2018-11-01,2018-09-01,2018-08-01,2018-07-01,2018-06-01,2018-05-01,2018-02-01,2018-01-01,2017-12-01,2017-08-01,2017-06-01,2017-05-10,2017-05-01,2017-03-01,2016-09-01,2016-07-01,2016-06-01,2016-02-01,2015-11-01,2015-01-01,2014-04-01-preview,2014-04-01,2014-01-01,2013-03-01,2014-02-26,2014-04'.\"}}","title":"pitfalls"},{"location":"publiccloud/azure/#container-instances","text":"They change their IP Adresse. To expose them in a application gateway you need to: set dns-name-label when they have public ip addresses. if they are in a private vnet you can add a private dns zone and configure a init container which adds its ip adress to the dns","title":"container instances"},{"location":"publiccloud/azure/#policies","text":"Overview of Azure Policies - Azure Policy is a service in Azure which allows you create polices which enforce and control the properties of a resource An Azure initiative is a collection of Azure policy definitions that are grouped together towards a specific goal or purpose in mind.","title":"Policies"},{"location":"publiccloud/azure/#private-endpoints","text":"The domains of a private endpoint have a public DNS entry and a public IP. If you want to use the endpoint you must override this on the client machiines. If you are inside an Azure vnet you can use a private DNS zone. Otherwise you can follow this guide .","title":"private endpoints"},{"location":"publiccloud/azure/#private-dns-zones","text":"Are linked to one or more vnet. They are account global and can be linked to vnets in different ressource groups or subscriptions. The main features are auto registration of VMs in the network, forward and reverse lookup for all VMs which are in the connected Vnets.","title":"private dns zones"},{"location":"publiccloud/azure/#vpn","text":"A Virtual Network Gateway is the Azure-side component of a VPN connection. It acts as a gateway for connecting your Azure virtual network to one or more on-premises networks. A Local Network Gateway represents the on-premises network that you want to connect to your Azure virtual network. A Connection in Azure represents the actual link or tunnel between a Virtual Network Gateway (VNG) and a Local Network Gateway (LNG).","title":"VPN"},{"location":"publiccloud/azure/#services-to-regularly-check","text":"security center advisor policy","title":"services to regularly check"},{"location":"publiccloud/azure_enterprise/","text":"Azure Enterprise ARM Tempalte deployment Deploy ARM with Azure portal create landing zone arch with hub& spoke Deploying ALZ HubAndSpoke 1. SME Deploylink: https://portal.azure.com/#blade/Microsoft_Azure_CreateUIDef/CustomDeploymentBlade/uri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/es-lite.json/createUIDefinitionUri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/portal-es-lite.json readme: https://github.com/Azure/Enterprise-Scale/blob/main/docs/reference/treyresearch/README.md wiki: https://github.com/Azure/Enterprise-Scale/wiki/Deploying-ALZ-BasicSetup ARM templates: tenantDeploymentTemplate: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/es-lite.json uiFormDefinition: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/portal-es-lite.json 2. Enterprise Deploylink: https://portal.azure.com/#blade/Microsoft_Azure_CreateUIDef/CustomDeploymentBlade/uri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslzArm.json/uiFormDefinitionUri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslz-portal.json readme: https://github.com/Azure/Enterprise-Scale/blob/main/docs/reference/adventureworks/README.md wiki: https://github.com/Azure/Enterprise-Scale/wiki/Deploying-ALZ-BasicSetup tenantDeploymentTemplate: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslzArm.json uiFormDefinition: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslz-portal.json 3. manual cli https://github.com/Azure/Enterprise-Scale/tree/main/eslzArm#deploying-in-azure-global-regions transition to azure landing zone Scenario: Transition existing Azure environments to the Azure landing zone conceptual architecture Transition existing Azure environments to the Azure landing zone conceptual architecture move resources Move resources to a new resource group or subscription resource ID: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName} is resource id unique identifier?","title":"Azure Enterprise"},{"location":"publiccloud/azure_enterprise/#azure-enterprise","text":"","title":"Azure Enterprise"},{"location":"publiccloud/azure_enterprise/#arm-tempalte-deployment","text":"Deploy ARM with Azure portal","title":"ARM Tempalte deployment"},{"location":"publiccloud/azure_enterprise/#create-landing-zone-arch-with-hub-spoke","text":"Deploying ALZ HubAndSpoke","title":"create landing zone arch with hub&amp; spoke"},{"location":"publiccloud/azure_enterprise/#1-sme","text":"Deploylink: https://portal.azure.com/#blade/Microsoft_Azure_CreateUIDef/CustomDeploymentBlade/uri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/es-lite.json/createUIDefinitionUri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/portal-es-lite.json readme: https://github.com/Azure/Enterprise-Scale/blob/main/docs/reference/treyresearch/README.md wiki: https://github.com/Azure/Enterprise-Scale/wiki/Deploying-ALZ-BasicSetup ARM templates: tenantDeploymentTemplate: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/es-lite.json uiFormDefinition: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/docs/reference/treyresearch/armTemplates/portal-es-lite.json","title":"1. SME"},{"location":"publiccloud/azure_enterprise/#2-enterprise","text":"Deploylink: https://portal.azure.com/#blade/Microsoft_Azure_CreateUIDef/CustomDeploymentBlade/uri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslzArm.json/uiFormDefinitionUri/https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslz-portal.json readme: https://github.com/Azure/Enterprise-Scale/blob/main/docs/reference/adventureworks/README.md wiki: https://github.com/Azure/Enterprise-Scale/wiki/Deploying-ALZ-BasicSetup tenantDeploymentTemplate: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslzArm.json uiFormDefinition: https://raw.githubusercontent.com/Azure/Enterprise-Scale/main/eslzArm/eslz-portal.json","title":"2. Enterprise"},{"location":"publiccloud/azure_enterprise/#3-manual-cli-httpsgithubcomazureenterprise-scaletreemaineslzarmdeploying-in-azure-global-regions","text":"","title":"3. manual cli https://github.com/Azure/Enterprise-Scale/tree/main/eslzArm#deploying-in-azure-global-regions"},{"location":"publiccloud/azure_enterprise/#transition-to-azure-landing-zone","text":"Scenario: Transition existing Azure environments to the Azure landing zone conceptual architecture Transition existing Azure environments to the Azure landing zone conceptual architecture","title":"transition to azure landing zone"},{"location":"publiccloud/azure_enterprise/#move-resources","text":"Move resources to a new resource group or subscription resource ID: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName} is resource id unique identifier?","title":"move resources"},{"location":"publiccloud/billing/","text":"Billing Data transfer aws Overview of Data Transfer Costs for Common Architectures Source Last updated: Dec 2019","title":"Billing"},{"location":"publiccloud/billing/#billing","text":"","title":"Billing"},{"location":"publiccloud/billing/#data-transfer","text":"","title":"Data transfer"},{"location":"publiccloud/billing/#aws","text":"Overview of Data Transfer Costs for Common Architectures Source Last updated: Dec 2019","title":"aws"},{"location":"publiccloud/gcp/","text":"GCP orientation Information on Service: Look into the FAQ advisor recommendations create graphic: IaC: google cloud deployment mamnager (GDM) - seems to be maintained but not recommended by google (terraform is promoted instead) load attributes of resource as yaml: gcloud <serviceTier> <service> describe <resourceName> (matches the field of pulumi native-google) audit and see erros in deployment: terms Organization Project Zones Regions # login gcloud auth login # list projects in organisation gcloud projects list # set default project for commands gcloud config set project <PROJECT> # list resources (of default project or --project project) gcloud asset search-all-resources #--projects <RPOJECT> roles ref Cloud Functions Developer enable api const apis = [ 'firestore.googleapis.com' // Firestore ]; const enabledAPIs : gcp.projects.Service[] = []; apis.forEach(api => { // Enable the API const enabledAPI = new gcp.projects.Service(`enable-${api}`, { service: api, project: projectId }); enabledAPIs.push(enabledAPI); }); //APIs const database = new gcp.firestore.Database(`${stack}-firestore`, { locationId: 'eur3', project: projectId, type: 'FIRESTORE_NATIVE' }, { dependsOn: enabledAPIs }); service account vs. principal (of service account) https://cloud.google.com/iam/docs/best-practices-service-accounts Add roles to service account (principal, not ressource): Pulumi: // bind role to service account principal new gcp.projects.IAMBinding(`principal-of-sa-roleassignment`, { project: gcpProjectId, role: \"roles/secretmanager.admin\", members: [\"serviceAccount:sa@gcp-projectname.iam.gserviceaccount.com\"], }); // bind role to service account resource new gcp.serviceaccount.IAMBinding`principal-of-sa-roleassignment`, { serviceAccountId: serviceAccount.id, role: \"roles/secretmanager.admin\", members: [\"serviceAccount:sa@gcp-projectname.iam.gserviceaccount.com\"], });","title":"GCP"},{"location":"publiccloud/gcp/#gcp","text":"","title":"GCP"},{"location":"publiccloud/gcp/#orientation","text":"Information on Service: Look into the FAQ advisor recommendations create graphic: IaC: google cloud deployment mamnager (GDM) - seems to be maintained but not recommended by google (terraform is promoted instead) load attributes of resource as yaml: gcloud <serviceTier> <service> describe <resourceName> (matches the field of pulumi native-google) audit and see erros in deployment:","title":"orientation"},{"location":"publiccloud/gcp/#terms","text":"Organization Project Zones Regions # login gcloud auth login # list projects in organisation gcloud projects list # set default project for commands gcloud config set project <PROJECT> # list resources (of default project or --project project) gcloud asset search-all-resources #--projects <RPOJECT>","title":"terms"},{"location":"publiccloud/gcp/#roles-ref","text":"Cloud Functions Developer","title":"roles ref"},{"location":"publiccloud/gcp/#enable-api","text":"const apis = [ 'firestore.googleapis.com' // Firestore ]; const enabledAPIs : gcp.projects.Service[] = []; apis.forEach(api => { // Enable the API const enabledAPI = new gcp.projects.Service(`enable-${api}`, { service: api, project: projectId }); enabledAPIs.push(enabledAPI); }); //APIs const database = new gcp.firestore.Database(`${stack}-firestore`, { locationId: 'eur3', project: projectId, type: 'FIRESTORE_NATIVE' }, { dependsOn: enabledAPIs });","title":"enable api"},{"location":"publiccloud/gcp/#service-account-vs-principal-of-service-account","text":"https://cloud.google.com/iam/docs/best-practices-service-accounts Add roles to service account (principal, not ressource): Pulumi: // bind role to service account principal new gcp.projects.IAMBinding(`principal-of-sa-roleassignment`, { project: gcpProjectId, role: \"roles/secretmanager.admin\", members: [\"serviceAccount:sa@gcp-projectname.iam.gserviceaccount.com\"], }); // bind role to service account resource new gcp.serviceaccount.IAMBinding`principal-of-sa-roleassignment`, { serviceAccountId: serviceAccount.id, role: \"roles/secretmanager.admin\", members: [\"serviceAccount:sa@gcp-projectname.iam.gserviceaccount.com\"], });","title":"service account vs. principal (of service account)"},{"location":"publiccloud/patterns/","text":"Patterns For public cloud. Landing zones A landing zone architecture is a pattern to structure you public cloud account. \"A landing zone is a well-architected, multi-account AWS environment that is scalable and secure. This is a starting point from which your organization can quickly launch and deploy workloads and applications with confidence in your security and infrastructure environment.\" ( What is a landing zone? (AWS) ) Landing zone CloudFormation tempalte \"An Azure landing zone is the output of a multi-subscription Azure environment that accounts for scale, security governance, networking, and identity. An Azure landing zone enables application migration, modernization, and innovation at enterprise-scale in Azure.\" ( What is an Azure landing zone? ) Landing zone architecture ARM templates Landing zone architecture ARM templates documentation Add a landing zone ARM template Hub & Spoke The hub and spoke pattern refers to a network topology where a central cloud service (the hub) is used to connect to multiple remote locations or users (the spokes) through a virtual private network (VPN) or direct connection. The spokes can access cloud services directly through the hub, without having to establish individual connections to the cloud provider. The hub can also provide centralized security, network, and traffic management services, which can improve the overall performance, scalability, and security of the system. Azure Hub & spoke (with landing zones) for SME Azure Hub & spoke (with landing zones) for Enterprises","title":"Patterns"},{"location":"publiccloud/patterns/#patterns","text":"For public cloud.","title":"Patterns"},{"location":"publiccloud/patterns/#landing-zones","text":"A landing zone architecture is a pattern to structure you public cloud account. \"A landing zone is a well-architected, multi-account AWS environment that is scalable and secure. This is a starting point from which your organization can quickly launch and deploy workloads and applications with confidence in your security and infrastructure environment.\" ( What is a landing zone? (AWS) ) Landing zone CloudFormation tempalte \"An Azure landing zone is the output of a multi-subscription Azure environment that accounts for scale, security governance, networking, and identity. An Azure landing zone enables application migration, modernization, and innovation at enterprise-scale in Azure.\" ( What is an Azure landing zone? ) Landing zone architecture ARM templates Landing zone architecture ARM templates documentation Add a landing zone ARM template","title":"Landing zones"},{"location":"publiccloud/patterns/#hub-spoke","text":"The hub and spoke pattern refers to a network topology where a central cloud service (the hub) is used to connect to multiple remote locations or users (the spokes) through a virtual private network (VPN) or direct connection. The spokes can access cloud services directly through the hub, without having to establish individual connections to the cloud provider. The hub can also provide centralized security, network, and traffic management services, which can improve the overall performance, scalability, and security of the system. Azure Hub & spoke (with landing zones) for SME Azure Hub & spoke (with landing zones) for Enterprises","title":"Hub &amp; Spoke"},{"location":"reverseproxy/apache/","text":"Apache http webserver (httpd) how to modules for the given examples Add at the following modules to use the following examples: headers proxy proxy_connect proxy_ajp proxy_http proxy_html proxy_wstunnel ssl configure reverse proxy for service <VirtualHost *:80> ServerName backend.example.de ServerAlias backend.example.de ErrorLog /var/log/apache2/reverseproxy-error_log CustomLog /var/log/apache2/reverseproxy-access_log combined ProxyPass \"/\" \"http://backendhost:80/\" ProxyPassReverse \"/\" \"http://backendhost:80/\" ## trance1 - trace8, debug # LogLevel debug ## off/on # TraceEnable off </VirtualHost> configure reverse proxy for service with TLS To access a TLS backend you need to add SSLProxyEngine on to the vhost. <VirtualHost *:80> ServerName backend.example.de ServerAlias backend.example.de ErrorLog /var/log/apache2/reverseproxy-error_log CustomLog /var/log/apache2/reverseproxy-access_log combined ProxyPass \"/\" \"https://backendhost:443/\" ProxyPassReverse \"/\" \"https://backendhost:443/\" SSLProxyEngine on ## trance1 - trace8, debug # LogLevel debug ## off/on # TraceEnable off </VirtualHost> add tls certificates to reverse proxy Connect to ldap You need to trust cert of ldap if it uses TLS. Add the following tp apache2.conf: LDAPTrustedGlobalCert CA_BASE64 \"/etc/ssl/CA_cert.pem\" <VirtualHost *:80> ServerName backend.example.de ServerAlias backend.example.de ErrorLog /var/log/apache2/reverseproxy-error_log CustomLog /var/log/apache2/reverseproxy-access_log combined ProxyPass \"/\" \"https://backendhost:443/\" ProxyPassReverse \"/\" \"https://backendhost:443/\" # use Location instead of Proxy for static files <Proxy *> AuthType Basic AuthName \"Enter LDAP credentials\" AuthBasicProvider ldap AuthLDAPSubGroupClass group AuthLDAPGroupAttributeIsDN On AuthLDAPURL \"ldaps://{{example.ldap.host}}:{{example.ldap.port}}/DC=example,DC=de?uid?sub?(objectClass=*)\" AuthLDAPBindDN CN={{example.ldap.user}},OU=People,DC=example,DC=de AuthLDAPBindPassword {{example.ldap.password}} require ldap-group cn={{item.ldapRole}},ou=wasgruppen,ou=rechte,dc=example,dc=de </Proxy> ## trance1 - trace8, debug # LogLevel debug ## off/on # TraceEnable off </VirtualHost>","title":"Apache http webserver (httpd)"},{"location":"reverseproxy/apache/#apache-http-webserver-httpd","text":"","title":"Apache http webserver (httpd)"},{"location":"reverseproxy/apache/#how-to","text":"","title":"how to"},{"location":"reverseproxy/apache/#modules-for-the-given-examples","text":"Add at the following modules to use the following examples: headers proxy proxy_connect proxy_ajp proxy_http proxy_html proxy_wstunnel ssl","title":"modules for the given examples"},{"location":"reverseproxy/apache/#configure-reverse-proxy-for-service","text":"<VirtualHost *:80> ServerName backend.example.de ServerAlias backend.example.de ErrorLog /var/log/apache2/reverseproxy-error_log CustomLog /var/log/apache2/reverseproxy-access_log combined ProxyPass \"/\" \"http://backendhost:80/\" ProxyPassReverse \"/\" \"http://backendhost:80/\" ## trance1 - trace8, debug # LogLevel debug ## off/on # TraceEnable off </VirtualHost>","title":"configure reverse proxy for service"},{"location":"reverseproxy/apache/#configure-reverse-proxy-for-service-with-tls","text":"To access a TLS backend you need to add SSLProxyEngine on to the vhost. <VirtualHost *:80> ServerName backend.example.de ServerAlias backend.example.de ErrorLog /var/log/apache2/reverseproxy-error_log CustomLog /var/log/apache2/reverseproxy-access_log combined ProxyPass \"/\" \"https://backendhost:443/\" ProxyPassReverse \"/\" \"https://backendhost:443/\" SSLProxyEngine on ## trance1 - trace8, debug # LogLevel debug ## off/on # TraceEnable off </VirtualHost>","title":"configure reverse proxy for service with TLS"},{"location":"reverseproxy/apache/#add-tls-certificates-to-reverse-proxy","text":"","title":"add tls certificates to reverse proxy"},{"location":"reverseproxy/apache/#connect-to-ldap","text":"You need to trust cert of ldap if it uses TLS. Add the following tp apache2.conf: LDAPTrustedGlobalCert CA_BASE64 \"/etc/ssl/CA_cert.pem\" <VirtualHost *:80> ServerName backend.example.de ServerAlias backend.example.de ErrorLog /var/log/apache2/reverseproxy-error_log CustomLog /var/log/apache2/reverseproxy-access_log combined ProxyPass \"/\" \"https://backendhost:443/\" ProxyPassReverse \"/\" \"https://backendhost:443/\" # use Location instead of Proxy for static files <Proxy *> AuthType Basic AuthName \"Enter LDAP credentials\" AuthBasicProvider ldap AuthLDAPSubGroupClass group AuthLDAPGroupAttributeIsDN On AuthLDAPURL \"ldaps://{{example.ldap.host}}:{{example.ldap.port}}/DC=example,DC=de?uid?sub?(objectClass=*)\" AuthLDAPBindDN CN={{example.ldap.user}},OU=People,DC=example,DC=de AuthLDAPBindPassword {{example.ldap.password}} require ldap-group cn={{item.ldapRole}},ou=wasgruppen,ou=rechte,dc=example,dc=de </Proxy> ## trance1 - trace8, debug # LogLevel debug ## off/on # TraceEnable off </VirtualHost>","title":"Connect to ldap"},{"location":"reverseproxy/general/","text":"Misc reverse proxy stuff general issue you might face when usimg a reverse proxy false redirect use X-Forward-Headers may need custom redirect rules in proxy SameSite Cookie Attribute CORS issues Terms Reverseproxy vs API Gateway Reverseproxy handels the following things Load balancing policy SSL/TLS Termination SSL/TLS Encryption Name based virtual hosting API Gateway can additionally handle the following things: API request validation Payload transformation rate limiting, quotas and throttling retry policy reverse proxy in front of tomcat 9 By default tomcat 9 ignores x-forwarded header which can be a issue if you happen to have redirects in your app. To adress this you need to add org.apache.catalina.valves.RemoteIpValve to the /conf/server.xml of the tomcat and add the needed fields. But beware, if the proxy does not run on the same host or the same local network you also need to configure the trustedProxies field inside the Valve. .... # in Server.Service.Engine.Host <Valve className=\"org.apache.catalina.valves.RemoteIpValve\" remoteIpHeader=\"x-forwarded-for\" proxiesHeader=\"x-forwarded-by\" protocolHeader=\"x-forwarded-proto\" portHeader=\"x-forwarded-port\" /> </Host> </Engine> </Service> </Server> `````` Here is a rudimetary ansible task to add this: ```yaml name: Configure Tomcat to work behind a reverse proxy lineinfile: state: present path: '{{ tomcat_home_dir }}/conf/server.xml' regexp: 'className=\"org.apache.catalina.valves.RemoteIpValve\"' insertbefore: '</Host>' # line breaks will break the regex and therefore the task line: | <Valve className=\"org.apache.catalina.valves.RemoteIpValve\" remoteIpHeader=\"x-forwarded-for\" proxiesHeader=\"x-forwarded-by\" protocolHeader=\"x-forwarded-proto\" portHeader=\"x-forwarded-port\" />","title":"Misc reverse proxy stuff"},{"location":"reverseproxy/general/#misc-reverse-proxy-stuff","text":"","title":"Misc reverse proxy stuff"},{"location":"reverseproxy/general/#general-issue-you-might-face-when-usimg-a-reverse-proxy","text":"false redirect use X-Forward-Headers may need custom redirect rules in proxy SameSite Cookie Attribute CORS issues","title":"general issue you might face when usimg a reverse proxy"},{"location":"reverseproxy/general/#terms-reverseproxy-vs-api-gateway","text":"Reverseproxy handels the following things Load balancing policy SSL/TLS Termination SSL/TLS Encryption Name based virtual hosting API Gateway can additionally handle the following things: API request validation Payload transformation rate limiting, quotas and throttling retry policy","title":"Terms Reverseproxy vs API Gateway"},{"location":"reverseproxy/general/#reverse-proxy-in-front-of-tomcat-9","text":"By default tomcat 9 ignores x-forwarded header which can be a issue if you happen to have redirects in your app. To adress this you need to add org.apache.catalina.valves.RemoteIpValve to the /conf/server.xml of the tomcat and add the needed fields. But beware, if the proxy does not run on the same host or the same local network you also need to configure the trustedProxies field inside the Valve. .... # in Server.Service.Engine.Host <Valve className=\"org.apache.catalina.valves.RemoteIpValve\" remoteIpHeader=\"x-forwarded-for\" proxiesHeader=\"x-forwarded-by\" protocolHeader=\"x-forwarded-proto\" portHeader=\"x-forwarded-port\" /> </Host> </Engine> </Service> </Server> `````` Here is a rudimetary ansible task to add this: ```yaml name: Configure Tomcat to work behind a reverse proxy lineinfile: state: present path: '{{ tomcat_home_dir }}/conf/server.xml' regexp: 'className=\"org.apache.catalina.valves.RemoteIpValve\"' insertbefore: '</Host>' # line breaks will break the regex and therefore the task line: | <Valve className=\"org.apache.catalina.valves.RemoteIpValve\" remoteIpHeader=\"x-forwarded-for\" proxiesHeader=\"x-forwarded-by\" protocolHeader=\"x-forwarded-proto\" portHeader=\"x-forwarded-port\" />","title":"reverse proxy in front of tomcat 9"},{"location":"reverseproxy/nginx/","text":"Apache webserver GitHub Repo to test Proxy conf how to configure Directories: Config /etc/nginx/nginx.conf set global settings and loads the config files in following two folders. All files in /etc/nginx/conf.d/ which ends with .conf are loaded (only servers block allowed. No event or http blocl because it is already present in /etc/nginx/nginx.conf ) (non container version) All files in /etc/nginx/sites-enabled which are usually only a symlink to files in /etc/nginx/sites-available . This directory contains the default configuration of nginx. The default settings exposes files in /var/www directory. Commands: nginx - start server nginx -s stop - stop the server nginx -s reload - reload the config nginx -t - test config Directives: A server block is a subset of Nginx\u2019s configuration that defines a virtual server used to handle requests of a defined type A location block lives within a server block and is used to define how Nginx should handle requests for different resources and URIs for the parent server. A location can either be defined by a prefix string, or by a regular expression. Regular expressions are specified with the preceding \u201c~*\u201d modifier (for case-insensitive matching), or the \u201c~\u201d modifier (for case-sensitive matching). see Getting started Reference core module Reference proxy module see logs Logs usually are saved in /var/log/nginx . configure reverse proxy for service Reference proxy module Simple examples simple server { listen 80; server_name proxyhost.de; # call to proxyhost.de:80 get forwarded to 127.0.0.1:8008 location / { # Sets the protocol and address of a proxied server and an optional URI. ## https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass proxy_pass http://127.0.0.1:8008; } } keep client information Forwarded-XXXX Header explained (used to be X-Forwarded) List of variables build in nginx How to work with paths server { listen 80; server_name proxyhost.de; # call to proxyhost.de:80 get forwarded to 127.0.0.1:8008 location / { # Sets the protocol and address of a proxied server and an optional URI. ## https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass proxy_pass http://backend.de:8008; proxy_set_header Forwarded-Host $host:$server_port; proxy_set_header Forwarded-Server $host; proxy_set_header Forwarded-For $proxy_add_x_forwarded_for; } } path examples Test your proxy_pass with this docker-compose setup proxy_pass without path ( : ) Location path will get appended: ... # proxyhost.de:80 -> 127.0.0.1:2000 # proxyhost.de:80/example -> 127.0.0.1:2000/example location / { proxy_pass http://127.0.0.1:2000; } # proxyhost.de:80/api -> 127.0.0.1:3000/api # proxyhost.de:80/api/v1 -> 127.0.0.1:3000/api/v1 location /api { proxy_pass http://127.0.0.1:3000; } proxy_pass with path ( : / ) Location path will be replaced: ... # proxyhost.de/app -> 127.0.0.1:4000/api2 # proxyhost.de/app/api2 -> 127.0.0.1:4000/api2/api2 location /app { proxy_pass http://127.0.0.1:4000/api2; } # => /app get replaced by /api2 # proxyhost.de/app2 -> 127.0.0.1:5000/ # proxyhost.de/app2/api/v1 -> 127.0.0.1:5000/api/v1 location /app2 { # / at the end counts a path! proxy_pass http://127.0.0.1:5000/; } # => /app2 get replaced by / ... add tls certificates Nginx TLS Guide events { } http { server { listen 443 ssl; server_name www.example.com; ssl_certificate /etc/nginx/conf/www.example.com.crt; ssl_certificate_key /etc/nginx/conf/www.example.com.key; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA; ssl_prefer_server_ciphers off; location / { proxy_pass http://127.0.0.1:8008; } } } Connect to ldap Precedence of server block and location block The guide on how config blocks are chosen by nginx is helpful. The outline is: find server block with the most 'specific' ip address and port in the listen directive if multiple listen directives are equally specific (or just equal) then the server_name directive will decide which will execute it. Search for the most 'specific one' example Example of reverse proxy with tls support which proxies to a non-tls server (except for the subdomain it is generated the tool ): server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name weyrich.dev; root /var/www/weyrich.dev/public; # SSL ssl_certificate /etc/letsencrypt/live/weyrich.dev/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/weyrich.dev/privkey.pem; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/weyrich.dev.access.log; error_log /var/log/nginx/weyrich.dev.error.log warn; # index.html fallback location / { try_files $uri $uri/ /index.html; } # additional config include nginxconfig.io/general.conf; } # ci subdomains proxy pass server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name ci.weyrich.dev; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/ci.weyrich.dev.access.log; error_log /var/log/nginx/ci.weyrich.dev.error.log warn; # reverse proxy location / { proxy_pass http://127.0.0.1:180/; include nginxconfig.io/proxy.conf; } } # HTTP redirect server { listen 78.47.255.193:80; listen [::]:80; server_name .weyrich.dev; return 301 https://weyrich.dev$request_uri; }","title":"Apache webserver"},{"location":"reverseproxy/nginx/#apache-webserver","text":"GitHub Repo to test Proxy conf","title":"Apache webserver"},{"location":"reverseproxy/nginx/#how-to","text":"","title":"how to"},{"location":"reverseproxy/nginx/#configure","text":"Directories: Config /etc/nginx/nginx.conf set global settings and loads the config files in following two folders. All files in /etc/nginx/conf.d/ which ends with .conf are loaded (only servers block allowed. No event or http blocl because it is already present in /etc/nginx/nginx.conf ) (non container version) All files in /etc/nginx/sites-enabled which are usually only a symlink to files in /etc/nginx/sites-available . This directory contains the default configuration of nginx. The default settings exposes files in /var/www directory. Commands: nginx - start server nginx -s stop - stop the server nginx -s reload - reload the config nginx -t - test config Directives: A server block is a subset of Nginx\u2019s configuration that defines a virtual server used to handle requests of a defined type A location block lives within a server block and is used to define how Nginx should handle requests for different resources and URIs for the parent server. A location can either be defined by a prefix string, or by a regular expression. Regular expressions are specified with the preceding \u201c~*\u201d modifier (for case-insensitive matching), or the \u201c~\u201d modifier (for case-sensitive matching). see Getting started Reference core module Reference proxy module","title":"configure"},{"location":"reverseproxy/nginx/#see-logs","text":"Logs usually are saved in /var/log/nginx .","title":"see logs"},{"location":"reverseproxy/nginx/#configure-reverse-proxy-for-service","text":"Reference proxy module Simple examples","title":"configure reverse proxy for service"},{"location":"reverseproxy/nginx/#simple","text":"server { listen 80; server_name proxyhost.de; # call to proxyhost.de:80 get forwarded to 127.0.0.1:8008 location / { # Sets the protocol and address of a proxied server and an optional URI. ## https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass proxy_pass http://127.0.0.1:8008; } }","title":"simple"},{"location":"reverseproxy/nginx/#keep-client-information","text":"Forwarded-XXXX Header explained (used to be X-Forwarded) List of variables build in nginx How to work with paths server { listen 80; server_name proxyhost.de; # call to proxyhost.de:80 get forwarded to 127.0.0.1:8008 location / { # Sets the protocol and address of a proxied server and an optional URI. ## https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass proxy_pass http://backend.de:8008; proxy_set_header Forwarded-Host $host:$server_port; proxy_set_header Forwarded-Server $host; proxy_set_header Forwarded-For $proxy_add_x_forwarded_for; } }","title":"keep client information"},{"location":"reverseproxy/nginx/#path-examples","text":"Test your proxy_pass with this docker-compose setup","title":"path examples"},{"location":"reverseproxy/nginx/#proxy_pass-without-path","text":"Location path will get appended: ... # proxyhost.de:80 -> 127.0.0.1:2000 # proxyhost.de:80/example -> 127.0.0.1:2000/example location / { proxy_pass http://127.0.0.1:2000; } # proxyhost.de:80/api -> 127.0.0.1:3000/api # proxyhost.de:80/api/v1 -> 127.0.0.1:3000/api/v1 location /api { proxy_pass http://127.0.0.1:3000; }","title":"proxy_pass without path (:)"},{"location":"reverseproxy/nginx/#proxy_pass-with-path","text":"Location path will be replaced: ... # proxyhost.de/app -> 127.0.0.1:4000/api2 # proxyhost.de/app/api2 -> 127.0.0.1:4000/api2/api2 location /app { proxy_pass http://127.0.0.1:4000/api2; } # => /app get replaced by /api2 # proxyhost.de/app2 -> 127.0.0.1:5000/ # proxyhost.de/app2/api/v1 -> 127.0.0.1:5000/api/v1 location /app2 { # / at the end counts a path! proxy_pass http://127.0.0.1:5000/; } # => /app2 get replaced by / ...","title":"proxy_pass with path (:/)"},{"location":"reverseproxy/nginx/#add-tls-certificates","text":"Nginx TLS Guide events { } http { server { listen 443 ssl; server_name www.example.com; ssl_certificate /etc/nginx/conf/www.example.com.crt; ssl_certificate_key /etc/nginx/conf/www.example.com.key; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA; ssl_prefer_server_ciphers off; location / { proxy_pass http://127.0.0.1:8008; } } }","title":"add tls certificates"},{"location":"reverseproxy/nginx/#connect-to-ldap","text":"","title":"Connect to ldap"},{"location":"reverseproxy/nginx/#precedence-of-server-block-and-location-block","text":"The guide on how config blocks are chosen by nginx is helpful. The outline is: find server block with the most 'specific' ip address and port in the listen directive if multiple listen directives are equally specific (or just equal) then the server_name directive will decide which will execute it. Search for the most 'specific one'","title":"Precedence of server block and location block"},{"location":"reverseproxy/nginx/#example","text":"Example of reverse proxy with tls support which proxies to a non-tls server (except for the subdomain it is generated the tool ): server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name weyrich.dev; root /var/www/weyrich.dev/public; # SSL ssl_certificate /etc/letsencrypt/live/weyrich.dev/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/weyrich.dev/privkey.pem; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/weyrich.dev.access.log; error_log /var/log/nginx/weyrich.dev.error.log warn; # index.html fallback location / { try_files $uri $uri/ /index.html; } # additional config include nginxconfig.io/general.conf; } # ci subdomains proxy pass server { listen 78.47.255.193:443 ssl http2; listen [::]:443 ssl http2; server_name ci.weyrich.dev; # security include nginxconfig.io/security.conf; # logging access_log /var/log/nginx/ci.weyrich.dev.access.log; error_log /var/log/nginx/ci.weyrich.dev.error.log warn; # reverse proxy location / { proxy_pass http://127.0.0.1:180/; include nginxconfig.io/proxy.conf; } } # HTTP redirect server { listen 78.47.255.193:80; listen [::]:80; server_name .weyrich.dev; return 301 https://weyrich.dev$request_uri; }","title":"example"},{"location":"reverseproxy/traefik/","text":"Traefik components Providers discover the services that live on your infrastructure (their IP, health, ...) Entrypoints listen for incoming traffic (ports, ...) Routers analyse the requests (host, path, headers, SSL, ...) Services forward the request to your services (load balancing, ...) Middlewares may update the request or make decisions based on the request (authentication, rate limiting, headers, ...) static vs. dynamic config Configuration in Traefik can refer to two different things: The fully dynamic routing configuration (referred to as the dynamic configuration ) The startup configuration (referred to as the static configuration ) activate dashboard insecure with auth TLS HTTPs is configured at the router level (so in dynamic config) (routers. .tls=true) TLS certs can be provided in a seperate dynmic config file Traefik searches though certificates (tls.certificates) for the right cert When no cert is found it serves a default certificate You can override the default certificate by creating a certificate store named default Traefik Proxy 2.x and TLS 101 Example for dynamic tls config file: # Dynamic Configuration ## https://doc.traefik.io/traefik/https/tls/#default-certificate tls: stores: default: defaultCertificate: certFile: /etc/mycerts/fullchain.pem keyFile: /etc/mycerts/privkey.pem trust certificates of backend services (import CA certificates) configure on global level: https://doc.traefik.io/traefik/routing/overview/#rootcas ## Static configuration serversTransport: rootCAs: - foo.crt - bar.crt configure for service only: Source simple reverse proxy example static conf entryPoints: web: address: ':80' providers: file: filename: /path/to/dynamic/conf.yaml dynamic conf (reverse-proxy) # /path/to/dynamic/conf.yaml http: routers: my-router: rule: PathPrefix(`/`) service: my-service middlewares: - stripPath ## If not specified, HTTP routers will accept requests from all defined entry points # entrypoints: # - web # examples for path resultin blue box \"Behavior examples\": https://doc.traefik.io/traefik/middlewares/http/stripprefix/#forceslash middlewares: stripPath: stripPrefix: prefixes: - \"test\" forceSlash: false services: my-service: loadBalancer: servers: - url: 'http://localhost:8080' Source systemd service file Can be found here . rewrite location header (after redirect) If a server behind traefik send an redirect (302) you may need to intercept it otherwise the domain or the protocol can be switched (like with proxy_redirect in nginx or ProxyPassReverse in apache) Possible solutions: Use RedirectRegex Middleware (caveats browser sees second redirect and might create a loop) use traefik-plugin-rewrite-headers traefik plugin (supposedly breaks websockets) There is an open issue in traefik to support it out of the box Source chain of proxies You might want to forward existing X-Forwarded-* Headers or create some of you own, when they are not there. Traefik creates The X-Forwarded-* Headers by default. The list of created X-Forwarded headers can be found here . To keep existing X-Forwarded Headers you need to configure trusted IPs on the endpoint ( forwardedHeaders.trustedIPs ) you need to be aware of HostRules . It might be that the first traefik has another domain as the second. In this case you HostRule will not work properly To debug requests and mapping logic of rules activate accessLogs with json format # /path/to/dynamic/conf.yaml accessLog: # without file path logs to stdout # filePath: \"/path/to/access.log\" format: json","title":"Traefik"},{"location":"reverseproxy/traefik/#traefik","text":"","title":"Traefik"},{"location":"reverseproxy/traefik/#components","text":"Providers discover the services that live on your infrastructure (their IP, health, ...) Entrypoints listen for incoming traffic (ports, ...) Routers analyse the requests (host, path, headers, SSL, ...) Services forward the request to your services (load balancing, ...) Middlewares may update the request or make decisions based on the request (authentication, rate limiting, headers, ...)","title":"components"},{"location":"reverseproxy/traefik/#static-vs-dynamic-config","text":"Configuration in Traefik can refer to two different things: The fully dynamic routing configuration (referred to as the dynamic configuration ) The startup configuration (referred to as the static configuration )","title":"static vs. dynamic config"},{"location":"reverseproxy/traefik/#activate-dashboard","text":"insecure with auth","title":"activate dashboard"},{"location":"reverseproxy/traefik/#tls","text":"HTTPs is configured at the router level (so in dynamic config) (routers. .tls=true) TLS certs can be provided in a seperate dynmic config file Traefik searches though certificates (tls.certificates) for the right cert When no cert is found it serves a default certificate You can override the default certificate by creating a certificate store named default Traefik Proxy 2.x and TLS 101 Example for dynamic tls config file: # Dynamic Configuration ## https://doc.traefik.io/traefik/https/tls/#default-certificate tls: stores: default: defaultCertificate: certFile: /etc/mycerts/fullchain.pem keyFile: /etc/mycerts/privkey.pem","title":"TLS"},{"location":"reverseproxy/traefik/#trust-certificates-of-backend-services-import-ca-certificates","text":"configure on global level: https://doc.traefik.io/traefik/routing/overview/#rootcas ## Static configuration serversTransport: rootCAs: - foo.crt - bar.crt configure for service only: Source","title":"trust certificates of backend services (import CA certificates)"},{"location":"reverseproxy/traefik/#simple-reverse-proxy-example","text":"static conf entryPoints: web: address: ':80' providers: file: filename: /path/to/dynamic/conf.yaml dynamic conf (reverse-proxy) # /path/to/dynamic/conf.yaml http: routers: my-router: rule: PathPrefix(`/`) service: my-service middlewares: - stripPath ## If not specified, HTTP routers will accept requests from all defined entry points # entrypoints: # - web # examples for path resultin blue box \"Behavior examples\": https://doc.traefik.io/traefik/middlewares/http/stripprefix/#forceslash middlewares: stripPath: stripPrefix: prefixes: - \"test\" forceSlash: false services: my-service: loadBalancer: servers: - url: 'http://localhost:8080' Source","title":"simple reverse proxy example"},{"location":"reverseproxy/traefik/#systemd-service-file","text":"Can be found here .","title":"systemd service file"},{"location":"reverseproxy/traefik/#rewrite-location-header-after-redirect","text":"If a server behind traefik send an redirect (302) you may need to intercept it otherwise the domain or the protocol can be switched (like with proxy_redirect in nginx or ProxyPassReverse in apache) Possible solutions: Use RedirectRegex Middleware (caveats browser sees second redirect and might create a loop) use traefik-plugin-rewrite-headers traefik plugin (supposedly breaks websockets) There is an open issue in traefik to support it out of the box Source","title":"rewrite location header (after redirect)"},{"location":"reverseproxy/traefik/#chain-of-proxies","text":"You might want to forward existing X-Forwarded-* Headers or create some of you own, when they are not there. Traefik creates The X-Forwarded-* Headers by default. The list of created X-Forwarded headers can be found here . To keep existing X-Forwarded Headers you need to configure trusted IPs on the endpoint ( forwardedHeaders.trustedIPs ) you need to be aware of HostRules . It might be that the first traefik has another domain as the second. In this case you HostRule will not work properly To debug requests and mapping logic of rules activate accessLogs with json format # /path/to/dynamic/conf.yaml accessLog: # without file path logs to stdout # filePath: \"/path/to/access.log\" format: json","title":"chain of proxies"},{"location":"spring/0_spring-fundamentals/","text":"Spring fundamentals general There is always at least on Inversion of Control container which is called Spring ApplicationContext . It is possible to have multiple ApplicationContexts. In this case there is a root ApplicationContext which is used to create child ApplicationContexts. In order to keep consistent versions of the spring framework dependencies you can import platform-bom as a pom into your own pom. configuring application context configuration annotation A Configuration Annotation is used on classes which defines beans. It is an alternative to a xml config file. The @Bean Annotation: is used on Methods can only be used in classes which are annotated with @Configuration is an alternative to @Component/ @Autowire. It allows more fine grained instantiation logic (decouples bean definition and bean declaration) Differences of Bean and Component Configuration bootsrapping a spring application (context) @Configuration @ComponentScan // alternative to usage of explicit @Bean annotations. Needed for classes which are annotated with a @Component stereotype public class SpringApp { private static ApplicationContext applicationContext; @Bean public ExampleBean exampleBean() { // do further declaration stuff if needed return new ExampleBean(); } public static void main(String[] args) { this.applicationContext = new AnnotationConfigApplicationContext(SpringApp.class); for (String beanName : applicationContext.getBeanDefinitionNames()) { System.out.println(beanName); } } } properties and environmental variables The @Value annotations is used on fields, constructor parameters and method parameters. It allows to inject values from a property file or a environmental variable into a filed or a parameter. It supports the spring expressions language (SPELL). To import a property of a property file: @PropertySource(\"classpath:/application.property\") @Value(\"${property.name}\") private String example; To import a environmental property: @Value(\"${environmental_property_name}\") private String example; profiles Spring profiles allow you to load alternate configurations based on environment variables With @Profile() a bean can be added to a specific profile environmental variable to control the spring profile: spring.profiles.active You can set profile via: Context parameter in web.xml ( <ontext-param> <param-name>spring.profiles.active</param-name><param-value>...</param-value></context-param> ) WebApplicationInitializer JVM System parameter (spring.profiles.active) Environment variable (spring.profiles.active) Maven profile ( ) @ActiveProfile for unit test You can read profile via: Environment class @Value(spring.profiles.active) Examples how to activate profile from cli: # as jvm parameter java -jar -Dspring.profiles.active=dev app.jar # as application arguments java -jar app.jar --spring.profiles.active=dev # as maven profile spring boot mvn spring-boot:run -Dspring-boot.run.profiles=dev # as maven profile spring mvn spring-boot:run -Drun.profiles=dev spring expression language SPELL examples: read environmental variables: \"${environmental_variable_name}\" Object creation \"#{new Boolean(environment['spring.profiles.active'] == 'dev')\" bean scopes To explicitly use a scope add @Scope() to Bean. Scopes: singleton bean (default) prototype bean: new instance every time it is referenced sessions scoped bean (web) request scoped bean (web) proxies proxies add behavior to beans like caching or transactions are only used if the bean is instantiated by spring Annotation-based configuration component scan Annotation-based configuration/ component scanning is an alternative to adding beans to Application context via @Bean or xml config. You need to add @ComponentScan to a class. The annotation without arguments tells Spring to scan the current package and all of its sub-packages for classes which are annotated witch @Component or other stereotypes. The Component annotation is a so called generic stereotype , which is used to mark a class which should be instantiated by the BeanFactory. The other stereotypes are Controller , Service and Repository , which are specializations of the @Component class. Some stereotypes bring default proxies with them. Use @Autowire to inject a bean. The @Qualifier Annotation can be used if there are multiple implementations for a bean interface. autowiring field level injection: is not good because it is hard to test/ mock can not control the construction setter level injection: for optional dependecies for changing dependecies (mutable variables) constructor level injection: preferred method immutable lifecycle methods @PostConstruct is used on a void-method and is called after the initialization of a bean. @PreDestroy is used on a void-method and is called before the bean is destroyed. The annotations are part of java ee and therefore removed in java 11. To use them you must define them as depdency: <dependency> <groupId>javax.annotation</groupId> <artifactId>javax.annotation-api</artifactId> <version>1.3.2</version> </dependency> xml-based config (legacy) It is legacy . xml elements are used for beans attributes and sub-elements are used for configuration of beans xml namespaces are used typical names are: application-config.xml, mvc-config.xml, web-application-config.xml bean lifecycle Initialization use destroy Initialization varies for the three ways beans can be defined. This one is for xml, but it is similar to the two other ways: Create ApplicationContext which wraps the BeanFactory Initialize the BeanFactory Load Bean definitions (via java config, xml config, autoconfig and component scanning) BeanFactory Post-Processing (resolve SPELL or interpolation from property files) Foreach bean: init constructor setter (and fields) Bean Post-Processing pre init init post init aspects Aspects are: cross cutting concerns reusable blocks of code e.g. logging, security, transaction management, caching Spring uses AspectJ for Aspect oriented programming (AOP), which uses byte code modification (runtime interweaving). If (vanilla) spring is used @EnableAspectJAutoProxy needs to be added to the Configuration class. An alternative for AspekctJ are JavaEE CDI interceptors (they are simpler but less flebile and apsectj is spring native). Nomenclature Joint point - code where advice is used Pointcut - selects a joint point Advice - code that is exectuted at join point Aspect - Module containing pointcuts and advice @Aspect is used to mark a class to show that it contains pointcuts and advice. Pointcuts are methods marked with @Pointcut( ) annotation. pointcut expressions: @annotation - use a annotation to mark pointcut execution - matches method signature within - matches class type signature patterns bean - matches class name patterns || - combines pointcut expressions Advice are methods which are marked with one of the following annotations and reference the method name of a pointcut (method annotated with @Pointcut): Before After Around","title":"Spring fundamentals"},{"location":"spring/0_spring-fundamentals/#spring-fundamentals","text":"","title":"Spring fundamentals"},{"location":"spring/0_spring-fundamentals/#general","text":"There is always at least on Inversion of Control container which is called Spring ApplicationContext . It is possible to have multiple ApplicationContexts. In this case there is a root ApplicationContext which is used to create child ApplicationContexts. In order to keep consistent versions of the spring framework dependencies you can import platform-bom as a pom into your own pom.","title":"general"},{"location":"spring/0_spring-fundamentals/#configuring-application-context","text":"","title":"configuring application context"},{"location":"spring/0_spring-fundamentals/#configuration-annotation","text":"A Configuration Annotation is used on classes which defines beans. It is an alternative to a xml config file. The @Bean Annotation: is used on Methods can only be used in classes which are annotated with @Configuration is an alternative to @Component/ @Autowire. It allows more fine grained instantiation logic (decouples bean definition and bean declaration) Differences of Bean and Component Configuration","title":"configuration annotation"},{"location":"spring/0_spring-fundamentals/#bootsrapping-a-spring-application-context","text":"@Configuration @ComponentScan // alternative to usage of explicit @Bean annotations. Needed for classes which are annotated with a @Component stereotype public class SpringApp { private static ApplicationContext applicationContext; @Bean public ExampleBean exampleBean() { // do further declaration stuff if needed return new ExampleBean(); } public static void main(String[] args) { this.applicationContext = new AnnotationConfigApplicationContext(SpringApp.class); for (String beanName : applicationContext.getBeanDefinitionNames()) { System.out.println(beanName); } } }","title":"bootsrapping a spring application (context)"},{"location":"spring/0_spring-fundamentals/#properties-and-environmental-variables","text":"The @Value annotations is used on fields, constructor parameters and method parameters. It allows to inject values from a property file or a environmental variable into a filed or a parameter. It supports the spring expressions language (SPELL). To import a property of a property file: @PropertySource(\"classpath:/application.property\") @Value(\"${property.name}\") private String example; To import a environmental property: @Value(\"${environmental_property_name}\") private String example;","title":"properties and environmental variables"},{"location":"spring/0_spring-fundamentals/#profiles","text":"Spring profiles allow you to load alternate configurations based on environment variables With @Profile() a bean can be added to a specific profile environmental variable to control the spring profile: spring.profiles.active You can set profile via: Context parameter in web.xml ( <ontext-param> <param-name>spring.profiles.active</param-name><param-value>...</param-value></context-param> ) WebApplicationInitializer JVM System parameter (spring.profiles.active) Environment variable (spring.profiles.active) Maven profile ( ) @ActiveProfile for unit test You can read profile via: Environment class @Value(spring.profiles.active) Examples how to activate profile from cli: # as jvm parameter java -jar -Dspring.profiles.active=dev app.jar # as application arguments java -jar app.jar --spring.profiles.active=dev # as maven profile spring boot mvn spring-boot:run -Dspring-boot.run.profiles=dev # as maven profile spring mvn spring-boot:run -Drun.profiles=dev","title":"profiles"},{"location":"spring/0_spring-fundamentals/#spring-expression-language-spell","text":"examples: read environmental variables: \"${environmental_variable_name}\" Object creation \"#{new Boolean(environment['spring.profiles.active'] == 'dev')\"","title":"spring expression language SPELL"},{"location":"spring/0_spring-fundamentals/#bean-scopes","text":"To explicitly use a scope add @Scope() to Bean. Scopes: singleton bean (default) prototype bean: new instance every time it is referenced sessions scoped bean (web) request scoped bean (web)","title":"bean scopes"},{"location":"spring/0_spring-fundamentals/#proxies","text":"proxies add behavior to beans like caching or transactions are only used if the bean is instantiated by spring","title":"proxies"},{"location":"spring/0_spring-fundamentals/#annotation-based-configuration","text":"","title":"Annotation-based configuration"},{"location":"spring/0_spring-fundamentals/#component-scan","text":"Annotation-based configuration/ component scanning is an alternative to adding beans to Application context via @Bean or xml config. You need to add @ComponentScan to a class. The annotation without arguments tells Spring to scan the current package and all of its sub-packages for classes which are annotated witch @Component or other stereotypes. The Component annotation is a so called generic stereotype , which is used to mark a class which should be instantiated by the BeanFactory. The other stereotypes are Controller , Service and Repository , which are specializations of the @Component class. Some stereotypes bring default proxies with them. Use @Autowire to inject a bean. The @Qualifier Annotation can be used if there are multiple implementations for a bean interface.","title":"component scan"},{"location":"spring/0_spring-fundamentals/#autowiring","text":"field level injection: is not good because it is hard to test/ mock can not control the construction setter level injection: for optional dependecies for changing dependecies (mutable variables) constructor level injection: preferred method immutable","title":"autowiring"},{"location":"spring/0_spring-fundamentals/#lifecycle-methods","text":"@PostConstruct is used on a void-method and is called after the initialization of a bean. @PreDestroy is used on a void-method and is called before the bean is destroyed. The annotations are part of java ee and therefore removed in java 11. To use them you must define them as depdency: <dependency> <groupId>javax.annotation</groupId> <artifactId>javax.annotation-api</artifactId> <version>1.3.2</version> </dependency>","title":"lifecycle methods"},{"location":"spring/0_spring-fundamentals/#xml-based-config-legacy","text":"It is legacy . xml elements are used for beans attributes and sub-elements are used for configuration of beans xml namespaces are used typical names are: application-config.xml, mvc-config.xml, web-application-config.xml","title":"xml-based config (legacy)"},{"location":"spring/0_spring-fundamentals/#bean-lifecycle","text":"Initialization use destroy Initialization varies for the three ways beans can be defined. This one is for xml, but it is similar to the two other ways: Create ApplicationContext which wraps the BeanFactory Initialize the BeanFactory Load Bean definitions (via java config, xml config, autoconfig and component scanning) BeanFactory Post-Processing (resolve SPELL or interpolation from property files) Foreach bean: init constructor setter (and fields) Bean Post-Processing pre init init post init","title":"bean lifecycle"},{"location":"spring/0_spring-fundamentals/#aspects","text":"Aspects are: cross cutting concerns reusable blocks of code e.g. logging, security, transaction management, caching Spring uses AspectJ for Aspect oriented programming (AOP), which uses byte code modification (runtime interweaving). If (vanilla) spring is used @EnableAspectJAutoProxy needs to be added to the Configuration class. An alternative for AspekctJ are JavaEE CDI interceptors (they are simpler but less flebile and apsectj is spring native). Nomenclature Joint point - code where advice is used Pointcut - selects a joint point Advice - code that is exectuted at join point Aspect - Module containing pointcuts and advice @Aspect is used to mark a class to show that it contains pointcuts and advice. Pointcuts are methods marked with @Pointcut( ) annotation. pointcut expressions: @annotation - use a annotation to mark pointcut execution - matches method signature within - matches class type signature patterns bean - matches class name patterns || - combines pointcut expressions Advice are methods which are marked with one of the following annotations and reference the method name of a pointcut (method annotated with @Pointcut): Before After Around","title":"aspects"},{"location":"spring/1_spring-boot/","text":"Spring Boot helpful commands mvn spring-boot:run compiles and run spring boot application in exploded form (from target directory) To start a RestMock Server use the following gist and start it with spring run SpringRestMock.groovy initiate example package de.example; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.scheduling.annotation.EnableScheduling; import org.springframework.web.client.RestTemplate; @SpringBootApplication public class ExampleApplication { public static void main(String[] args) { SpringApplication.run(ExampleApplication.class, args); } } @SpringBootApplication adds the following annotations: @Configuration: Tags the class as a source of bean definitions for the application context. @EnableAutoConfiguration: Tells Spring Boot to start adding beans, based on classpath settings, other beans, and various property settings @ComponentScan: Tells Spring to look for other components, configurations , and services in the de.example package. auto configuration @EnableAutoConfiguration: configuration classes are scanned dynamically usually based on jars on classpath Conditional Loading: load (predefined) configurations if specified classes are on the classpath Properties: there are default properties for AutoConfiguration classes. May be overwritten. assumption : auto config is only relevant for -spring-boot-starter- dependencies Good video on insights on LinkedInLearn Property-based configuration set/overwrite predefined properties: application.properties or application.yml in src/main/resources folder(dev focused) Environment variables command line injections Cloud configuration (most common) You can define a property file fo the test cases by adding a file to src/test/resources Bean Configuration adding beans to default application class e.g. as inner class adding bean to separate configuration classes (@Import or autoscan) xml based config (legacy) component scanning (most common) and implemented Interfaces Profiles Profiles allow configuration based on environment (e.g. to set log-level or urls). They leverage the property-based-configuration feature. When using files a Application.yml is more common to use to leverage profiles than aapplication.properties. Yaml uses spring.profile to name a profile and --- to mark a section and all properties defined in that section belong to the specified profile. spring: profiles: dev server: port: 8000 --- spring: profiles: test server: port: 9000 When using application.properties you need to create one file per profile with the naming scheme: application- .properties. You can set active profile via spring.profiles.active environment-variable/jvm-property or directly in the properties/yaml file. Start spring with a specific profile -Dspring.profiles.active= e.g. ( java -jar -Dspring.profiles.active=default app.jar de.MainClass ) spring-boot-devtools spring-boot-devtools help rapid development triggered by changes on existing files on classpath ignores new files on classpath you may need to adjust IDE settings to work added via maven dependecy Output on console \"LiveReload Server is Running\" Also adds h2 console on http://localhost:8080/h2-console. You may need override the \"JDBC URL\" with the one from the startup console message from spring boot. If you do not use devtools and want the h2 console you need to add spring.h2.console.enabled=true property. packaging Standard: Jar with all dependencies included (Fat Jar, Shaded Jar) and also executable if war needed: set maven artifact to war exclude spring-bootstarter-tomcat from spring-boot-starter-web remember that spring-web and spring-boot-starter-web are different things! CommandLineRunner & ApplicationRunner can be used for plain programming stuff like admin tasks. are executed when application is started. need to implement the interface and overwrite the run method CommandLineRunner run method got plain string array of arguments as parameter. ApplicationRunner run method got ApplicationArguments Parameter which has convenience methods for typical command line arguments spring-boot-web-starter config of tomcat via: property or via @WebFilter, @WebServler, @WebListener annotated classes MVC pattern is used for ReST as well view: mimetype controller: Resource Controller (@RestController) model: Resource Representation Class spring boot data (spring-boot-starter-data-*) database drivers are configured by properties one database can be autoconfigured Controlling Database Creation: If spring finds a data.sql and a scheme.sql on classpath it will use it To prevent overwrite set the property spring.jpa.hibernate.ddl-auto=false In order to run test it is common to add com.h2database.h2 dependency in maven. actuator (spring-boot-starter-actuator) actuators... give insight into an application allow monitoring allows to change configuration settings via jmx libraries changed frequently in spring. actuator endpoints can be configured via properties management.endpoint.* you should activate security (ENDPOINT_ADMIN) only for dev deactivate all actuators by adding management.endoints.web.exposure.include=* property Predefined endpoints: Health endpoint application status status of dependencies (db, etc.) Info endpoint: maintainer git commit build number JXM Functions: list of bean state of environment dumps url mappings metrics spring boot security (spring-boot-starter-security) General note: use Bcrypt for password hashing. basic auth can be configured by properties default when security starter is not configured. forced for all urls. Username is: user password is logged in console at startup Form based auth configured by: extend WebSecurityConfigurerAdapter annotate @EnableWebSecurity to activate form-based Auth in favor of BasicAuth annotate @Configuration overwrite method configure to define where auth is needed you need to be cautious to consider every part of the path you need to allow access to login page AuthenticationManagerBuilder is used to config where passwords are stored (it is possible to save user and password in code) Use thymeleaf to create the authentication html form. Serve it with a @Controller and @GetMapping (Return a String with the Name of the thymeleaf file without file ending) OAuth2 Possible providers are GitHub, Google, Facebook etc. can be configured via properties. or via Java config: @EnableOAuth2Client is used to config a client @EnableAuthorizationServer is used to config a server Client dependency spring-boot-starter-oauth2-client","title":"Spring Boot"},{"location":"spring/1_spring-boot/#spring-boot","text":"","title":"Spring Boot"},{"location":"spring/1_spring-boot/#helpful-commands","text":"mvn spring-boot:run compiles and run spring boot application in exploded form (from target directory) To start a RestMock Server use the following gist and start it with spring run SpringRestMock.groovy","title":"helpful commands"},{"location":"spring/1_spring-boot/#initiate-example","text":"package de.example; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.scheduling.annotation.EnableScheduling; import org.springframework.web.client.RestTemplate; @SpringBootApplication public class ExampleApplication { public static void main(String[] args) { SpringApplication.run(ExampleApplication.class, args); } } @SpringBootApplication adds the following annotations: @Configuration: Tags the class as a source of bean definitions for the application context. @EnableAutoConfiguration: Tells Spring Boot to start adding beans, based on classpath settings, other beans, and various property settings @ComponentScan: Tells Spring to look for other components, configurations , and services in the de.example package.","title":"initiate example"},{"location":"spring/1_spring-boot/#auto-configuration","text":"@EnableAutoConfiguration: configuration classes are scanned dynamically usually based on jars on classpath Conditional Loading: load (predefined) configurations if specified classes are on the classpath Properties: there are default properties for AutoConfiguration classes. May be overwritten. assumption : auto config is only relevant for -spring-boot-starter- dependencies Good video on insights on LinkedInLearn","title":"auto configuration"},{"location":"spring/1_spring-boot/#property-based-configuration","text":"set/overwrite predefined properties: application.properties or application.yml in src/main/resources folder(dev focused) Environment variables command line injections Cloud configuration (most common) You can define a property file fo the test cases by adding a file to src/test/resources","title":"Property-based configuration"},{"location":"spring/1_spring-boot/#bean-configuration","text":"adding beans to default application class e.g. as inner class adding bean to separate configuration classes (@Import or autoscan) xml based config (legacy) component scanning (most common) and implemented Interfaces","title":"Bean Configuration"},{"location":"spring/1_spring-boot/#profiles","text":"Profiles allow configuration based on environment (e.g. to set log-level or urls). They leverage the property-based-configuration feature. When using files a Application.yml is more common to use to leverage profiles than aapplication.properties. Yaml uses spring.profile to name a profile and --- to mark a section and all properties defined in that section belong to the specified profile. spring: profiles: dev server: port: 8000 --- spring: profiles: test server: port: 9000 When using application.properties you need to create one file per profile with the naming scheme: application- .properties. You can set active profile via spring.profiles.active environment-variable/jvm-property or directly in the properties/yaml file. Start spring with a specific profile -Dspring.profiles.active= e.g. ( java -jar -Dspring.profiles.active=default app.jar de.MainClass )","title":"Profiles"},{"location":"spring/1_spring-boot/#spring-boot-devtools","text":"spring-boot-devtools help rapid development triggered by changes on existing files on classpath ignores new files on classpath you may need to adjust IDE settings to work added via maven dependecy Output on console \"LiveReload Server is Running\" Also adds h2 console on http://localhost:8080/h2-console. You may need override the \"JDBC URL\" with the one from the startup console message from spring boot. If you do not use devtools and want the h2 console you need to add spring.h2.console.enabled=true property.","title":"spring-boot-devtools"},{"location":"spring/1_spring-boot/#packaging","text":"Standard: Jar with all dependencies included (Fat Jar, Shaded Jar) and also executable if war needed: set maven artifact to war exclude spring-bootstarter-tomcat from spring-boot-starter-web remember that spring-web and spring-boot-starter-web are different things!","title":"packaging"},{"location":"spring/1_spring-boot/#commandlinerunner-applicationrunner","text":"can be used for plain programming stuff like admin tasks. are executed when application is started. need to implement the interface and overwrite the run method CommandLineRunner run method got plain string array of arguments as parameter. ApplicationRunner run method got ApplicationArguments Parameter which has convenience methods for typical command line arguments","title":"CommandLineRunner &amp; ApplicationRunner"},{"location":"spring/1_spring-boot/#spring-boot-web-starter","text":"config of tomcat via: property or via @WebFilter, @WebServler, @WebListener annotated classes MVC pattern is used for ReST as well view: mimetype controller: Resource Controller (@RestController) model: Resource Representation Class","title":"spring-boot-web-starter"},{"location":"spring/1_spring-boot/#spring-boot-data-spring-boot-starter-data-","text":"database drivers are configured by properties one database can be autoconfigured Controlling Database Creation: If spring finds a data.sql and a scheme.sql on classpath it will use it To prevent overwrite set the property spring.jpa.hibernate.ddl-auto=false In order to run test it is common to add com.h2database.h2 dependency in maven.","title":"spring boot data (spring-boot-starter-data-*)"},{"location":"spring/1_spring-boot/#actuator-spring-boot-starter-actuator","text":"actuators... give insight into an application allow monitoring allows to change configuration settings via jmx libraries changed frequently in spring. actuator endpoints can be configured via properties management.endpoint.* you should activate security (ENDPOINT_ADMIN) only for dev deactivate all actuators by adding management.endoints.web.exposure.include=* property Predefined endpoints: Health endpoint application status status of dependencies (db, etc.) Info endpoint: maintainer git commit build number JXM Functions: list of bean state of environment dumps url mappings metrics","title":"actuator (spring-boot-starter-actuator)"},{"location":"spring/1_spring-boot/#spring-boot-security-spring-boot-starter-security","text":"General note: use Bcrypt for password hashing.","title":"spring boot security (spring-boot-starter-security)"},{"location":"spring/1_spring-boot/#basic-auth","text":"can be configured by properties default when security starter is not configured. forced for all urls. Username is: user password is logged in console at startup","title":"basic auth"},{"location":"spring/1_spring-boot/#form-based-auth","text":"configured by: extend WebSecurityConfigurerAdapter annotate @EnableWebSecurity to activate form-based Auth in favor of BasicAuth annotate @Configuration overwrite method configure to define where auth is needed you need to be cautious to consider every part of the path you need to allow access to login page AuthenticationManagerBuilder is used to config where passwords are stored (it is possible to save user and password in code) Use thymeleaf to create the authentication html form. Serve it with a @Controller and @GetMapping (Return a String with the Name of the thymeleaf file without file ending)","title":"Form based auth"},{"location":"spring/1_spring-boot/#oauth2","text":"Possible providers are GitHub, Google, Facebook etc. can be configured via properties. or via Java config: @EnableOAuth2Client is used to config a client @EnableAuthorizationServer is used to config a server Client dependency spring-boot-starter-oauth2-client","title":"OAuth2"},{"location":"spring/2_spring-boot-data/","text":"Spring Boot Data overview Spring data is like a generic entity access object. It functions as a layer above JPA or other ORM mapper specifications. It uses the Repository pattern The base interface is Repository , which expects a Entity T and a Type for the ID as generic. It functions as a marker for spring to find classes which extends it. CrudRepository (extends Repository) - gives crud methods JPARepository (extends CrudRepository) - adds flush and further jpa specific methods For each domain object you need: entity class a repository interface which extends Repository or a child of it initialize database The easiest way is to use initializer scripts. Save a the following files under resoures/: schema.sql for defining the table schemas data.sql for adding data to the tables If you want to use scheme.sql you should deactivate auto ddl via property spring.jpa.hibernate.ddl-auto=None . Auto DDL creates the tables according to the entity classes. transactions In order to activate transactions in spring data you should annotate a service class with @Transactional. All code whcih is called form hence on is transactional. if a exception occurs the whole commit in thr database is rolled back. query methods Query methods allow to search for entities in the database. You do not need to implement the query methods. Spring data will generate the necessary code at startup if the used syntax is correct (does not check semantic at startup) and it will throw an error if it not ok. Query methods support Optional as return type wrapper . You can use query methods: add method signatures which are conform to property expressions add @Query(\"\") annotation to a method signature use the standard methods of a repository interfaces The following examples are valid for use with jpa repository (and therefore relational databases). They may not be used on document based databases such as mongo db. Property Expressions Add methods signatures to your repository which name must follow the rules of Property Expression language. You do not need to implement the mehods. They will work out of the box. If the Property Expressions git Systax error spring will tell you at the startup of the server. The method signature must follow the rules of property expressions: return type of the searched entity \"findBy\" entity attribute name in camel case chain further findBy (optinal) Parameter which matches the type of the searched attributes. It is possible to search with primitive types as well as with references entity objects. Example method signature: public interface ExmapleRepository extends CrudInterface<Student,Integer> Student findByStudentId(Integer id); Student findByStudentIdAndAge(Integer studentId, Integer age); List<Student> findByAgeGreaterThan(int minimumAge); List<Student> findByAgeLessThan(int maximumAge); List<Student> findByLastNameLike(String likeString); List<Student> findByLastNameIgnoreCase(String lastName); Student findFirstByOrderByLastNameAsc(); Student findTopByOrderByAgeDesc(); List<Student> findTop3ByOrderByAgeDesc(); @Entity @Table(name=\"STUDENT\") public class Student { @Id @GeneratedValue private Integer studentId; @Column private boolean fullTime; @Column private Integer age; @Column private String lastName // getter and setter ... } Valid return types ( source ): void Primitives Wrapper types T Iterator Collection List Optional Option Stream Streamable Types that implement Streamable and take a Streamable constructor or factory method argument Vavr Seq, List, Map, Set Future CompletableFuture ListenableFuture Slice Page GeoResult GeoResults GeoPage Mono Flux Single Maybe Flowable @Query annotation Annotate a method with the @Query annotation. The method signature must contain a correct return type and parameter if (named /numbered) query parameters are used (:id / ?1). The parameter must be annotated with @Param if named query parameters are used. can execute JPQL @Query(\"JPQL query\") or SQL @Query(value=\"SQL query\", nativeQuery=true) paging and sorting paging an sorting can be achieved by: implementing the PagingAndSortingRepository<> repo (only standard queries are pagable and sortable) or manually by: add Parameter of type Pageable to method Return Page<> object from method when calling method use PageRequest.of() to create Object of type Pageable Query by example search for objects similar to a another one works with all data sources limited features To use need to extends QueryByExampleExecutor Interface. List<Teacher> cologneTeacher = teacherRepository.findOne(Example.of(new Teacher(null, \"Cologne\"))); // or List<Teacher> cologneTeacher = teacherRepository.findOne(new Teacher(null, \"Cologne\"), ExampleMatcher.matching(). ignoreCase(). withStringMatcher(StringMatcher.ENDING)); queryDSL Another way to query data. You can use the QueryDslPredicateExecutor to execute so called BooleanExpressions. Spring data REST Spring data rest exposes the query methods for every entity as rest webservices. To use add spring-boot-starter-data-rest dependency to the project. Finds all spring data repositories creates endpoint for every entity appends an s exposes the repository methods as a Rest Webservice custom methods can be found unter s/search/ The mapping is described here . Repository detection strategies: default: expose all public repository interfaces, but respects exported flag of @(Repository)RestResource annotation all: expose all repository interfaces annotated: only repositories annotated with @(Repository)RestResource (unless exported flag is set to false) visibility: only public repositories auditing logging when entity is created and who created it. Use Annotation on fields: @CreatedDate @LastModifiedBy or extend the abstract AbstractAuditable<> class for user info you need to wire up the AuditorAware<> interface from spring security spring config server If a spring config server is used, the clients must have the following things set: spring.application.name which corresponds to the the directory in the config git Repository in which the appliyation.yaml lays spring.cloud.config.server.git.uri which points to the endpoit The spring-cloud-dependencies an a explizit version for it further notable repositories [Full list] spring data ldap JDBC Repository - for direct access to db via sql (without features like caching or lazy loading) reactive repositories - uses non blocking calls (which run in a eventloop inside one thread just like in javascript) GemFire spring data key value (high level api for key-value store like redis) spring data redis (usable as database, cache, and message broker) MongoDb Repository spring data apache cassandra spring data apache solr (search engine) Community modules listed under","title":"Spring Boot Data"},{"location":"spring/2_spring-boot-data/#spring-boot-data","text":"","title":"Spring Boot Data"},{"location":"spring/2_spring-boot-data/#overview","text":"Spring data is like a generic entity access object. It functions as a layer above JPA or other ORM mapper specifications. It uses the Repository pattern The base interface is Repository , which expects a Entity T and a Type for the ID as generic. It functions as a marker for spring to find classes which extends it. CrudRepository (extends Repository) - gives crud methods JPARepository (extends CrudRepository) - adds flush and further jpa specific methods For each domain object you need: entity class a repository interface which extends Repository or a child of it","title":"overview"},{"location":"spring/2_spring-boot-data/#initialize-database","text":"The easiest way is to use initializer scripts. Save a the following files under resoures/: schema.sql for defining the table schemas data.sql for adding data to the tables If you want to use scheme.sql you should deactivate auto ddl via property spring.jpa.hibernate.ddl-auto=None . Auto DDL creates the tables according to the entity classes.","title":"initialize database"},{"location":"spring/2_spring-boot-data/#transactions","text":"In order to activate transactions in spring data you should annotate a service class with @Transactional. All code whcih is called form hence on is transactional. if a exception occurs the whole commit in thr database is rolled back.","title":"transactions"},{"location":"spring/2_spring-boot-data/#query-methods","text":"Query methods allow to search for entities in the database. You do not need to implement the query methods. Spring data will generate the necessary code at startup if the used syntax is correct (does not check semantic at startup) and it will throw an error if it not ok. Query methods support Optional as return type wrapper . You can use query methods: add method signatures which are conform to property expressions add @Query(\"\") annotation to a method signature use the standard methods of a repository interfaces The following examples are valid for use with jpa repository (and therefore relational databases). They may not be used on document based databases such as mongo db.","title":"query methods"},{"location":"spring/2_spring-boot-data/#property-expressions","text":"Add methods signatures to your repository which name must follow the rules of Property Expression language. You do not need to implement the mehods. They will work out of the box. If the Property Expressions git Systax error spring will tell you at the startup of the server. The method signature must follow the rules of property expressions: return type of the searched entity \"findBy\" entity attribute name in camel case chain further findBy (optinal) Parameter which matches the type of the searched attributes. It is possible to search with primitive types as well as with references entity objects. Example method signature: public interface ExmapleRepository extends CrudInterface<Student,Integer> Student findByStudentId(Integer id); Student findByStudentIdAndAge(Integer studentId, Integer age); List<Student> findByAgeGreaterThan(int minimumAge); List<Student> findByAgeLessThan(int maximumAge); List<Student> findByLastNameLike(String likeString); List<Student> findByLastNameIgnoreCase(String lastName); Student findFirstByOrderByLastNameAsc(); Student findTopByOrderByAgeDesc(); List<Student> findTop3ByOrderByAgeDesc(); @Entity @Table(name=\"STUDENT\") public class Student { @Id @GeneratedValue private Integer studentId; @Column private boolean fullTime; @Column private Integer age; @Column private String lastName // getter and setter ... } Valid return types ( source ): void Primitives Wrapper types T Iterator Collection List Optional Option Stream Streamable Types that implement Streamable and take a Streamable constructor or factory method argument Vavr Seq, List, Map, Set Future CompletableFuture ListenableFuture Slice Page GeoResult GeoResults GeoPage Mono Flux Single Maybe Flowable","title":"Property Expressions"},{"location":"spring/2_spring-boot-data/#query-annotation","text":"Annotate a method with the @Query annotation. The method signature must contain a correct return type and parameter if (named /numbered) query parameters are used (:id / ?1). The parameter must be annotated with @Param if named query parameters are used. can execute JPQL @Query(\"JPQL query\") or SQL @Query(value=\"SQL query\", nativeQuery=true)","title":"@Query annotation"},{"location":"spring/2_spring-boot-data/#paging-and-sorting","text":"paging an sorting can be achieved by: implementing the PagingAndSortingRepository<> repo (only standard queries are pagable and sortable) or manually by: add Parameter of type Pageable to method Return Page<> object from method when calling method use PageRequest.of() to create Object of type Pageable","title":"paging and sorting"},{"location":"spring/2_spring-boot-data/#query-by-example","text":"search for objects similar to a another one works with all data sources limited features To use need to extends QueryByExampleExecutor Interface. List<Teacher> cologneTeacher = teacherRepository.findOne(Example.of(new Teacher(null, \"Cologne\"))); // or List<Teacher> cologneTeacher = teacherRepository.findOne(new Teacher(null, \"Cologne\"), ExampleMatcher.matching(). ignoreCase(). withStringMatcher(StringMatcher.ENDING));","title":"Query by example"},{"location":"spring/2_spring-boot-data/#querydsl","text":"Another way to query data. You can use the QueryDslPredicateExecutor to execute so called BooleanExpressions.","title":"queryDSL"},{"location":"spring/2_spring-boot-data/#spring-data-rest","text":"Spring data rest exposes the query methods for every entity as rest webservices. To use add spring-boot-starter-data-rest dependency to the project. Finds all spring data repositories creates endpoint for every entity appends an s exposes the repository methods as a Rest Webservice custom methods can be found unter s/search/ The mapping is described here . Repository detection strategies: default: expose all public repository interfaces, but respects exported flag of @(Repository)RestResource annotation all: expose all repository interfaces annotated: only repositories annotated with @(Repository)RestResource (unless exported flag is set to false) visibility: only public repositories","title":"Spring data REST"},{"location":"spring/2_spring-boot-data/#auditing","text":"logging when entity is created and who created it. Use Annotation on fields: @CreatedDate @LastModifiedBy or extend the abstract AbstractAuditable<> class for user info you need to wire up the AuditorAware<> interface from spring security","title":"auditing"},{"location":"spring/2_spring-boot-data/#spring-config-server","text":"If a spring config server is used, the clients must have the following things set: spring.application.name which corresponds to the the directory in the config git Repository in which the appliyation.yaml lays spring.cloud.config.server.git.uri which points to the endpoit The spring-cloud-dependencies an a explizit version for it","title":"spring config server"},{"location":"spring/2_spring-boot-data/#further-notable-repositories","text":"[Full list] spring data ldap JDBC Repository - for direct access to db via sql (without features like caching or lazy loading) reactive repositories - uses non blocking calls (which run in a eventloop inside one thread just like in javascript) GemFire spring data key value (high level api for key-value store like redis) spring data redis (usable as database, cache, and message broker) MongoDb Repository spring data apache cassandra spring data apache solr (search engine) Community modules listed under","title":"further notable repositories"},{"location":"spring/3_spring-rest/","text":"Spring Boot rest (with HATEOAS) Example of Unittest for SpringBoot Rest Controller rest general principles Representational State Transfer (REST) is a collection of (six) architectural constraints: Client-server architecture Statelessness Cacheability Layered system Code on demand Uniform interface Resource identification in requests (e.g. URI) Resource manipulation through representations (client can use crud on a resource via one of its representations) Self-descriptive messages (e.g. MediaType) Hypermedia as the Engine of Application State (HATEOAS) Hypermedia as the Engine of Application State (HATEOAS): you can find all available resources through the publication of links which point to these resources. Like a landing page of a website which leads to all further links on the website. spring rest (via spring mvc) Official hands on guide with emphasis on hateoas Nice overview of topic related to rest needs spring-boot-starter-web as dependecy (not spring-boot-starter-jersey which is Jax_RS (Java EE)) annotate class with @RestController (shorthand for @ResponseBody and @Controller) and @RequestMapping(\"/\") use following annotations to mark a method to be exposed via @GetMapping @PostMapping @PutMapping @DeleteMapping @PatchMapping RequestMapping (generic mapper/ old way) the @*Mapping annotations allows: to define which http verb is used value=\"\" . to define under which path the method is called (based on the path defined in @RequestMapping on the class) produces= : to define which representation formats are allowed to to define path parameters e.g. @GetMapping(\"/{id}/specialpath\") @ResponseStatus on a method defines the http code for a successful call On method parameters: @RequestParam defines it to be a query-string (?id=...&name=...) @PathVariable defines it as a path variable (http://rooturl.de/books/myPathVariable/) @RequestBody binds the parameter to the body of the HTTP (usually used by http post) @ModelAttribute for complex objects as query params or post bodys note: @PathParam and @QueryParam, are the jax-rs annotations spring hateoas (HAL by default) add spring-boot-starter-hateoas dependency to add HATEOAS support. You have multiple options to create a RepresentationModel which can hold links: create a class which extends RepresentationModel . (to set the name of the entity in the representation use the org.springframework.hateoas.server.core.Relation Annotation) wrap a collection in a CollectionModel use EntityModel to wraps an existing class ( EntityModel.of(person) ) add a selfLink and optional further links to the RepresentationModels you want to return use Methods from WebMvcLinkBuilder like linkTo() to return _links to relevant operations implement RepresentationModelAssembler and the toModel method for every entity (in order to remove the code from the RestController) LEGACY: Spring HATEOAS Version 1.0 dropped the resourceSupport/Resource/Resources/PagedResources group of classes error handling @ControllerAdvice works on global level for all controller beans and allows to save all @ExceptionHandler in one place ResponseStatusException allows to define error handling for one method @ExceptionHandler works only for the class where a method is annotated with. HandlerExceptionResolver can be extended or just use predefined implementations Spring also support \"Problem Details for HTTP APIs\" (RFC 7807 and RFC 9457) spring.mvc.problemdetails.enabled Example here parsing date query parameters on parameter level @DateTimeFormat(pattern=\"MM/dd/yyyy\") before the parameter or @DateTimeFormat( iso=DateTimeFormat.ISO.<>) before the parameter on application level @Configuration(proxyBeanMethods = false) class RestConfiguration { public DateTimeFormatterRegistrar registrar() { DateTimeFormatterRegistrar registrar = new DateTimeFormatterRegistrar() registrar.setDateFormatter(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")); registrar.setTimeFormatter(DateTimeFormatter.ofPattern(\"HH:mm:ss\")); registrar.setDateTimeFormatter(DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss\")); return registrar; } } Set default value for a LocalDate param methodname(@RequestParam(defaultValue=\"#{T(java.time.LocalDate).now()}\") LocalDate date) calls to <resturl>?requestParam= or <resturl>?requestParam=&requestParam2=bla or just <resturl> would trigger the default value cache control Nice overview evolving api Add new fields to your JSON representations, but do not take any away (Never delete a column in a database) Test RestController Use TestRestTemplate and just call the endpoints. Only make rudimentary assertions or such which can only be done on Rest like error page. Test the real logic in the service classs which provides the information to the restcontroller. spring data rest official documentation When adding spring-boot-starter-data-rest as dependency to your spring boot project all spring data repositories will be exposed via rest api. by default api root is under / root can be changed via spring.data.rest.basePath property A domain class is reachable on the path under its: (example Order is reachable under /orders) uncapitalized pluralized (added s) simple class name to prevent spring from exporting a repository add @RestResource(exported = false) to class or to a query method (you may overwrite them in your interface in order to annotate them) by default spring data rest uses HATEOS via HAL. When you call the basePath of spring data rest in a browser you get a overview of the exposed rest api. Paging and sorting is supported via query parameters when a PagingAndSortingRepository is used ?size=5 ?page=2 ?sort=name,desc custom query methods in Repository classes are exposed as well you can configure these with @RequestParam and @PathVariable as you would do with regular methods in a RestController if you want to use a @Query annotation at the same time use @Param and reference the paramname with leading colons (e.g. :date). if you do not configure a @PathVariable they are exposed under <host>/<entityPlural>/search/<queryMethodName> (they appear in the hal browser!) you can define generic rest query urls for a spring data repository with the Specification interface add basic auth @Configuration public class WebserviceConfig { @Value(\"${}) private String user; @Value(\"${}) private String password; // is used for all restTemplates! You need to name the bean if you want to call mltiple domains @Bean public RestTemplate myRestTemplate(RestTemplateBuilder builder) { return builder.basicAuthorization(user, password).build(); } } prevent circular dependecies in generated JSON Use @JsonManagedReference and @JsonBackReference see here . Misc General adive on spell you can: - call a static Method with spell: #{T(java.time.LocalDate).now()} - call a constructor with spell: #{new java.util Date()}","title":"Spring Boot rest (with HATEOAS)"},{"location":"spring/3_spring-rest/#spring-boot-rest-with-hateoas","text":"Example of Unittest for SpringBoot Rest Controller","title":"Spring Boot rest (with HATEOAS)"},{"location":"spring/3_spring-rest/#rest-general-principles","text":"Representational State Transfer (REST) is a collection of (six) architectural constraints: Client-server architecture Statelessness Cacheability Layered system Code on demand Uniform interface Resource identification in requests (e.g. URI) Resource manipulation through representations (client can use crud on a resource via one of its representations) Self-descriptive messages (e.g. MediaType) Hypermedia as the Engine of Application State (HATEOAS) Hypermedia as the Engine of Application State (HATEOAS): you can find all available resources through the publication of links which point to these resources. Like a landing page of a website which leads to all further links on the website.","title":"rest general principles"},{"location":"spring/3_spring-rest/#spring-rest-via-spring-mvc","text":"Official hands on guide with emphasis on hateoas Nice overview of topic related to rest needs spring-boot-starter-web as dependecy (not spring-boot-starter-jersey which is Jax_RS (Java EE)) annotate class with @RestController (shorthand for @ResponseBody and @Controller) and @RequestMapping(\"/\") use following annotations to mark a method to be exposed via @GetMapping @PostMapping @PutMapping @DeleteMapping @PatchMapping RequestMapping (generic mapper/ old way) the @*Mapping annotations allows: to define which http verb is used value=\"\" . to define under which path the method is called (based on the path defined in @RequestMapping on the class) produces= : to define which representation formats are allowed to to define path parameters e.g. @GetMapping(\"/{id}/specialpath\") @ResponseStatus on a method defines the http code for a successful call On method parameters: @RequestParam defines it to be a query-string (?id=...&name=...) @PathVariable defines it as a path variable (http://rooturl.de/books/myPathVariable/) @RequestBody binds the parameter to the body of the HTTP (usually used by http post) @ModelAttribute for complex objects as query params or post bodys note: @PathParam and @QueryParam, are the jax-rs annotations","title":"spring rest (via spring mvc)"},{"location":"spring/3_spring-rest/#spring-hateoas-hal-by-default","text":"add spring-boot-starter-hateoas dependency to add HATEOAS support. You have multiple options to create a RepresentationModel which can hold links: create a class which extends RepresentationModel . (to set the name of the entity in the representation use the org.springframework.hateoas.server.core.Relation Annotation) wrap a collection in a CollectionModel use EntityModel to wraps an existing class ( EntityModel.of(person) ) add a selfLink and optional further links to the RepresentationModels you want to return use Methods from WebMvcLinkBuilder like linkTo() to return _links to relevant operations implement RepresentationModelAssembler and the toModel method for every entity (in order to remove the code from the RestController) LEGACY: Spring HATEOAS Version 1.0 dropped the resourceSupport/Resource/Resources/PagedResources group of classes","title":"spring hateoas (HAL by default)"},{"location":"spring/3_spring-rest/#error-handling","text":"@ControllerAdvice works on global level for all controller beans and allows to save all @ExceptionHandler in one place ResponseStatusException allows to define error handling for one method @ExceptionHandler works only for the class where a method is annotated with. HandlerExceptionResolver can be extended or just use predefined implementations Spring also support \"Problem Details for HTTP APIs\" (RFC 7807 and RFC 9457) spring.mvc.problemdetails.enabled Example here","title":"error handling"},{"location":"spring/3_spring-rest/#parsing-date-query-parameters","text":"","title":"parsing date query parameters"},{"location":"spring/3_spring-rest/#on-parameter-level","text":"@DateTimeFormat(pattern=\"MM/dd/yyyy\") before the parameter or @DateTimeFormat( iso=DateTimeFormat.ISO.<>) before the parameter","title":"on parameter level"},{"location":"spring/3_spring-rest/#on-application-level","text":"@Configuration(proxyBeanMethods = false) class RestConfiguration { public DateTimeFormatterRegistrar registrar() { DateTimeFormatterRegistrar registrar = new DateTimeFormatterRegistrar() registrar.setDateFormatter(DateTimeFormatter.ofPattern(\"yyyy-MM-dd\")); registrar.setTimeFormatter(DateTimeFormatter.ofPattern(\"HH:mm:ss\")); registrar.setDateTimeFormatter(DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss\")); return registrar; } } Set default value for a LocalDate param methodname(@RequestParam(defaultValue=\"#{T(java.time.LocalDate).now()}\") LocalDate date) calls to <resturl>?requestParam= or <resturl>?requestParam=&requestParam2=bla or just <resturl> would trigger the default value","title":"on application level"},{"location":"spring/3_spring-rest/#cache-control","text":"Nice overview","title":"cache control"},{"location":"spring/3_spring-rest/#evolving-api","text":"Add new fields to your JSON representations, but do not take any away (Never delete a column in a database)","title":"evolving api"},{"location":"spring/3_spring-rest/#test-restcontroller","text":"Use TestRestTemplate and just call the endpoints. Only make rudimentary assertions or such which can only be done on Rest like error page. Test the real logic in the service classs which provides the information to the restcontroller.","title":"Test RestController"},{"location":"spring/3_spring-rest/#spring-data-rest","text":"official documentation When adding spring-boot-starter-data-rest as dependency to your spring boot project all spring data repositories will be exposed via rest api. by default api root is under / root can be changed via spring.data.rest.basePath property A domain class is reachable on the path under its: (example Order is reachable under /orders) uncapitalized pluralized (added s) simple class name to prevent spring from exporting a repository add @RestResource(exported = false) to class or to a query method (you may overwrite them in your interface in order to annotate them) by default spring data rest uses HATEOS via HAL. When you call the basePath of spring data rest in a browser you get a overview of the exposed rest api. Paging and sorting is supported via query parameters when a PagingAndSortingRepository is used ?size=5 ?page=2 ?sort=name,desc custom query methods in Repository classes are exposed as well you can configure these with @RequestParam and @PathVariable as you would do with regular methods in a RestController if you want to use a @Query annotation at the same time use @Param and reference the paramname with leading colons (e.g. :date). if you do not configure a @PathVariable they are exposed under <host>/<entityPlural>/search/<queryMethodName> (they appear in the hal browser!) you can define generic rest query urls for a spring data repository with the Specification interface","title":"spring data rest"},{"location":"spring/3_spring-rest/#add-basic-auth","text":"@Configuration public class WebserviceConfig { @Value(\"${}) private String user; @Value(\"${}) private String password; // is used for all restTemplates! You need to name the bean if you want to call mltiple domains @Bean public RestTemplate myRestTemplate(RestTemplateBuilder builder) { return builder.basicAuthorization(user, password).build(); } }","title":"add basic auth"},{"location":"spring/3_spring-rest/#prevent-circular-dependecies-in-generated-json","text":"Use @JsonManagedReference and @JsonBackReference see here .","title":"prevent circular dependecies in generated JSON"},{"location":"spring/3_spring-rest/#misc","text":"General adive on spell you can: - call a static Method with spell: #{T(java.time.LocalDate).now()} - call a constructor with spell: #{new java.util Date()}","title":"Misc"},{"location":"spring/Swagger-ui/","text":"Spring swagger ui URL: http://localhost:8080/swagger-ui/index.htm activate in application.properties springdoc.api-docs.enabled=true springdoc.swagger-ui.enabled=true springdoc.swagger-ui.disable-swagger-default-url=true","title":"Spring swagger ui"},{"location":"spring/Swagger-ui/#spring-swagger-ui","text":"URL: http://localhost:8080/swagger-ui/index.htm activate in application.properties springdoc.api-docs.enabled=true springdoc.swagger-ui.enabled=true springdoc.swagger-ui.disable-swagger-default-url=true","title":"Spring swagger ui"},{"location":"spring/spring-boot-docker/","text":"Spring Boot Dockerize official docker guide from spring.io . official docker getting started from spring.io . dockerfile + fatjar easy setup dockerfile + exploded jar leverages layer system of docker dockerfile multistage build (build + execution in container) allows build to run everywhere where docker is installed google jib (maven + gradle plugins) no docker installation needed on machine where the build runs no dockerfile needed opinionated about usage of docker layers dockerfile + fatjar FROM openjdk:14-jdk-slim ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} myApp.jar ENTRYPOINT [\"java\",\"-jar\",\"/myApp.jar\"] dockerfile + exploded jar To explode a jar: jar -xf myapp.jar to execute a exploded jar you can start JarLauncher: java org.springframework.boot.loader.JarLauncher Your main class: java -cp BOOT-INF/classes:BOOT-INF/lib/* de.example.MyApplication Dokerfile to use exploded jar: FROM openjdk:8-jre-alpine ARG DEPENDENCY=target/dependency COPY --from=builder ${DEPENDENCY}/BOOT-INF/lib /app/lib COPY --from=builder ${DEPENDENCY}/META-INF /app/META-INF COPY --from=builder ${DEPENDENCY}/BOOT-INF/classes /app ENTRYPOINT [\"java\",\"-cp\",\"app:app/lib/*\",\"de.example.MyApplication\"] dockerfile multistage build (build + execution in container) build jar in multistage build: FROM openjdk:8-jdk-alpine as build WORKDIR /workspace/app COPY mvnw . COPY .mvn .mvn COPY pom.xml . COPY src src RUN ./mvnw install -DskipTests FROM openjdk:8-jdk-alpine ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} myApp.jar ENTRYPOINT [\"java\",\"-jar\",\"/myApp.jar\"] explode jar in multistage build: FROM openjdk:8-jdk-alpine AS builder WORKDIR target/dependency ARG APPJAR=target/*.jar COPY ${APPJAR} app.jar RUN jar -xf ./app.jar FROM openjdk:8-jre-alpine VOLUME /tmp ARG DEPENDENCY=target/dependency COPY --from=builder ${DEPENDENCY}/BOOT-INF/lib /app/lib COPY --from=builder ${DEPENDENCY}/META-INF /app/META-INF COPY --from=builder ${DEPENDENCY}/BOOT-INF/classes /app ENTRYPOINT [\"java\",\"-cp\",\"app:app/lib/*\",\"com.example.MyApplication\"] the maven wrapper seems to ignore http proxies which are set with the http_proxy or the https_proxy environmental variable.Instead you can set a proxy for maven with MAVEN_OPTS environmental variables: export MAVEN_OPTS=-Dhttp.proxyHost=bla - DhttpProxyPort=80 -Dhttps.proxyHost=bla - DhttpsProxyPort=80 google jib (maven + gradle plugins) google jib maven plugin gradle plugin You do not need to install docker on the client to create a docker image with jib. devtools Developer tools are automatically disabled when running a fully packaged application. Launch via java -jar or special classloader is considered a production environment so they are deactivated. source . This will have effects on several property defaults like spring.h2.console.enabled . For full list see .","title":"Spring Boot Dockerize"},{"location":"spring/spring-boot-docker/#spring-boot-dockerize","text":"official docker guide from spring.io . official docker getting started from spring.io . dockerfile + fatjar easy setup dockerfile + exploded jar leverages layer system of docker dockerfile multistage build (build + execution in container) allows build to run everywhere where docker is installed google jib (maven + gradle plugins) no docker installation needed on machine where the build runs no dockerfile needed opinionated about usage of docker layers","title":"Spring Boot Dockerize"},{"location":"spring/spring-boot-docker/#dockerfile-fatjar","text":"FROM openjdk:14-jdk-slim ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} myApp.jar ENTRYPOINT [\"java\",\"-jar\",\"/myApp.jar\"]","title":"dockerfile + fatjar"},{"location":"spring/spring-boot-docker/#dockerfile-exploded-jar","text":"To explode a jar: jar -xf myapp.jar to execute a exploded jar you can start JarLauncher: java org.springframework.boot.loader.JarLauncher Your main class: java -cp BOOT-INF/classes:BOOT-INF/lib/* de.example.MyApplication Dokerfile to use exploded jar: FROM openjdk:8-jre-alpine ARG DEPENDENCY=target/dependency COPY --from=builder ${DEPENDENCY}/BOOT-INF/lib /app/lib COPY --from=builder ${DEPENDENCY}/META-INF /app/META-INF COPY --from=builder ${DEPENDENCY}/BOOT-INF/classes /app ENTRYPOINT [\"java\",\"-cp\",\"app:app/lib/*\",\"de.example.MyApplication\"]","title":"dockerfile + exploded jar"},{"location":"spring/spring-boot-docker/#dockerfile-multistage-build-build-execution-in-container","text":"build jar in multistage build: FROM openjdk:8-jdk-alpine as build WORKDIR /workspace/app COPY mvnw . COPY .mvn .mvn COPY pom.xml . COPY src src RUN ./mvnw install -DskipTests FROM openjdk:8-jdk-alpine ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} myApp.jar ENTRYPOINT [\"java\",\"-jar\",\"/myApp.jar\"] explode jar in multistage build: FROM openjdk:8-jdk-alpine AS builder WORKDIR target/dependency ARG APPJAR=target/*.jar COPY ${APPJAR} app.jar RUN jar -xf ./app.jar FROM openjdk:8-jre-alpine VOLUME /tmp ARG DEPENDENCY=target/dependency COPY --from=builder ${DEPENDENCY}/BOOT-INF/lib /app/lib COPY --from=builder ${DEPENDENCY}/META-INF /app/META-INF COPY --from=builder ${DEPENDENCY}/BOOT-INF/classes /app ENTRYPOINT [\"java\",\"-cp\",\"app:app/lib/*\",\"com.example.MyApplication\"] the maven wrapper seems to ignore http proxies which are set with the http_proxy or the https_proxy environmental variable.Instead you can set a proxy for maven with MAVEN_OPTS environmental variables: export MAVEN_OPTS=-Dhttp.proxyHost=bla - DhttpProxyPort=80 -Dhttps.proxyHost=bla - DhttpsProxyPort=80","title":"dockerfile multistage build (build + execution in container)"},{"location":"spring/spring-boot-docker/#google-jib-maven-gradle-plugins","text":"google jib maven plugin gradle plugin You do not need to install docker on the client to create a docker image with jib.","title":"google jib (maven + gradle plugins)"},{"location":"spring/spring-boot-docker/#devtools","text":"Developer tools are automatically disabled when running a fully packaged application. Launch via java -jar or special classloader is considered a production environment so they are deactivated. source . This will have effects on several property defaults like spring.h2.console.enabled . For full list see .","title":"devtools"},{"location":"spring/spring-externalize-k8s-secret-config/","text":"Load mounted secrets as spring boot properties motivation Sometimes it is forbidden by platform or security teams to consume secrets as environments varibales (which would be the easiest way). Mounting secrets is generally considered (a little bit) more secure than using them as environmental variables in Kubernetes. Here's why: Environmental Variables: Exposure in Logs : When an application logs information about its environment, which can be common during debugging or errors, any secret values set as environment variables could be accidentally leaked. Process Visibility : Any process running within the pod can potentially access environment variables, which could be a security risk if a process has elevated privileges. Mounted Secrets: Limited Access : Secrets mounted as files are typically only accessible to the application process itself, reducing the risk of unauthorized access. No Accidental Leaks : Since the secret value isn't directly embedded in the application code or configuration, accidental leaks through logging or code exposure are less. solutions utilizing secrets You can test the following solutions in the PoprertyLogger projekt inside your boilerplate repo . You got the following secrets: apiVersion: v1 kind: Secret metadata: name: secret-properties stringData: spring.datasource.url: jdbc://bla.de password.super: secret########### --- apiVersion: v1 kind: Secret metadata: name: secret-properties-2 stringData: second_secret_entry: also_secret Import with with spring.config.import=configtree:/ noteworthy: runs out of the box is able to read property keys from directory paths + filename or only from filename # e.g. Deplyoment #... env: - name: SPRING_CONFIG_IMPORT # use * to use all directories on this level and start searching one layer deeper value: \"optional:configtree:/app/config/*/\" volumeMounts: - mountPath: /app/config/firstsecret/ name: secret-properties - mountPath: /app/config/secondsecret/ name: secret-properties-2 volumes: - name: secret-properties secret: secretName: secret-properties # using path items: # use same property - key: spring.datasource.url path: spring/datasource/url # remap property - key: password.super path: spring/datasource/password - name: secret-properties-2 secret: secretName: secret-properties-2 # using plain files items: # remap property - key: second_secret_entry path: second.secret.entry DEPRECATED Import with k spring.cloud.kubernetes.secrets.paths DEPRECATED in favor of spring.config.import see issue . noteworthy: The spring boot application needs the following dependency: org.springframework.cloud:spring-cloud-starter-kubernetes-client-config spring.cloud.kubernetes.secrets.paths searches recursivly so you can create one mountpoint where every secret is mounted if you want to deactivate other features: management.health.kubernetes.enabled: false spring.cloud.kubernetes.config.enabled: true Example yaml and conf: # e.g. Deplyoment #... env: # to prevent automatic config map discovery and stacktraces - name: SPRING_CLOUD_KUBERNETES_CONFIG_ENABLED value: \"false\" # searches recursivly - name: SPRING_CLOUD_KUBERNETES_SECRETS_PATHS value: /app/secrets/ volumeMounts: # this create one file per secret entry - name: secret-properties mountPath: /app/secrets/firstsecret/ - name: secret-properties-2 mountPath: /app/secrets/secondsecret/ volumes: - name: secret-properties secret: secretName: secret-properties - name: secret-properties-2 secret: secretName: secret-properties-2 Addtional application.yaml with spring.config.additional-location TODO programmatically with @ConfigurationProperties TODO alternatives without secrets spring cloud config server TODO but my own expirience with this is approach is bulky. ? TODO what other mechanismn/operators are there?","title":"Load mounted secrets as spring boot properties"},{"location":"spring/spring-externalize-k8s-secret-config/#load-mounted-secrets-as-spring-boot-properties","text":"","title":"Load mounted secrets as spring boot properties"},{"location":"spring/spring-externalize-k8s-secret-config/#motivation","text":"Sometimes it is forbidden by platform or security teams to consume secrets as environments varibales (which would be the easiest way). Mounting secrets is generally considered (a little bit) more secure than using them as environmental variables in Kubernetes. Here's why: Environmental Variables: Exposure in Logs : When an application logs information about its environment, which can be common during debugging or errors, any secret values set as environment variables could be accidentally leaked. Process Visibility : Any process running within the pod can potentially access environment variables, which could be a security risk if a process has elevated privileges. Mounted Secrets: Limited Access : Secrets mounted as files are typically only accessible to the application process itself, reducing the risk of unauthorized access. No Accidental Leaks : Since the secret value isn't directly embedded in the application code or configuration, accidental leaks through logging or code exposure are less.","title":"motivation"},{"location":"spring/spring-externalize-k8s-secret-config/#solutions-utilizing-secrets","text":"You can test the following solutions in the PoprertyLogger projekt inside your boilerplate repo . You got the following secrets: apiVersion: v1 kind: Secret metadata: name: secret-properties stringData: spring.datasource.url: jdbc://bla.de password.super: secret########### --- apiVersion: v1 kind: Secret metadata: name: secret-properties-2 stringData: second_secret_entry: also_secret","title":"solutions utilizing secrets"},{"location":"spring/spring-externalize-k8s-secret-config/#import-with-with-springconfigimportconfigtree","text":"noteworthy: runs out of the box is able to read property keys from directory paths + filename or only from filename # e.g. Deplyoment #... env: - name: SPRING_CONFIG_IMPORT # use * to use all directories on this level and start searching one layer deeper value: \"optional:configtree:/app/config/*/\" volumeMounts: - mountPath: /app/config/firstsecret/ name: secret-properties - mountPath: /app/config/secondsecret/ name: secret-properties-2 volumes: - name: secret-properties secret: secretName: secret-properties # using path items: # use same property - key: spring.datasource.url path: spring/datasource/url # remap property - key: password.super path: spring/datasource/password - name: secret-properties-2 secret: secretName: secret-properties-2 # using plain files items: # remap property - key: second_secret_entry path: second.secret.entry","title":"Import with with spring.config.import=configtree:/"},{"location":"spring/spring-externalize-k8s-secret-config/#deprecated-import-with-k-springcloudkubernetessecretspaths","text":"DEPRECATED in favor of spring.config.import see issue . noteworthy: The spring boot application needs the following dependency: org.springframework.cloud:spring-cloud-starter-kubernetes-client-config spring.cloud.kubernetes.secrets.paths searches recursivly so you can create one mountpoint where every secret is mounted if you want to deactivate other features: management.health.kubernetes.enabled: false spring.cloud.kubernetes.config.enabled: true Example yaml and conf: # e.g. Deplyoment #... env: # to prevent automatic config map discovery and stacktraces - name: SPRING_CLOUD_KUBERNETES_CONFIG_ENABLED value: \"false\" # searches recursivly - name: SPRING_CLOUD_KUBERNETES_SECRETS_PATHS value: /app/secrets/ volumeMounts: # this create one file per secret entry - name: secret-properties mountPath: /app/secrets/firstsecret/ - name: secret-properties-2 mountPath: /app/secrets/secondsecret/ volumes: - name: secret-properties secret: secretName: secret-properties - name: secret-properties-2 secret: secretName: secret-properties-2","title":"DEPRECATED Import with k spring.cloud.kubernetes.secrets.paths"},{"location":"spring/spring-externalize-k8s-secret-config/#addtional-applicationyaml-with-springconfigadditional-location","text":"TODO","title":"Addtional application.yaml with spring.config.additional-location"},{"location":"spring/spring-externalize-k8s-secret-config/#programmatically-with-configurationproperties","text":"TODO","title":"programmatically with @ConfigurationProperties"},{"location":"spring/spring-externalize-k8s-secret-config/#alternatives-without-secrets","text":"","title":"alternatives without secrets"},{"location":"spring/spring-externalize-k8s-secret-config/#spring-cloud-config-server","text":"TODO but my own expirience with this is approach is bulky.","title":"spring cloud config server"},{"location":"spring/spring-externalize-k8s-secret-config/#_1","text":"TODO what other mechanismn/operators are there?","title":"?"},{"location":"spring/spring-helpful-properties/","text":"Spring Boot useful properties # show executed sql statement in logs spring.jpa.show-sql=true # creates the tables from provided entities spring.jpa.generate-ddl=true # deletes old tables before recreating them spring.jpa.hibernate.ddl-auto=create-drop # activate h2 console (automatically set by developer tools, when app is started in exploded form) spring.h2.console.enabled=true # access of h2 console from other client spring.h2.console.settings.web-allow-others=true","title":"Spring Boot useful properties"},{"location":"spring/spring-helpful-properties/#spring-boot-useful-properties","text":"# show executed sql statement in logs spring.jpa.show-sql=true # creates the tables from provided entities spring.jpa.generate-ddl=true # deletes old tables before recreating them spring.jpa.hibernate.ddl-auto=create-drop # activate h2 console (automatically set by developer tools, when app is started in exploded form) spring.h2.console.enabled=true # access of h2 console from other client spring.h2.console.settings.web-allow-others=true","title":"Spring Boot useful properties"},{"location":"spring/spring-soap-client/","text":"Spring Soap client add dependecy spring-boot-starter-web-services add maven plugin to generate classes from wsdl add client add client and marshaller as spring bean Wrap Request in JAXBElement if error \"unable to marshal type \"xyz\" as an element because it is missing an @XmlRootElement annotation\" happended add dependecy spring-boot-starter-web-services Add to pom: <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web-services</artifactId> </dependency> add maven plugin to generate classes from wsdl Add to pom: <!-- SOAP client--> .... <plugin> <groupId>org.jvnet.jaxb</groupId> <artifactId>jaxb-maven-plugin</artifactId> <version>4.0.8</version> <executions> <execution> <goals> <goal>generate</goal> </goals> </execution> </executions> <configuration> <schemaLanguage>WSDL</schemaLanguage> <generateDirectory>${project.basedir}/src/main/java</generateDirectory> <!-- also set in spring config --> <generatePackage>de.weyrich.soapclient.gen</generatePackage> <schemaDirectory>${project.basedir}/src/main/resources</schemaDirectory> <schemaIncludes> <include>meine.wsdl</include> </schemaIncludes> </configuration> </plugin> </plugins> </build> </project> add client package de.weyrich; import de.weyrich.soapclient.gen.BeispielRequest; import de.weyrich.soapclient.gen.BesipielResponse; import jakarta.xml.bind.JAXBElement; import org.springframework.ws.client.core.support.WebServiceGatewaySupport; public class SoapClientExamle extends WebServiceGatewaySupport { public BeispielRequestResponse callBeispielRequest(String typeName) { BeispielRequestRequest request = new BeispielRequestRequest(); request.setTypeName(typeName); return (BeispielRequestResponse) getWebServiceTemplate().marshalSendAndReceive(request); } } see last example if this thtows a @XMLRootElement error or an error that JAXB cannot find BeispielRequest. add client and marshaller as spring bean package de.weyrich; import de.weyrich.SoapClientExamle; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.oxm.jaxb.Jaxb2Marshaller; @Configuration public class SoapClientConfig { @Bean public Jaxb2Marshaller marshaller() { Jaxb2Marshaller marshaller = new Jaxb2Marshaller(); // must match plugin in pom marshaller.setContextPath(\"de.weyrich.soapclient.gen\"); return marshaller; } @Bean public SoapClientExamle SoapClientExamle(Jaxb2Marshaller marshaller) { SoapClientExamle client = new SoapClientExamle(); client.setDefaultUri(\"https://example.de/soapservice\"); client.setMarshaller(marshaller); client.setUnmarshaller(marshaller); return client; } } Wrap Request in JAXBElement if error \"unable to marshal type \"xyz\" as an element because it is missing an @XmlRootElement annotation\" happended Use ObjectFactory to wrap request in JAXBElement. Needs to be done when @XmlRootElement is missing in generated classes which might happen because of leagcy WSDL stuff. ```java package de.weyrich; import de.weyrich.soapclient.gen.BeispielRequest; import de.weyrich.soapclient.gen.BesipielResponse; import de.weyrich.soapclient.gen.ObjectFactory; import jakarta.xml.bind.JAXBElement; import org.springframework.ws.client.core.support.WebServiceGatewaySupport; public class SoapClientExamle extends WebServiceGatewaySupport { public BeispielRequestResponse callBeispielRequest(String typeName) { BeispielRequestRequest request = new BeispielRequestRequest(); request.setTypeName(typeName); JAXBElement<BeispielRequestRequest> requestWrapped = new ObjectFactory().createBeispielRequestRequest(request); return (BeispielRequestResponse) getWebServiceTemplate().marshalSendAndReceive(requestWrapped); } }","title":"Spring Soap client"},{"location":"spring/spring-soap-client/#spring-soap-client","text":"add dependecy spring-boot-starter-web-services add maven plugin to generate classes from wsdl add client add client and marshaller as spring bean Wrap Request in JAXBElement if error \"unable to marshal type \"xyz\" as an element because it is missing an @XmlRootElement annotation\" happended","title":"Spring Soap client"},{"location":"spring/spring-soap-client/#add-dependecy-spring-boot-starter-web-services","text":"Add to pom: <dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-web-services</artifactId> </dependency>","title":"add dependecy spring-boot-starter-web-services"},{"location":"spring/spring-soap-client/#add-maven-plugin-to-generate-classes-from-wsdl","text":"Add to pom: <!-- SOAP client--> .... <plugin> <groupId>org.jvnet.jaxb</groupId> <artifactId>jaxb-maven-plugin</artifactId> <version>4.0.8</version> <executions> <execution> <goals> <goal>generate</goal> </goals> </execution> </executions> <configuration> <schemaLanguage>WSDL</schemaLanguage> <generateDirectory>${project.basedir}/src/main/java</generateDirectory> <!-- also set in spring config --> <generatePackage>de.weyrich.soapclient.gen</generatePackage> <schemaDirectory>${project.basedir}/src/main/resources</schemaDirectory> <schemaIncludes> <include>meine.wsdl</include> </schemaIncludes> </configuration> </plugin> </plugins> </build> </project>","title":"add maven plugin to generate classes from wsdl"},{"location":"spring/spring-soap-client/#add-client","text":"package de.weyrich; import de.weyrich.soapclient.gen.BeispielRequest; import de.weyrich.soapclient.gen.BesipielResponse; import jakarta.xml.bind.JAXBElement; import org.springframework.ws.client.core.support.WebServiceGatewaySupport; public class SoapClientExamle extends WebServiceGatewaySupport { public BeispielRequestResponse callBeispielRequest(String typeName) { BeispielRequestRequest request = new BeispielRequestRequest(); request.setTypeName(typeName); return (BeispielRequestResponse) getWebServiceTemplate().marshalSendAndReceive(request); } } see last example if this thtows a @XMLRootElement error or an error that JAXB cannot find BeispielRequest.","title":"add client"},{"location":"spring/spring-soap-client/#add-client-and-marshaller-as-spring-bean","text":"package de.weyrich; import de.weyrich.SoapClientExamle; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.oxm.jaxb.Jaxb2Marshaller; @Configuration public class SoapClientConfig { @Bean public Jaxb2Marshaller marshaller() { Jaxb2Marshaller marshaller = new Jaxb2Marshaller(); // must match plugin in pom marshaller.setContextPath(\"de.weyrich.soapclient.gen\"); return marshaller; } @Bean public SoapClientExamle SoapClientExamle(Jaxb2Marshaller marshaller) { SoapClientExamle client = new SoapClientExamle(); client.setDefaultUri(\"https://example.de/soapservice\"); client.setMarshaller(marshaller); client.setUnmarshaller(marshaller); return client; } }","title":"add client and marshaller as spring bean"},{"location":"spring/spring-soap-client/#wrap-request-in-jaxbelement-if-error-unable-to-marshal-type-xyz-as-an-element-because-it-is-missing-an-xmlrootelement-annotation-happended","text":"Use ObjectFactory to wrap request in JAXBElement. Needs to be done when @XmlRootElement is missing in generated classes which might happen because of leagcy WSDL stuff. ```java package de.weyrich; import de.weyrich.soapclient.gen.BeispielRequest; import de.weyrich.soapclient.gen.BesipielResponse; import de.weyrich.soapclient.gen.ObjectFactory; import jakarta.xml.bind.JAXBElement; import org.springframework.ws.client.core.support.WebServiceGatewaySupport; public class SoapClientExamle extends WebServiceGatewaySupport { public BeispielRequestResponse callBeispielRequest(String typeName) { BeispielRequestRequest request = new BeispielRequestRequest(); request.setTypeName(typeName); JAXBElement<BeispielRequestRequest> requestWrapped = new ObjectFactory().createBeispielRequestRequest(request); return (BeispielRequestResponse) getWebServiceTemplate().marshalSendAndReceive(requestWrapped); } }","title":"Wrap Request in JAXBElement if error \"unable to marshal type \"xyz\" as an element because it is missing an @XmlRootElement annotation\" happended"},{"location":"spring/spring-test/","text":"Spring Boot test For overview of Mockito see the mockito memory aid on this page. MockitoBean As of Spring Boot 3.4 MockitoBean is replacement for deprecated MockBean. @MockBean & @Autowired @MockBean allows to addd a mocked bean to the spring application context. So when you use @Autowired on another class it will load the mocked bean if it was referenced. It is a convenient way to work with mocks, because you do not need to mock every field of the (autowired) class under test. Just the ones you need to behave differently. MockRestServiceServer The MockRestServiceServer allows to intercepts the calls to a RestTemplate. It is handy to mock calls to external APIs. @InjectMock & @Mock You may want to avoid using @InjectMock , because it fails silently without an exception, which is kind of annoying. Alternatively use @MockBean when using spring or just call the constructor of the class under test and mock every parameter of it with @Mock. Remember that a class which is instantiated by a constructor call is not spring managed. Do not load full fledged spring boot server while testing @SpringBootTest -> @ContextConfiguration + @ExtendWith(SpringExtension.class) @AutoConfigureMockMVC -> @WebMVCTest Use test slices to acitvate functionality in bulk: https://docs.spring.io/spring-boot/appendix/test-auto-configuration/slices.html","title":"Spring Boot test"},{"location":"spring/spring-test/#spring-boot-test","text":"For overview of Mockito see the mockito memory aid on this page.","title":"Spring Boot test"},{"location":"spring/spring-test/#mockitobean","text":"As of Spring Boot 3.4 MockitoBean is replacement for deprecated MockBean.","title":"MockitoBean"},{"location":"spring/spring-test/#mockbean-autowired","text":"@MockBean allows to addd a mocked bean to the spring application context. So when you use @Autowired on another class it will load the mocked bean if it was referenced. It is a convenient way to work with mocks, because you do not need to mock every field of the (autowired) class under test. Just the ones you need to behave differently.","title":"@MockBean &amp; @Autowired"},{"location":"spring/spring-test/#mockrestserviceserver","text":"The MockRestServiceServer allows to intercepts the calls to a RestTemplate. It is handy to mock calls to external APIs.","title":"MockRestServiceServer"},{"location":"spring/spring-test/#injectmock-mock","text":"You may want to avoid using @InjectMock , because it fails silently without an exception, which is kind of annoying. Alternatively use @MockBean when using spring or just call the constructor of the class under test and mock every parameter of it with @Mock. Remember that a class which is instantiated by a constructor call is not spring managed.","title":"@InjectMock &amp; @Mock"},{"location":"spring/spring-test/#do-not-load-full-fledged-spring-boot-server-while-testing","text":"@SpringBootTest -> @ContextConfiguration + @ExtendWith(SpringExtension.class) @AutoConfigureMockMVC -> @WebMVCTest Use test slices to acitvate functionality in bulk: https://docs.spring.io/spring-boot/appendix/test-auto-configuration/slices.html","title":"Do not load full fledged spring boot server while testing"},{"location":"spring/spring_security/","text":"Spring security add custom authentication implement AuthenticationProvider in you own class and register it via nice example: https://github.com/dewantrie/springboot-custom-authentication-provider/blob/master/src/main/java/com/example/demo/config/SecurityConfig.java Authentication AuthenticationProvider AuthenticationManager UserDetailsService UserDetails SecurityFilterChain Configure exception for auth filter Disable for: for actuator endpoints swagger-ui @Configuration @EnableWebSecurity public class WebSecurityConfiguration extends AbstractSecurityWebApplicationInitializer { @Bean public SecurityFilterChain filterChain(HttpSecurity http) throws Exception { return http .authorizeHttpRequests(authz -> authz // actuator .requestMatchers(antMatcher(\"actuator/**\")).permitAll() //swagger .requestMatchers(antMatcher(\"/swagger-ui/**\")).permitAll() .requestMatchers(antMatcher(\"/v3/api-docs/**\")).permitAll() ) // disable CSRF .csrf(AbstractHttpConfigurer::disable) .build(); } }","title":"Spring security"},{"location":"spring/spring_security/#spring-security","text":"","title":"Spring security"},{"location":"spring/spring_security/#add-custom-authentication","text":"implement AuthenticationProvider in you own class and register it via nice example: https://github.com/dewantrie/springboot-custom-authentication-provider/blob/master/src/main/java/com/example/demo/config/SecurityConfig.java Authentication AuthenticationProvider AuthenticationManager UserDetailsService UserDetails SecurityFilterChain","title":"add custom authentication"},{"location":"spring/spring_security/#configure-exception-for-auth-filter","text":"Disable for: for actuator endpoints swagger-ui @Configuration @EnableWebSecurity public class WebSecurityConfiguration extends AbstractSecurityWebApplicationInitializer { @Bean public SecurityFilterChain filterChain(HttpSecurity http) throws Exception { return http .authorizeHttpRequests(authz -> authz // actuator .requestMatchers(antMatcher(\"actuator/**\")).permitAll() //swagger .requestMatchers(antMatcher(\"/swagger-ui/**\")).permitAll() .requestMatchers(antMatcher(\"/v3/api-docs/**\")).permitAll() ) // disable CSRF .csrf(AbstractHttpConfigurer::disable) .build(); } }","title":"Configure exception for auth filter"},{"location":"spring/spring_vs_jakartaee/","text":"Spring vs. Jakarta EE define beans Source: Enji's Blog inject beans Source: Enji's Blog","title":"Spring vs. Jakarta EE"},{"location":"spring/spring_vs_jakartaee/#spring-vs-jakarta-ee","text":"","title":"Spring vs. Jakarta EE"},{"location":"spring/spring_vs_jakartaee/#define-beans","text":"Source: Enji's Blog","title":"define beans"},{"location":"spring/spring_vs_jakartaee/#inject-beans","text":"Source: Enji's Blog","title":"inject beans"},{"location":"spring/time/","text":"Time JPA TO save dates in UTC in database in spring boot you can set the following: spring.jpa.properties.hibernate.jdbc.time_zone=UTC ZonedDateTime and daylight saving List of time format strings List of valid zoneID for ZonedDateTime date parsing for java time helpful","title":"Time"},{"location":"spring/time/#time","text":"","title":"Time"},{"location":"spring/time/#jpa","text":"TO save dates in UTC in database in spring boot you can set the following: spring.jpa.properties.hibernate.jdbc.time_zone=UTC","title":"JPA"},{"location":"spring/time/#zoneddatetime-and-daylight-saving","text":"List of time format strings List of valid zoneID for ZonedDateTime","title":"ZonedDateTime and daylight saving"},{"location":"spring/time/#date-parsing-for-java-time","text":"helpful","title":"date parsing for java time"},{"location":"web/css/","text":"css selectors List of css selectors List of css properties .class #id element [attribute] selector { property: value; property2: values; /* mutiple values are separated by a space */ } Block Element Modifier BEM Overview TODO Tools TODO: Icon, Color schemes etc... sass preprocessor for css .scss files more structured code through variables nesting imports mixins functions import You can import other scss files into the current one. The command @import(\"mixin\") would import mixin.scss and add all variables and mixins into your file. if you import multiple files the order is kept. variables You can define variables outside of a selector block and use them inside of one. $variable_name=20px .image { height: $variable_name } nesting With sass you do not need to nest the selectors inside each other if you want to address . Instead you can use the following syntax: button { /* code */ } img button { /* code which addresses all img Elements inside a button Element*/ } mixins Mixins allow to define reusable code which can be used in selector blocks with @include: @mixin boxmodel($radius) { padding: $radius; margin: $radius; } button { @include boxmodel(4px) } functions and build-in modules With @function ypu can define complex operations. @Use allows you to import build-in modules which contain mixins and functions. A list of the modules is given here","title":"css"},{"location":"web/css/#css","text":"","title":"css"},{"location":"web/css/#selectors","text":"List of css selectors List of css properties .class #id element [attribute] selector { property: value; property2: values; /* mutiple values are separated by a space */ }","title":"selectors"},{"location":"web/css/#block-element-modifier-bem","text":"Overview TODO","title":"Block Element Modifier BEM"},{"location":"web/css/#tools","text":"TODO: Icon, Color schemes etc...","title":"Tools"},{"location":"web/css/#sass","text":"preprocessor for css .scss files more structured code through variables nesting imports mixins functions","title":"sass"},{"location":"web/css/#import","text":"You can import other scss files into the current one. The command @import(\"mixin\") would import mixin.scss and add all variables and mixins into your file. if you import multiple files the order is kept.","title":"import"},{"location":"web/css/#variables","text":"You can define variables outside of a selector block and use them inside of one. $variable_name=20px .image { height: $variable_name }","title":"variables"},{"location":"web/css/#nesting","text":"With sass you do not need to nest the selectors inside each other if you want to address . Instead you can use the following syntax: button { /* code */ } img button { /* code which addresses all img Elements inside a button Element*/ }","title":"nesting"},{"location":"web/css/#mixins","text":"Mixins allow to define reusable code which can be used in selector blocks with @include: @mixin boxmodel($radius) { padding: $radius; margin: $radius; } button { @include boxmodel(4px) }","title":"mixins"},{"location":"web/css/#functions-and-build-in-modules","text":"With @function ypu can define complex operations. @Use allows you to import build-in modules which contain mixins and functions. A list of the modules is given here","title":"functions and build-in modules"},{"location":"web/dom/","text":"dom vs. virtual dom vs. shadow dom dom (browser) The html document object model (dom) is an API that allows the manipulation of a webpage by representing the structure of a html document. Source virtual dom (framework) A virtual dom is designed to optimize calls to the real dom. A change in the dom leads to a rendering of the webpage, which is in comparison a slow operation. So javascript frameworks (like react.js) keep a copy of the dom (the virtual dom) which allows them to bundle changes before updating the dom. Source shadow dom (browser) A shadow dom allows the encapsulation of structure, style and behavior inside another dom. A shadow dom is a hidden separated dom which can be attached to an html element. Source","title":"dom vs. virtual dom vs. shadow dom"},{"location":"web/dom/#dom-vs-virtual-dom-vs-shadow-dom","text":"","title":"dom vs. virtual dom vs. shadow dom"},{"location":"web/dom/#dom-browser","text":"The html document object model (dom) is an API that allows the manipulation of a webpage by representing the structure of a html document. Source","title":"dom (browser)"},{"location":"web/dom/#virtual-dom-framework","text":"A virtual dom is designed to optimize calls to the real dom. A change in the dom leads to a rendering of the webpage, which is in comparison a slow operation. So javascript frameworks (like react.js) keep a copy of the dom (the virtual dom) which allows them to bundle changes before updating the dom. Source","title":"virtual dom (framework)"},{"location":"web/dom/#shadow-dom-browser","text":"A shadow dom allows the encapsulation of structure, style and behavior inside another dom. A shadow dom is a hidden separated dom which can be attached to an html element. Source","title":"shadow dom (browser)"},{"location":"web/javascript/","text":"javascript Overview of ES2015 features/syntax rest client Usual apis: Fetch API XMLHttpRequest jQuery.ajax() The fetch api is the newest API is completely Promise-based provides global fetch() method promises, async and await State: pending fullfilled rejected arrow functions","title":"javascript"},{"location":"web/javascript/#javascript","text":"Overview of ES2015 features/syntax","title":"javascript"},{"location":"web/javascript/#rest-client","text":"Usual apis: Fetch API XMLHttpRequest jQuery.ajax() The fetch api is the newest API is completely Promise-based provides global fetch() method","title":"rest client"},{"location":"web/javascript/#promises-async-and-await","text":"State: pending fullfilled rejected","title":"promises, async and await"},{"location":"web/javascript/#arrow-functions","text":"","title":"arrow functions"},{"location":"web/layout/","text":"css layout techniques overview Nice overview of css layouts. Layouts: flex box : allows one dimensions, either a columns or a row. Grid : build like a matrix Float Table","title":"css layout techniques"},{"location":"web/layout/#css-layout-techniques","text":"","title":"css layout techniques"},{"location":"web/layout/#overview","text":"Nice overview of css layouts. Layouts: flex box : allows one dimensions, either a columns or a row. Grid : build like a matrix Float Table","title":"overview"},{"location":"web/planning-steps/","text":"Webpage planning steps create layout identify sections first. (navbar, footer, hero etc.) identify the elements (images, dropdown, searchbar) in the section decide how to group elements (flex or grid) for a groups of elements in a section use semantic html Overview Most commonly used: header - general header or of a section nav section article aside footer Use them instead of div as a container element. design examples UI examples for different tasks Checklist for UI compponents","title":"Webpage planning steps"},{"location":"web/planning-steps/#webpage-planning-steps","text":"","title":"Webpage planning steps"},{"location":"web/planning-steps/#create-layout","text":"identify sections first. (navbar, footer, hero etc.) identify the elements (images, dropdown, searchbar) in the section decide how to group elements (flex or grid) for a groups of elements in a section","title":"create layout"},{"location":"web/planning-steps/#use-semantic-html","text":"Overview Most commonly used: header - general header or of a section nav section article aside footer Use them instead of div as a container element.","title":"use semantic html"},{"location":"web/planning-steps/#design-examples","text":"UI examples for different tasks Checklist for UI compponents","title":"design examples"},{"location":"web/vue/","text":"Vue JS Vue style guide Vue instance properties vue cli Vue clie allows to easily scafffold a vue project: install vie npm install -g @vue/cli create project: vue ui is ui guided vue create my-project To set environmental variables in vue during transpile time: use .env-XXX files in the root of the project to define variables .env global definition .env.local ignored by git .env.development default used by npm serve / vue-cli-service serve .env.production default used by npm build / vue-cli-service build variables need to starts with VUE_APP_ usage in code with process.env.VUE_APP_ overview the vue core library focuses on view layer only can be used via cdn or with vue-cli scafffolded project (with webpack, babel etc.) allows to bind js variables with html content reactivly change content on html page when js variable changes. No need for direct DOM manipulation declarative rendering via template syntax in html {{}} two way binding via v-model writable html fields v-if for conditional rendering v-for for loops v-event for binding events to function. The feature here is that the function do not need to manipulate the dom directly. concept which works like Custom Elements of the web components standard vue instance You need at least one Vue instance to start a application. Loosely coupled to the Model\u2013view\u2013viewmodel (MVVM) and the Vue instance is the viewmodel(vm) and the html the view. The component system also use Vue instances the data property of a Vue object is used to reactly injected into the dom. (you can not define new fields in data after the page was rendered) Vue instance properties relevant ( full list ): data el computed methods watch vue lifecycle is shown in this graphic . An overview of the lifecylce is: set up data observation compile the template mount the instance to the DOM update the DOM when data changes there are lifecylce hooks for the different events. These are functions with a predefined name (like created or mounted). Here is the full list . template syntax allows to declaratively bind the rendered DOM to the vue instance Vue templates are valid HTML Vue uses a virtual DOM to optimize the operations on the real DOM Syntax: bind (uninterpreted/escaped) text with mustache syntax: {{}} bind html with: v-html directive bind attributes with: v-bind: before the attribute or just : bind events with: v-on before the event name or just @ Use (one) javascript expression between curlys: {{ text.split('') }} (They do have access to the data property and a few global ) directives link to a javascript expression and are started with v- (predefined examples are v-for, v-if) some directives take arguments (like v-bind(: =\" \") does which takes the attributes value as an argument) directives can also take dynamic arguments which are javascript expression in square brackets. An example is <a v-bind:[attributeName]=\"url\"> ... </a> . But there are constraints on this feature. modifiers are postfixes of directives which starts with a . computed properties Have the same function as the inline javascript expression between curlys in html Allows to move this code to a function defined inside the computed field of the Vue instance called with just the name of the computed expressions (so without the braces () of the funciton) If a field inside the data field changes vue will automagically update all bindings of a computed properties which use this field as well unfavourable but more flexible alternatives: vue instance methods, because computed properties are cached watched properties, because their sytanx is imperative and tends to be verbose style binding v-bind has a special features when binded to class and style attribute toggle a single class with a isActive property: v-bind:class=\"{ preDefinedClass: isActive }\" toggle multiple classes : v-bind:class=\"{ preDefinedClass: isActive, 'my-class-name': show }\" have static class and toggled class: class=\"static\" v-bind:class=\"{ classDefinedInProperty: isActive } it is also possbile to define no inline expression but a data property or a computed expression for the class toggling logic add list of classes with an array syntax : v-bind:class=\"[class1, class2]\" bind to style attribute is similar see conditional rendering and list rendering add v-if attribute to a element to conditionally render it has effect on all nested elements there are also v-else and v-else-if allows to build input forms which do not delete its content when the user change to a different option if you want to prevent this behaviour you need to add a key attribute v-show is similar to v-if but it keeps the elements in the dom at all times (only toggles the display CSS property) list rendering use v-for=\"item in items\" to print an elements for every item in the items field of data vue only propagate changes to the DOM when the following methods are used: push(), pop(), shift(), unshift(), splice(), sort(), reverse() for filtering iterate on computed properties instead of the property directly events use v-on:<event> or @<event> directive you can use inline exrepssion or a directive call in the vue instance to define the behaviour when the event happends to pass the event reference to the method handler add a $event parameter in the template form input bindings you can two way data bind a input field inside a form element with v-model the initial value will be picked from the data binding not the value in the html template the events to update a data binding from template are: input event on the value property for text and textarea change event on the checked property for checkboxes and radiobuttons change event on the value property for select fields there are modifiers to add some behavoiur to the data binding (.lazy, .trim etc.)","title":"Vue JS"},{"location":"web/vue/#vue-js","text":"Vue style guide Vue instance properties","title":"Vue JS"},{"location":"web/vue/#vue-cli","text":"Vue clie allows to easily scafffold a vue project: install vie npm install -g @vue/cli create project: vue ui is ui guided vue create my-project To set environmental variables in vue during transpile time: use .env-XXX files in the root of the project to define variables .env global definition .env.local ignored by git .env.development default used by npm serve / vue-cli-service serve .env.production default used by npm build / vue-cli-service build variables need to starts with VUE_APP_ usage in code with process.env.VUE_APP_","title":"vue cli"},{"location":"web/vue/#overview","text":"the vue core library focuses on view layer only can be used via cdn or with vue-cli scafffolded project (with webpack, babel etc.) allows to bind js variables with html content reactivly change content on html page when js variable changes. No need for direct DOM manipulation declarative rendering via template syntax in html {{}} two way binding via v-model writable html fields v-if for conditional rendering v-for for loops v-event for binding events to function. The feature here is that the function do not need to manipulate the dom directly. concept which works like Custom Elements of the web components standard","title":"overview"},{"location":"web/vue/#vue-instance","text":"You need at least one Vue instance to start a application. Loosely coupled to the Model\u2013view\u2013viewmodel (MVVM) and the Vue instance is the viewmodel(vm) and the html the view. The component system also use Vue instances the data property of a Vue object is used to reactly injected into the dom. (you can not define new fields in data after the page was rendered) Vue instance properties relevant ( full list ): data el computed methods watch vue lifecycle is shown in this graphic . An overview of the lifecylce is: set up data observation compile the template mount the instance to the DOM update the DOM when data changes there are lifecylce hooks for the different events. These are functions with a predefined name (like created or mounted). Here is the full list .","title":"vue instance"},{"location":"web/vue/#template-syntax","text":"allows to declaratively bind the rendered DOM to the vue instance Vue templates are valid HTML Vue uses a virtual DOM to optimize the operations on the real DOM Syntax: bind (uninterpreted/escaped) text with mustache syntax: {{}} bind html with: v-html directive bind attributes with: v-bind: before the attribute or just : bind events with: v-on before the event name or just @ Use (one) javascript expression between curlys: {{ text.split('') }} (They do have access to the data property and a few global ) directives link to a javascript expression and are started with v- (predefined examples are v-for, v-if) some directives take arguments (like v-bind(: =\" \") does which takes the attributes value as an argument) directives can also take dynamic arguments which are javascript expression in square brackets. An example is <a v-bind:[attributeName]=\"url\"> ... </a> . But there are constraints on this feature. modifiers are postfixes of directives which starts with a .","title":"template syntax"},{"location":"web/vue/#computed-properties","text":"Have the same function as the inline javascript expression between curlys in html Allows to move this code to a function defined inside the computed field of the Vue instance called with just the name of the computed expressions (so without the braces () of the funciton) If a field inside the data field changes vue will automagically update all bindings of a computed properties which use this field as well unfavourable but more flexible alternatives: vue instance methods, because computed properties are cached watched properties, because their sytanx is imperative and tends to be verbose","title":"computed properties"},{"location":"web/vue/#style-binding","text":"v-bind has a special features when binded to class and style attribute toggle a single class with a isActive property: v-bind:class=\"{ preDefinedClass: isActive }\" toggle multiple classes : v-bind:class=\"{ preDefinedClass: isActive, 'my-class-name': show }\" have static class and toggled class: class=\"static\" v-bind:class=\"{ classDefinedInProperty: isActive } it is also possbile to define no inline expression but a data property or a computed expression for the class toggling logic add list of classes with an array syntax : v-bind:class=\"[class1, class2]\" bind to style attribute is similar see","title":"style binding"},{"location":"web/vue/#conditional-rendering-and-list-rendering","text":"add v-if attribute to a element to conditionally render it has effect on all nested elements there are also v-else and v-else-if allows to build input forms which do not delete its content when the user change to a different option if you want to prevent this behaviour you need to add a key attribute v-show is similar to v-if but it keeps the elements in the dom at all times (only toggles the display CSS property)","title":"conditional rendering and list rendering"},{"location":"web/vue/#list-rendering","text":"use v-for=\"item in items\" to print an elements for every item in the items field of data vue only propagate changes to the DOM when the following methods are used: push(), pop(), shift(), unshift(), splice(), sort(), reverse() for filtering iterate on computed properties instead of the property directly","title":"list rendering"},{"location":"web/vue/#events","text":"use v-on:<event> or @<event> directive you can use inline exrepssion or a directive call in the vue instance to define the behaviour when the event happends to pass the event reference to the method handler add a $event parameter in the template","title":"events"},{"location":"web/vue/#form-input-bindings","text":"you can two way data bind a input field inside a form element with v-model the initial value will be picked from the data binding not the value in the html template the events to update a data binding from template are: input event on the value property for text and textarea change event on the checked property for checkboxes and radiobuttons change event on the value property for select fields there are modifiers to add some behavoiur to the data binding (.lazy, .trim etc.)","title":"form input bindings"},{"location":"web/web-evolution/","text":"overlook over web evolution https://www.quora.com/What-are-the-pros-and-cons-of-server-side-rendering-vs-client-side-rendering-with-APIs-What-is-more-popular-when-building-web-apps webpage - static website (HTML files are stored on server) (Traditional) server-side rendered pages (use PHP, ASP, Servlets and HTML templates) Client Side Rendering (Bootstrapping HTML via Javascript) Symmetric server side rendering (like client side rendering but on server via NodeJS before it is served to client) static website (again): JAMStack (html pages which use javascript to call Rest Services.) Framwork evolution JS CSS","title":"overlook over web evolution"},{"location":"web/web-evolution/#overlook-over-web-evolution","text":"https://www.quora.com/What-are-the-pros-and-cons-of-server-side-rendering-vs-client-side-rendering-with-APIs-What-is-more-popular-when-building-web-apps webpage - static website (HTML files are stored on server) (Traditional) server-side rendered pages (use PHP, ASP, Servlets and HTML templates) Client Side Rendering (Bootstrapping HTML via Javascript) Symmetric server side rendering (like client side rendering but on server via NodeJS before it is served to client) static website (again): JAMStack (html pages which use javascript to call Rest Services.)","title":"overlook over web evolution"},{"location":"web/web-evolution/#framwork-evolution","text":"JS CSS","title":"Framwork evolution"}]}